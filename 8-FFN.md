<!-- JPWçš„Markdownç¬”è®°æ¨¡æ¿ v1, å…¶ä¸­çš„hreféœ€è¦è§†æƒ…æ›´æ”¹ä¸Šçº§ç›®å½•href="../../format.css -->
<link rel="stylesheet" type="text/css" href="../../format.css">


<h1>LLMsç³»åˆ—è¿›é˜¶ï¼šå‰é¦ˆç¥ç»ç½‘ç»œFFN</h1>

ğŸ’¡ å¤§æ¨¡å‹æ˜¯nä¸ªTransformer Blockçš„å †å ï¼Œæ¯ä¸ªBlockå†…ç”±Attentionæ¨¡å—å’Œå…¨è¿æ¥æ¨¡å—ç»„æˆã€‚å‰è€…è´Ÿè´£æ³¨æ„åŠ›æ•æ‰ï¼ˆä¸å«æ¿€æ´»ï¼‰ï¼Œåè€…è´Ÿè´£å¢å¼ºè¡¨ç¤ºï¼Œå¸¸å¸¸ä½¿ç”¨MLPï¼ˆå«æ¿€æ´»å‡½æ•°ï¼‰ã€‚


# 1. MLPä½œä¸ºFFN
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
class MLP( nn.Module ):
    """ä¸¤å±‚ï¼Œç†è®ºä¸ŠMLPå±‚æ‰©å±•ç»´åº¦4å€ï¼Œä½†å®é™…ä¸ä¸€æ ·"""
    def __init__(self, config, device = None):
        super().__init__()
        self.add_bias = config.add_bias_linear
        self.dense_h_to_4h = nn.Linear(
            config.hidden_size,
            config.ffn_hidden_size * 2,
            bias = self.add_bias,
            device = device,
        )
        
        def swiglu(x):
            x = torch.chunk(x, 2, dim = -1)
            return F.silu( x[0] ) * x[1]

        self.activation_func = swiglu

        self.dense_4h_to_h = nn.Linear( 
            config.ffn_hidden_size,
            config.hidden_size,
            bias = self.add_bias,
            device = device,
        )
    
    def forward( self, hidden_states ):
        intermediate = self.dense_h_to_4h( hidden_states )
        intermediate = self.activation_func( intermediate )
        output = self.dense_4h_to_h( intermediate )
        return output
```

# 2. MoEä½œä¸ºFFN
Mixture of Experts (MoE) æ··åˆä¸“å®¶æ¨¡å‹ï¼Œå®ƒé€šè¿‡åŠ¨æ€é€‰æ‹©å­æ¨¡å‹ï¼ˆä¸“å®¶ï¼‰è¿›è¡Œè®¡ç®—ï¼Œä»è€Œåœ¨æé«˜æ¨¡å‹å®¹é‡çš„åŒæ—¶ï¼ŒåŠ å¿«è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ï¼Œä½†å¯¹å¾®è°ƒå¸¦æ¥äº†è¾ƒå¤§æŒ‘æˆ˜ã€‚

<p align="center">
  <img src="images/moe.PNG" width=100%><br>
</p>
ä½œä¸ºä¸€ç§åŸºäº Transformer æ¶æ„çš„æ¨¡å‹ï¼Œæ··åˆä¸“å®¶æ¨¡å‹ä¸»è¦ç”±ä¸¤ä¸ªå…³é”®éƒ¨åˆ†ç»„æˆ:
- ç¨€ç– MoE å±‚: è¿™äº›å±‚ä»£æ›¿äº†ä¼ ç»Ÿ Transformer æ¨¡å‹ä¸­çš„å‰é¦ˆç½‘ç»œ (FFN) å±‚ã€‚MoE å±‚åŒ…å«è‹¥å¹²â€œä¸“å®¶â€(ä¾‹å¦‚ 8 ä¸ª)ï¼Œæ¯ä¸ªä¸“å®¶æœ¬èº«æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ç¥ç»ç½‘ç»œã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™äº›ä¸“å®¶é€šå¸¸æ˜¯å‰é¦ˆç½‘ç»œ (FFN)ï¼Œä½†å®ƒä»¬ä¹Ÿå¯ä»¥æ˜¯æ›´å¤æ‚çš„ç½‘ç»œç»“æ„ï¼Œç”šè‡³å¯ä»¥æ˜¯ MoE å±‚æœ¬èº«ï¼Œä»è€Œå½¢æˆå±‚çº§å¼çš„ MoE ç»“æ„ã€‚
- é—¨æ§ç½‘ç»œæˆ–è·¯ç”±: è¿™ä¸ªéƒ¨åˆ†ç”¨äºå†³å®šå“ªäº›ä»¤ç‰Œ (token) è¢«å‘é€åˆ°å“ªä¸ªä¸“å®¶ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸‹å›¾ä¸­ï¼Œâ€œMoreâ€è¿™ä¸ªä»¤ç‰Œå¯èƒ½è¢«å‘é€åˆ°ç¬¬äºŒä¸ªä¸“å®¶ï¼Œè€Œâ€œParametersâ€è¿™ä¸ªä»¤ç‰Œè¢«å‘é€åˆ°ç¬¬ä¸€ä¸ªä¸“å®¶ã€‚æœ‰æ—¶ï¼Œä¸€ä¸ªä»¤ç‰Œç”šè‡³å¯ä»¥è¢«å‘é€åˆ°å¤šä¸ªä¸“å®¶ã€‚ä»¤ç‰Œçš„è·¯ç”±æ–¹å¼æ˜¯ MoE ä½¿ç”¨ä¸­çš„ä¸€ä¸ªå…³é”®ç‚¹ï¼Œå› ä¸ºè·¯ç”±å™¨ç”±å­¦ä¹ çš„å‚æ•°ç»„æˆï¼Œå¹¶ä¸”ä¸ç½‘ç»œçš„å…¶ä»–éƒ¨åˆ†ä¸€åŒè¿›è¡Œé¢„è®­ç»ƒã€‚

ä¼˜ç‚¹ï¼š
- æ¨¡å‹å‚æ•°é‡å¯ä»¥éå¸¸å¤§
- è®¡ç®—æ•ˆç‡é«˜ï¼Œæ¯æ¬¡åªé€‰æ‹©ä¸€éƒ¨åˆ†ä¸“å®¶è¿›è¡Œè®¡ç®—
- è‡ªé€‚åº”æ€§å¼ºï¼Œé—¨æ§ç½‘ç»œä½¿å¾—æ¨¡å‹å¯ä»¥æ ¹æ®è¾“å…¥æ•°æ®è‡ªåŠ¨é€‰æ‹©ä¸åŒä¸“å®¶ï¼Œåœ¨ä¸åŒçš„ä»»åŠ¡åœºæ™¯é€‚åº”èƒ½åŠ›æ›´å¼º

æŒ‘æˆ˜ï¼š
- è´Ÿè½½ä¸å‡è¡¡ï¼šéƒ¨åˆ†ä¸“å®¶è¢«é¢‘ç¹é€‰æ‹©ï¼Œå…¶ä»–ä¸“å®¶å¾ˆå°‘æ¿€æ´»
- ä¸“å®¶çš„è®­ç»ƒéš¾é¢˜ï¼šé—¨æ§æœºåˆ¶åœ¨è®­ç»ƒä¸­ä¼šå­˜åœ¨éƒ¨åˆ†ä¸“å®¶è®­ç»ƒä¸è¶³ï¼Œå‚æ•°æ›´æ–°å¯èƒ½ä¸å¹³è¡¡ã€‚
- é—¨æ§ç½‘ç»œè®¾è®¡å…³é”®ä¸”å¯Œæœ‰æŒ‘æˆ˜

```python
# éä¸¥æ ¼MoE: è¿™ä¸ªè®¾è®¡æ¨¡å¼æ¯”è¾ƒç‰¹åˆ«ï¼Œé€šè¿‡å°†ä¿¡å·åˆ†ä¸ºâ€œé—¨æ§â€ä¿¡å·å’Œâ€œæ”¾å¤§â€ä¿¡å·çš„æ–¹å¼ï¼Œèƒ½å¤Ÿçµæ´»åœ°è°ƒæ•´ç½‘ç»œçš„è¡¨è¾¾èƒ½åŠ›ï¼Œç±»ä¼¼äºä¸“å®¶æ¨¡å‹ä¸­çš„åŠ æƒé€‰æ‹©è¿‡ç¨‹ï¼ˆä¸è¿‡å®ƒå¹¶ä¸æ˜¯ä¸¥æ ¼çš„ MoE æ¨¡å‹ï¼‰ã€‚è¿™ç§ç»“æ„å¯èƒ½æœ‰åŠ©äºç½‘ç»œåœ¨ä¸åŒç»´åº¦çš„ç‰¹å¾ä¹‹é—´è¿›è¡Œæ›´ç»†ç²’åº¦çš„è°ƒæ•´ï¼Œä»è€Œæé«˜æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ã€‚
import torch
import torch.nn as nn
class MistralMLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.hidden_size = config.hidden_size                   # 4096
        self.intermediate_size = config.intermediate_size       # 14336
        self.gate_proj = nn.Linear( self.hidden_size, self.intermediate_size, bias = False)     # ç”Ÿæˆæ¿€æ´»åçš„é—¨æ§ä¿¡å·ï¼Œä¸upé€ä½ä¹˜åè¾“å…¥down
        self.up_proj = nn.Linear( self.hidden_size, self.intermediate_size, bias = False)       # ç”Ÿæˆæ”¾å¤§ä¿¡å·
        self.down_proj = nn.Linear( self.intermediate_size, self.hidden_size, bias = False)     # æ˜ å°„å›åŸç©ºé—´
        self.act_fn = ACT2FN[config.hidden_act]     # "silu"
    
    def forward( self, x):
        gate = self.gate_proj( x )
        gate = self.act_fn( gate )
        up = self.up_proj( x )
        down = self.down_proj( gate*up )
        
        return down * up
```