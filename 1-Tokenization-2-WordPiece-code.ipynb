{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiangpeiwen2/jiangpeiwen2/miniconda3/envs/tkgt2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from typing import List\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPieceTokenizer:\n",
    "    \"\"\"Google的WordPiece分词法（未开源，根据原理实现），代码借鉴https://huggingface.co/learn/nlp-course/zh-CN/chapter6/6\n",
    "    \n",
    "    Args:\n",
    "        max_iters (int): 退出条件，最大迭代次数\n",
    "        vocab_size (int): 退出条件，最大词表长度\n",
    "        corpus (List[str]): 原始语料库\n",
    "        punctuations (List[str]): 需要特殊对待的标点符号，默认为[\",\", \".\", \"\\\"\", \"'\"]\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_iters:int=None,\n",
    "        vocab_size:int=None,\n",
    "        corpus:List[str]=None,\n",
    "        punctuations:List[str]=[\",\", \".\", \"\\\"\", \"'\"]\n",
    "    ):\n",
    "        self.max_iters = max_iters\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab_dict = None\n",
    "        self.punctuations = punctuations\n",
    "\n",
    "        # 初始的词频统计表（所有词：词频），语料库内所有单词出现的次数，不更新，仅作为后续倍乘基数\n",
    "        self.word_freqs = self.pre_states(corpus)       \n",
    "        alphabet = self.get_alphabet(self.word_freqs)\n",
    "        # 初始的词表，每次合并都需要更新（这里仅append，因此词表大小递增）\n",
    "        self.vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet\n",
    "        # 初始的词分词方案（所有词：原始分词方案），每次更新\n",
    "        self.splits = self.get_split_dict(self.word_freqs)\n",
    "    \n",
    "    def pre_states(self, corpus):\n",
    "        \"\"\"对语料集进行预分词并统计每个词的频数\"\"\"\n",
    "        word_freqs = defaultdict(int)\n",
    "        for sentence in corpus:\n",
    "            # 1.手动预分词：不能只根据空格分词，还要考虑标点\n",
    "            words = sentence.split()\n",
    "            for word in words:\n",
    "                for punc in self.punctuations:\n",
    "                    if word.endswith(punc):\n",
    "                        word = word.replace(punc, \"\")\n",
    "                        word_freqs[punc] += 1\n",
    "                word_freqs[word] += 1\n",
    "            '''# 2. 使用现有工具预分词\n",
    "            words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(sentence)   # 返回一个字符串中所有word和标点及其起止位置\n",
    "            words = [word for word, offset in words_with_offsets]\n",
    "            for word in words:\n",
    "                word_freqs[word] += 1'''\n",
    "        return word_freqs\n",
    "    \n",
    "    def get_alphabet(self, word_freqs):\n",
    "        \"\"\"从词频统计表中获取当前词表\"\"\"\n",
    "        alphabet = []\n",
    "        for word in word_freqs.keys():\n",
    "            if word[0] not in alphabet: # 首字母\n",
    "                alphabet.append(word[0])\n",
    "            for letter in word[1:]: # 后续字母\n",
    "                if f\"##{letter}\" not in alphabet:\n",
    "                    alphabet.append(f\"##{letter}\")\n",
    "        alphabet.sort() # 根据ASCII码降序以适应MM算法\n",
    "        return alphabet\n",
    "    \n",
    "    def get_split_dict(self, word_freqs):\n",
    "        \"\"\"对词频表中每个词进行原始分割并加入中间分隔符\"\"\"\n",
    "        splits = {\n",
    "            word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)]\n",
    "            for word in word_freqs.keys()\n",
    "        }\n",
    "        return splits\n",
    "    \n",
    "    def compute_PMI_scores(self):\n",
    "        \"\"\"基于整个语料库的词频字典word_freqs的PMI点互信息得分计算：遍历统计pair频数（与BPE一致）和每个letter的频数\"\"\"\n",
    "        letter_freqs = defaultdict(int)\n",
    "        pair_freqs = defaultdict(int)\n",
    "        for word, freq in self.word_freqs.items():\n",
    "            split = self.splits[word]\n",
    "            if len(split) == 1: # 单独处理只有一个元素无法成对的情况\n",
    "                letter_freqs[split[0]] += freq\n",
    "                continue\n",
    "            for i in range(len(split) - 1):\n",
    "                pair = (split[i], split[i + 1])\n",
    "                letter_freqs[split[i]] += freq  # 统计每个词出现的频数\n",
    "                pair_freqs[pair] += freq    # 统计每个pair出现的频数\n",
    "            letter_freqs[split[-1]] += freq # 补充统计最后一个词的频数\n",
    "\n",
    "        # 计算每个pair的点互信息：P(AB) / P(letter_A)*P(letter_B)\n",
    "        scores = {\n",
    "            pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
    "            for pair, freq in pair_freqs.items()\n",
    "        }\n",
    "        return scores\n",
    "    \n",
    "    def merge_pair(self, best_pair):\n",
    "        \"\"\"子词融合，更新splits分割字典\"\"\"\n",
    "        for word in self.splits.keys():\n",
    "            split = self.splits[word]\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == best_pair[0] and split[i + 1] == best_pair[1]:\n",
    "                    merge = best_pair[0] + best_pair[1][2:] if best_pair[1].startswith(\"##\") else best_pair[0] + best_pair[1]\n",
    "                    split = split[:i] + [merge] + split[i+2:]\n",
    "                else:\n",
    "                    i += 1\n",
    "            self.splits[word] = split\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"训练代码：基于极大似然的合并，以PMI近似\"\"\"\n",
    "        while len(self.vocab) < self.vocab_size:\n",
    "            print(\"#\"*20)\n",
    "            print(f\"当前词表长度：{len(self.vocab)}\")\n",
    "            pair_scores = self.compute_PMI_scores()\n",
    "            best_pair = max(pair_scores, key = pair_scores.get)\n",
    "            max_score = pair_scores[best_pair]\n",
    "            self.merge_pair(best_pair)\n",
    "            new_token = (\n",
    "                best_pair[0] + best_pair[1][2:]\n",
    "                if best_pair[1].startswith(\"##\")\n",
    "                else best_pair[0] + best_pair[1]\n",
    "            )\n",
    "            self.vocab.append(new_token)\n",
    "            print(f\"当前新合并pair：{best_pair}\")\n",
    "            print(f\"当前词表：{self.vocab}\")\n",
    "        self.get_vocab_dict()\n",
    "\n",
    "    def get_vocab_dict(self):\n",
    "        \"\"\"根据id-token的隐射vocab获取反向token-id的映射vocab_dict\"\"\"\n",
    "        vocab_dict = {}\n",
    "        for i in range(len(self.vocab)):\n",
    "            vocab_dict[self.vocab[i]] = i\n",
    "        self.vocab_dict = vocab_dict\n",
    "\n",
    "    def encode_word(self, word):\n",
    "        \"\"\"对单个单词最大正向匹配MM分词\"\"\"\n",
    "        tokens = []\n",
    "        while len(word) > 0:\n",
    "            i = len(word)\n",
    "            # 目标字符串待匹配子串的左端点不变，右侧依次回缩匹配\n",
    "            while i > 0 and word[:i] not in self.vocab:\n",
    "                i -= 1\n",
    "            if i == 0:\n",
    "                return [\"[UNK]\"]\n",
    "            tokens.append(word[:i])\n",
    "            word = word[i:]\n",
    "            if len(word) > 0:\n",
    "                word = f\"##{word}\"\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, sentence):\n",
    "        \"\"\"对整句话进行分词\"\"\"\n",
    "        words = sentence.split()\n",
    "\n",
    "        tokens = []\n",
    "        for word in words:\n",
    "            punc_flag = None\n",
    "            for punc in self.punctuations:\n",
    "                if word.endswith(punc):\n",
    "                    punc_flag = punc\n",
    "                    break\n",
    "            word = word.replace(punc_flag, \"\") if punc_flag else word\n",
    "            tokens.extend(self.encode_word(word))\n",
    "            if punc_flag:\n",
    "                tokens.append(punc_flag)\n",
    "        return [self.vocab_dict[token] for token in tokens]\n",
    "    \n",
    "    def decode(self, input_ids):\n",
    "        tokens = [self.vocab[id] for id in input_ids]\n",
    "        ret_list = \"\"\n",
    "        pre = \"\"\n",
    "        for token in tokens:\n",
    "            if \"#\" not in token:\n",
    "                if pre != \"\":\n",
    "                    ret_list += f\" {pre}\" if ret_list != \"\" else pre\n",
    "                pre = token\n",
    "            else:\n",
    "                pre += token.replace(\"#\", \"\")\n",
    "        if pre != \"\":\n",
    "            if len(pre) == 1:\n",
    "                if 'A' <= pre <= 'Z' or 'a' <= pre <= 'z':\n",
    "                    ret_list += f\" {pre}\"\n",
    "                else:\n",
    "                    ret_list += pre\n",
    "            else:\n",
    "                ret_list += pre\n",
    "        return ret_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "当前词表长度：45\n",
      "当前新合并pair：('a', '##b')\n",
      "当前词表：['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab']\n",
      "####################\n",
      "当前词表长度：46\n",
      "当前新合并pair：('##f', '##u')\n",
      "当前词表：['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu']\n",
      "####################\n",
      "当前词表长度：47\n",
      "当前新合并pair：('F', '##a')\n",
      "当前词表：['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa']\n",
      "####################\n",
      "当前词表长度：48\n",
      "当前新合并pair：('Fa', '##c')\n",
      "当前词表：['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac']\n",
      "####################\n",
      "当前词表长度：49\n",
      "当前新合并pair：('##c', '##t')\n",
      "当前词表：['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct']\n",
      "####################\n",
      "当前词表长度：50\n",
      "当前新合并pair：('##fu', '##l')\n",
      "当前词表：['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful']\n",
      "####################\n",
      "当前词表长度：51\n",
      "当前新合并pair：('##ful', '##l')\n",
      "当前词表：['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full']\n",
      "####################\n",
      "当前词表长度：52\n",
      "当前新合并pair：('##full', '##y')\n",
      "当前词表：['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully']\n",
      "####################\n",
      "当前词表长度：53\n",
      "当前新合并pair：('T', '##h')\n",
      "当前词表：['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully', 'Th']\n",
      "####################\n",
      "当前词表长度：54\n",
      "当前新合并pair：('c', '##h')\n",
      "当前词表：['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully', 'Th', 'ch']\n",
      "####################\n",
      "当前词表长度：55\n",
      "当前新合并pair：('##h', '##m')\n",
      "当前词表：['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully', 'Th', 'ch', '##hm']\n",
      "####################\n",
      "当前词表长度：56\n",
      "当前新合并pair：('ch', '##a')\n",
      "当前词表：['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully', 'Th', 'ch', '##hm', 'cha']\n",
      "####################\n",
      "当前词表长度：57\n",
      "当前新合并pair：('cha', '##p')\n",
      "当前词表：['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully', 'Th', 'ch', '##hm', 'cha', 'chap']\n",
      "####################\n",
      "当前词表长度：58\n",
      "当前新合并pair：('chap', '##t')\n",
      "当前词表：['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully', 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt']\n",
      "####################\n",
      "当前词表长度：59\n",
      "当前新合并pair：('##t', '##hm')\n",
      "当前词表：['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully', 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm']\n",
      "####################\n",
      "当前词表长度：60\n",
      "当前新合并pair：('H', '##u')\n",
      "当前词表：['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully', 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu']\n",
      "####################\n",
      "当前词表长度：61\n",
      "当前新合并pair：('Hu', '##g')\n",
      "当前词表：['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully', 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug']\n",
      "####################\n",
      "当前词表长度：62\n",
      "当前新合并pair：('Hug', '##g')\n",
      "当前词表：['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully', 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg']\n",
      "####################\n",
      "当前词表长度：63\n",
      "当前新合并pair：('s', '##h')\n",
      "当前词表：['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully', 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh']\n",
      "####################\n",
      "当前词表长度：64\n",
      "当前新合并pair：('t', '##h')\n",
      "当前词表：['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully', 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th']\n",
      "####################\n",
      "当前词表长度：65\n",
      "当前新合并pair：('i', '##s')\n",
      "当前词表：['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully', 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is']\n",
      "####################\n",
      "当前词表长度：66\n",
      "当前新合并pair：('##thm', '##s')\n",
      "当前词表：['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully', 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '##thms']\n",
      "####################\n",
      "当前词表长度：67\n",
      "当前新合并pair：('##z', '##a')\n",
      "当前词表：['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully', 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '##thms', '##za']\n",
      "####################\n",
      "当前词表长度：68\n",
      "当前新合并pair：('##za', '##t')\n",
      "当前词表：['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully', 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '##thms', '##za', '##zat']\n",
      "####################\n",
      "当前词表长度：69\n",
      "当前新合并pair：('##u', '##t')\n",
      "当前词表：['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully', 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '##thms', '##za', '##zat', '##ut']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This is the Hugging Face Course.'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = WordPieceTokenizer(vocab_size = 70, corpus = corpus)\n",
    "tokenizer.train()\n",
    "\n",
    "input = \"This is the Hugging Face Course.\"\n",
    "input_ids = tokenizer.encode(input)\n",
    "output = tokenizer.decode(input_ids)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is the Hugging Face Course.'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tkgt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
