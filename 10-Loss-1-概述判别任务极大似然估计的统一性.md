<!-- JPW的Markdown笔记模板 v1, 其中的href需要视情更改上级目录href="../../format.css -->
<link rel="stylesheet" type="text/css" href="../../format.css">


<h1>LLMs系列进阶：判别任务极大似然估计的统一性</h1>

💡 模型学习原理本质上是基于观测样本的参数估计。具体而言，在包含深度学习的整个机器学习语境中，人类希望基于某些方法从数据中习得一些规律或模型，并泛化到类似的任务中，从而实现自动化。这种学习是通过使用**观测**到的样本*数据*来**估计**一个学习模型的*参数*来实现的。

本章将主要包括：
- 机器学习理论中两种学习任务及其相应的参数估计方法
- 判别式任务的优化方法极大似然估计MLE
- 推导MLE优化原则与生成任务损失函数均方误差MSE（最小二乘优化）的等价性
- 推导MLE优化原则与分类任务损失函数交叉熵的等价性

# 1 判别与生成
如何从观测数据中推断最优模型参数呢？核心方法是基于统计进行参数估计，使得观测数据的概率最大化。主要有判别式和生成式两种范式：
- **判别式Discriminative任务**：直接学习条件概率分布，对似然概率P(Y|X)建模
    - 模型：线性回归（回归）、逻辑回归（分类）、感知机、神经网络等。
    - 参数估计方法：极大似然估计MLE，目标是找到使观测值似然函数最大化的参数。
    - 应用任务：在分类和回归任务中如下：
        - 分类：最小交叉熵优化（交叉熵损失），使得两个分布之间的差异最小化，是极大似然估计的特例。例如Decoder-only模型：使用交叉熵损失，目标是最大化下一个词的条件概率。
        - 回归：
            - 最小二乘优化（均方误差MSE损失）使预测值与真实值之间的平方误差最小化，是极大似然估计的特例。$$\text{Loss}=\frac{1}{N}\sum_{i=1}^N(y_i-\hat{y}_i)^2$$
            - 此外也可采用均绝对误差MAE作为损失函数：$$\text{Loss}=\frac{1}{N}\sum_{i=1}^N|y_i-\hat{y}_i|$$
- **生成式Generative任务**：对联合概率P(X,Y)（后验*先验）建模
    - 模型：朴素贝叶斯、使用高斯正太分布连续朴素贝叶斯GDA等
    - 参数估计方法：最大后验概率MAP，目标是找使后验概率最大化的参数。先学习联合P(X,Y)分布，然后用Bayes定理计算条件概率P(Y|X) = P(X, Y)/P(X)，理论上最优的分类器。
    - 应用任务：训练可以生成数据的模型，例如生成对抗网络GAN和隐马尔科夫模型。

由于目前主流的机器学习/深度学习任务都是判别式的，因此下面主要对其进行展开

# 2 判别式任务：最大似然估计MLE
最大似然估计（MLE, Maximum Likelihood Estimation）的优化目标是似然函数最大化，核心在于**已经发生的分布的概率最大化**。例如，我们需要求参数$\theta$的分布，并且有$N$个独立同分布的数据观测点$x_i (i\in[1,N])$，那么有理由相信这些值产生的概率是最大的，因此需要求一个使得这些概率最大的总体分布$\theta$，也就是最大似然函数，如下：
$$P(\theta)=\prod_{i=0}^Np(x_i|\theta)$$

既然这个数据集的分布已经发生了，我们就要让这个似然函数最大化，即对函数求导，在导数为0的点中找最大值。通常的似然函数都在外面套了一个log，这样可以简化等式右边联乘的计算变为连加，同样不影响函数的单调性，所以最大似然函数通常可以写作最小化负对数似然来实现，如下：
$$\log P(\theta)= - \sum_{i=0}^N \log p(x_i|\theta)$$

同时，上式最小化负对数似然函数也符合最小化损失的形式。作为判别式任务中的两大类别，分类与回归分别使用交叉熵和均方误差MSE作为损失函数，并采用最小化交叉熵损失和最小化均方误差（最小二乘法）作为优化目标。而这两种具体的优化方式本质上都遵循极大似然估计的原则。推导如下

# 3 MLE隐含最小二乘优化
最小二乘法LMS（均方误差MSE）是回归任务常用的损失函数优化方法，回归任务的学习模型如下

$$ y^{i} = \theta^Tx^i+\epsilon^i $$

MSE损失函数如下：

$$\text{Loss}=\frac{1}{n}\sum_{i=1}^n(y_i-\hat{y}_i)^2$$

先给出结论，回归任务中对模型权重$\theta$的极大似然估计MLE隐含上述最小二乘法LMS形式。我们设学习模型中$\epsilon^i$是随机噪声的误差项，假设符合高斯分布，可移项如下：

$$ \epsilon^i = y^{i} - \theta^Tx^i \sim N(0, \sigma^2)$$

因此，在知道$y^{i} - \theta^Tx^i$的分布后，可以写出其概率密度函数如下：

$$p(y^{i}|x^i;\theta)=\frac{1}{\sqrt{2\pi\sigma}}exp^{(-\frac{(y^i-\theta^Tx^i)^2}{2\sigma^2})}$$

上式可以视作$\theta$的似然函数（条件概率函数），写作
$$L(\theta) = L(\theta;X;\hat y) = p(\hat y|X;\theta)$$

因此，基于所有n个观测样本的关于$\theta$的似然函数可以写成所有样本似然函数的连乘，如下：
$$L(\theta) = \prod_{i=1}^np(y^{i}|x^i;\theta) = \prod_{i=1}^n\frac{1}{\sqrt{2\pi\sigma}}exp^{(-\frac{(y^i-\theta^Tx^i)^2}{2\sigma^2})}$$

为了便于计算，两边取-log符号（概率0-1范围，取log会变负数）

$$Target_{min} = -\log L(\theta) = -\sum_{i=1}^n\log\frac{1}{\sqrt{2\pi\sigma}}exp^{(-\frac{(y^i-\theta^Tx^i)^2}{2\sigma^2})} = \\
-\log\frac{n}{\sqrt{2\pi\sigma}}+\frac{1}{2\sigma^2}\sum_{i=1}^n(y^i-\theta^Tx^i)^2 = \\

K·\sum_{i=1}^n(y^i-\theta^Tx^i)^2+C
$$

即，最大似然估计在干扰项服从正态分布假设下蕴含最小二乘估计。

# 4 MLE隐含最小交叉熵优化
交叉熵是分类任务常用的损失函数优化方法，对有N个样本的系统而言损失函数如下：
$$ H(p,q) = -\sum_{i=1}^{N}p(y_i)\log q(y_i)$$

也先给出结论，分类任务中对模型权重$\theta$的极大似然估计MLE也隐含上述交叉熵形式。下面给出二分类交叉熵和MLE的推导，首先列出参数$\theta$的极大似然估计：
$$L(\theta) = L(\theta;X;y) = p(y|X;\theta) = \prod_{i=1}^Np(y^i|x^i;\theta)$$

上式代表对于已观测到的一组(y,x)样本（其中y为0或1），我们目前的模型$\theta$需要向着使得条件概率$p(y|x;\theta)$最大化的方向优化。即对于输入x，经过模型计算并经过sigmoid激活后的预测值$h_{\theta}(x^i) = \frac{1}{1+e^{-\theta^Tx^i}}$要最大概率地倾向于$y^i$。这种倾向在二分类任务中可以表示为基于0-1标签的概率选择，如下：
$$h_{\theta}(x^i)^{y^i}(1-h_{\theta}(x^i))^{(1-y^i)}$$
上式代表，当真实标签$y^i=1$时，我们采用模型预测值$h_{\theta}(x^i)$作为概率（前半部分指数为1；后半部分指数为0则全体为1，失效）；否则，当$y^i=0$时，1-模型预测值就是模型预测为0的概率（前半部分指数为0，失效；后半部分指数为1，采用）。因此当前模型在一对样本上展现的预测能力，即条件概率$p(y^i|x^i;\theta)$可以转写为指数选择的形式。基于此，改写极大似然函数如下：
$$L(\theta) =\prod_{i=1}^Nh_{\theta}(x^i)^{y^i}(1-h_{\theta}(x^i))^{(1-y^i)}$$

对等式两边采用负对数，转变为最小化问题
$$-\log L(\theta) = -\sum_{i=1}^N \log h_{\theta}(x^i)^{y^i}(1-h_{\theta}(x^i))^{(1-y^i)} = \\
-\sum_{i=1}^N y^i\log h_{\theta}(x^i) + (1-y^i)\log (1-h_{\theta}(x^i))
$$
上式已经具备交叉熵形式，其中log内的$h_{\theta}(x^i)$即表示当前模型的预测值，log外的$y^i$表示标签值，在二分类问题中，$y^i$要么等于0要么等于1，因此可以简化如下
$$-\log L(\theta) = -\sum_{i=1}^N p(y^i)\log q(\hat y^i)$$
