{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用反向KL散度作为损失函数，并使用beta系数\n",
    "\n",
    "import torch\n",
    "from typing import Tuple\n",
    "\n",
    "def dpo_loss(\n",
    "        self,\n",
    "        policy_chosen_logps: torch.FloatTensor,\n",
    "        policy_rejected_logps: torch.FloatTensor,\n",
    "        reference_chosen_logps: torch.FloatTensor,\n",
    "        reference_rejected_logps: torch.FloatTensor,\n",
    "    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n",
    "        \"\"\"传入策略模型和参考模型对chosen和reject的log后的概率logits/标量，返回batch中每个实例的损失值(shape = (batch_size))、chosen_rewards, rejected_rewards。代码中支持多种损失类型，每种类型对应不同的优化目标和数学公式。\n",
    "        Args:\n",
    "            policy_chosen_logps: Log probabilities of the policy model for the chosen responses. Shape: (batch_size,)\n",
    "            policy_rejected_logps: Log probabilities of the policy model for the rejected responses. Shape: (batch_size,)\n",
    "            reference_chosen_logps: Log probabilities of the reference model for the chosen responses. Shape: (batch_size,)\n",
    "            reference_rejected_logps: Log probabilities of the reference model for the rejected responses. Shape: (batch_size,)\n",
    "        \"\"\"\n",
    "\n",
    "        # （1）计算了策略模型和参考模型对选择的响应和未选择的响应的对数概率比（log ratios），即损失函数中各自的比值，用于后续的损失函数计算\n",
    "        chosen_logratios = policy_chosen_logps.to(self.accelerator.device) - (\n",
    "            not self.reference_free\n",
    "        ) * reference_chosen_logps.to(self.accelerator.device)\n",
    "        rejected_logratios = policy_rejected_logps.to(self.accelerator.device) - (\n",
    "            not self.reference_free\n",
    "        ) * reference_rejected_logps.to(self.accelerator.device)\n",
    "\n",
    "        # （2）将上面两者相减并对KL散度约束进行设置：实现了两种不同的 f-散度（f-divergence）计算方式（KL散度是F-散度的一种特例），用于优化策略模型（policy model）的输出，使其更符合人类偏好\n",
    "        if self.f_divergence_type == FDivergenceType.ALPHA_DIVERGENCE.value:\n",
    "            # The alpha-divergence formula: (1 - u^-alpha) / alpha\n",
    "            # The divergence difference between the chosen and rejected sample is:\n",
    "            #     (1 - u[w]^-alpha) / alpha - (1 - u[l]^-alpha) / alpha\n",
    "            #        = (u[l]^-alpha - u[w]^-alpha) / alpha\n",
    "            # where u[w] and u[l] are the policy/reference probability ratios\n",
    "            # for the chosen and rejected samples, respectively.\n",
    "            alpha_coef = FDivergenceConstants.ALPHA_DIVERGENCE_COEF_DEFAULT\n",
    "            if self.f_divergence_params and FDivergenceConstants.ALPHA_DIVERGENCE_COEF_KEY in self.f_divergence_params:\n",
    "                alpha_coef = float(self.f_divergence_params[FDivergenceConstants.ALPHA_DIVERGENCE_COEF_KEY])\n",
    "            logits = (cap_exp(rejected_logratios * -alpha_coef) - cap_exp(chosen_logratios * -alpha_coef)) / alpha_coef\n",
    "        else:\n",
    "            pi_logratios = policy_chosen_logps - policy_rejected_logps\n",
    "            if self.reference_free:\n",
    "                ref_logratios = torch.tensor([0], dtype=pi_logratios.dtype, device=pi_logratios.device)\n",
    "            else:\n",
    "                ref_logratios = reference_chosen_logps - reference_rejected_logps\n",
    "\n",
    "            pi_logratios = pi_logratios.to(self.accelerator.device)\n",
    "            ref_logratios = ref_logratios.to(self.accelerator.device)\n",
    "            logits = pi_logratios - ref_logratios\n",
    "\n",
    "            if self.f_divergence_type == FDivergenceType.JS_DIVERGENCE.value:\n",
    "                # The js-divergence formula: log(2 * u / (1 + u))\n",
    "                # The divergence difference between the chosen and rejected sample is:\n",
    "                #     log(2 * u[w] / (1 + u[w])) - log(2 * u[l] / (1 + u[l]))\n",
    "                #       = log(u[w]) - log(u[l]) - (log(1 + u[w]) - log(1 + u[l]))\n",
    "                # where u[w] and u[l] are the policy/reference probability ratios\n",
    "                # for the chosen and rejected samples, respectively.\n",
    "                logits -= F.softplus(chosen_logratios) - F.softplus(rejected_logratios)\n",
    "\n",
    "        # （3）使用不同的损失激活计算方式：sigmoid\n",
    "        # The beta is a temperature parameter for the DPO loss, typically something in the range of 0.1 to 0.5.\n",
    "        # We ignore the reference model as beta -> 0. The label_smoothing parameter encodes our uncertainty about the labels and\n",
    "        # calculates a conservative DPO loss.\n",
    "        if self.loss_type == \"sigmoid\":\n",
    "            losses = (\n",
    "                -F.logsigmoid(self.beta * logits) * (1 - self.label_smoothing)\n",
    "                - F.logsigmoid(-self.beta * logits) * self.label_smoothing\n",
    "            )\n",
    "        elif self.loss_type == \"robust\":\n",
    "            losses = (\n",
    "                -F.logsigmoid(self.beta * logits) * (1 - self.label_smoothing)\n",
    "                + F.logsigmoid(-self.beta * logits) * self.label_smoothing\n",
    "            ) / (1 - 2 * self.label_smoothing)\n",
    "        elif self.loss_type == \"exo_pair\":\n",
    "            # eqn (16) of the EXO paper: https://arxiv.org/pdf/2402.00856\n",
    "            import math\n",
    "\n",
    "            if self.label_smoothing == 0:\n",
    "                self.label_smoothing = 1e-3\n",
    "            losses = (self.beta * logits).sigmoid() * (\n",
    "                F.logsigmoid(self.beta * logits) - math.log(1 - self.label_smoothing)\n",
    "            ) + (-self.beta * logits).sigmoid() * (F.logsigmoid(-self.beta * logits) - math.log(self.label_smoothing))\n",
    "        elif self.loss_type == \"hinge\":\n",
    "            losses = torch.relu(1 - self.beta * logits)\n",
    "        elif self.loss_type == \"ipo\":\n",
    "            # eqn (17) of the paper where beta is the regularization parameter for the IPO loss, denoted by tau in the paper.\n",
    "            losses = (logits - 1 / (2 * self.beta)) ** 2\n",
    "        elif self.loss_type == \"bco_pair\":\n",
    "            chosen_logratios = policy_chosen_logps - reference_chosen_logps\n",
    "            rejected_logratios = policy_rejected_logps - reference_rejected_logps\n",
    "\n",
    "            chosen_rewards = self.beta * chosen_logratios\n",
    "            rejected_rewards = self.beta * rejected_logratios\n",
    "            rewards = torch.cat((chosen_rewards, rejected_rewards), 0).mean().detach()\n",
    "            self.running.update(rewards)\n",
    "            delta = self.running.mean\n",
    "\n",
    "            losses = -F.logsigmoid((self.beta * chosen_logratios) - delta) - F.logsigmoid(\n",
    "                -(self.beta * rejected_logratios - delta)\n",
    "            )\n",
    "        elif self.loss_type == \"sppo_hard\":\n",
    "            # In the paper (https://arxiv.org/pdf/2405.00675), SPPO employs a soft probability approach, estimated using the PairRM score. The probability calculation is conducted outside of the trainer class. The version described here is the hard probability version, where P in Equation (4.7) of Algorithm 1 is set to 1 for the winner and 0 for the loser.\n",
    "            a = policy_chosen_logps - reference_chosen_logps\n",
    "            b = policy_rejected_logps - reference_rejected_logps\n",
    "\n",
    "            losses = (a - 0.5 / self.beta) ** 2 + (b + 0.5 / self.beta) ** 2\n",
    "        elif self.loss_type == \"nca_pair\":\n",
    "            chosen_rewards = (policy_chosen_logps - reference_chosen_logps) * self.beta\n",
    "            rejected_rewards = (policy_rejected_logps - reference_rejected_logps) * self.beta\n",
    "            losses = (\n",
    "                -F.logsigmoid(chosen_rewards)\n",
    "                - 0.5 * F.logsigmoid(-chosen_rewards)\n",
    "                - 0.5 * F.logsigmoid(-rejected_rewards)\n",
    "            )\n",
    "        elif self.loss_type == \"aot_pair\":\n",
    "            chosen_logratios = policy_chosen_logps - reference_chosen_logps\n",
    "            rejected_logratios = policy_rejected_logps - reference_rejected_logps\n",
    "\n",
    "            chosen_logratios_sorted, _ = torch.sort(chosen_logratios, dim=0)\n",
    "            rejected_logratios_sorted, _ = torch.sort(rejected_logratios, dim=0)\n",
    "\n",
    "            delta = chosen_logratios_sorted - rejected_logratios_sorted\n",
    "\n",
    "            losses = (\n",
    "                -F.logsigmoid(self.beta * delta) * (1 - self.label_smoothing)\n",
    "                - F.logsigmoid(-self.beta * delta) * self.label_smoothing\n",
    "            )\n",
    "\n",
    "        elif self.loss_type == \"aot\":\n",
    "            pi_logratios = policy_chosen_logps - policy_rejected_logps\n",
    "            ref_logratios = reference_chosen_logps - reference_rejected_logps\n",
    "\n",
    "            pi_logratios_sorted, _ = torch.sort(pi_logratios, dim=0)\n",
    "            ref_logratios_sorted, _ = torch.sort(ref_logratios, dim=0)\n",
    "\n",
    "            delta = pi_logratios_sorted - ref_logratios_sorted\n",
    "\n",
    "            losses = (\n",
    "                -F.logsigmoid(self.beta * delta) * (1 - self.label_smoothing)\n",
    "                - F.logsigmoid(-self.beta * delta) * self.label_smoothing\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unknown loss type: {self.loss_type}. Should be one of ['sigmoid', 'hinge', 'ipo', 'bco_pair', 'sppo_hard', 'nca_pair', 'robust', 'exo_pair']\"\n",
    "            )\n",
    "\n",
    "        # （4）计算了选择的响应（chosen responses）和未选择的响应（rejected responses）的奖励值：policy/ref\n",
    "        chosen_rewards = (\n",
    "            self.beta\n",
    "            * (\n",
    "                policy_chosen_logps.to(self.accelerator.device) - reference_chosen_logps.to(self.accelerator.device)\n",
    "            ).detach()\n",
    "        )\n",
    "        rejected_rewards = (\n",
    "            self.beta\n",
    "            * (\n",
    "                policy_rejected_logps.to(self.accelerator.device)\n",
    "                - reference_rejected_logps.to(self.accelerator.device)\n",
    "            ).detach()\n",
    "        )\n",
    "\n",
    "        return losses, chosen_rewards, rejected_rewards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
