<!-- JPWçš„Markdownç¬”è®°æ¨¡æ¿ v1, å…¶ä¸­çš„hreféœ€è¦è§†æƒ…æ›´æ”¹ä¸Šçº§ç›®å½•href="../../format.css -->
<link rel="stylesheet" type="text/css" href="../../format.css">


<h1>LLMsç³»åˆ—è¿›é˜¶ï¼šæ—‹è½¬ä½ç½®ç¼–ç RoPE</h1>

ğŸ’¡ ç”±äºAttentionç»“æ„çš„æ ¸å¿ƒåœ¨äºè®¡ç®—æœ€æ–°æŸ¥è¯¢Tokenå¯¹æ‰€æœ‰é”®Tokençš„æ³¨æ„åŠ›ï¼Œè€Œè¿™ä¸€å¯å¹¶è¡ŒåŒ–çš„è®¡ç®—å…·æœ‰æ’åˆ—ä¸å˜æ€§ï¼Œå³äº¤æ¢tokençš„ä½ç½®åè®¡ç®—ç»“æœä¸å˜ã€‚æ’åˆ—ä¸å˜æ€§è™½ç„¶ä½¿å¾—attentionè®¡ç®—å¯ä»¥é«˜åº¦å¹¶è¡ŒåŒ–ï¼Œä½†ä¸¢å¤±äº†tokenè¯­åºä¿¡æ¯ã€‚ä¸ºäº†ä¿ç•™å¯å¹¶è¡Œç‰¹ç‚¹åŒæ—¶ä¿ç•™è¯­ä¹‰ä¿¡æ¯ï¼Œéœ€è¦å¯¹tokenè¿›è¡Œä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰ï¼Œæˆ–è€…è¯´ä½ç½®æ€§çš„ç¼–ç ã€‚

æœ¬ç« çš„é‡ç‚¹æ˜¯RoPEåŠå…¶æµç¨‹åŸç†
```python
# 1 é¢„å…ˆæ„å»ºç¼–ç å‘é‡
## 1.1 åˆå§‹åŒ–thetaåºåˆ—ï¼š
inv_freq = 1 / 10000**(torch.arange(0, self.qk_dim, 2)/ qk_dim)
## 1.2 position_idsåºåˆ—
seq_idx = torch.arange(seq_len)
## 1.3 å¤–ç§¯
idx_theta = torch.outer( seq_idx, theta).float()
## 1.4 ç¼–ç å½¢æˆcos/sinä¸€ç»„
cache = torch.stack( [torch.cos(idx_theta), torch.sin(idx_theta)], dim=-1)

# 2 åº”ç”¨
## 2.1 è¾“å…¥reshape
xshaped = X.reshape( batch_size, num_heads, seq_len, qk_dim // 2)
## 2.2 ç¼–ç é‡reshape
rope_cache.reshape( 1, 1, seq_len, qk_dim // 2, 2 )
## 2.3 æ—‹è½¬
x_out2 = torch.stack(
    [
        xshaped[..., 0] * rope_cache[..., 0] - xshaped[..., 1] * rope_cache[..., 1],
        xshaped[..., 1] * rope_cache[..., 0] + xshaped[..., 0] * rope_cache[..., 1]
    ],
    dim = -1
)
## 2.4 å±•å¹³å¹¶reshape
x_out2 = x_out2.flatten(3).reshape( batch_size, num_heads, seq_len, qk_dim)
```


# 1. ä½ç½®ç¼–ç çš„ç±»å‹
ä½ç½®ç¼–ç çš„åŸç†æ˜¯å¯¹è¾“å…¥åºåˆ—ä¸­çš„tokenè¡¨ç¤ºè¿›è¡Œä½ç½®ä¿¡æ¯ï¼ˆç»å¯¹æˆ–ç›¸å¯¹ï¼‰çš„æ³¨å…¥ã€‚ä½ç½®ç¼–ç ç»å†äº†èŒƒå¼ä¸Šçš„ç¼–ç ï¼Œé€æ¸å‘å±•æˆå¦‚ä»Šé€‚ç”¨äºTransformerçš„ç¼–ç ï¼Œå…·ä½“å¯å‚è§[***Transformerå­¦ä¹ ç¬”è®°ä¸€ï¼šPositional Encodingï¼ˆä½ç½®ç¼–ç ï¼‰ - çŒ›çŒ¿çš„æ–‡ç«  - çŸ¥ä¹***](https://zhuanlan.zhihu.com/p/454482273)ã€‚ç®€å•æ¥è¯´ï¼Œä½ç½®ç¼–ç å¯ä»¥åˆ†ä¸ºç»å¯¹å’Œç›¸å¯¹ä¸¤ç§ï¼š

## 1.1 ç»å¯¹ä½ç½®ç¼–ç 
å¯¹embeddingå‘é‡è¿›è¡Œä½ç½®ç¼–ç ï¼Œæœ‰å¦‚ä¸‹å‡ ç§ç±»å‹ï¼š
- **ç”¨æ•´å‹å€¼æ ‡è®°ä½ç½®**ï¼šè¾ƒä¸ºé€šç”¨ä¸”ç›´è§‚ï¼Œä½†æ— ç•Œä¸”é‡çº²ä¸Šä¸æµ®ç‚¹éšè—çŠ¶æ€ä¸åŒ¹é…ï¼Œä¸é€‚ç”¨äºAttentionã€‚
- **ç”¨[0,1]èŒƒå›´æµ®ç‚¹æ•°è¡¨ç¤º**ï¼šå¦‚Learned Positional Embedding(å­¦ä¹ å¼ç¼–ç )ï¼Œæœºæ¢°åˆ‡åˆ†ï¼Œæ— æ³•å¤–æ¨ã€‚æ—©æœŸçš„BERT, Roberta, GPT, GPT2ä½¿ç”¨çš„å°±æ˜¯è¿™ç§ï¼Œæœ€æ—©ä½¿ç”¨åˆ™å¯æº¯æºè‡³2017å¹´Facebookçš„ä¸€ç¯‡è®ºæ–‡Convolutional sequence to sequence learningã€‚TransformeråŸå§‹è®ºæ–‡ä¸­ä¹Ÿæåˆ°äº†è¿™ä¸ªæ–¹æ³•å¹¶ç”¨äºå¯¹æ¯”å®éªŒï¼Œå®éªŒç»“æœæ˜¯Transformerçš„ä½ç½®ç¼–ç æ–¹æ¡ˆï¼ˆSinusoidalï¼‰ä¸è¿™ç§å¯å­¦ä¹ çš„æ–¹æ¡ˆçš„ç»“æœå·®ä¸å¤šï¼Œä½†è¿™ç§æ–¹æ¡ˆæ— æ³•å¤–æ¨extrapolateè‡³è¶…è¿‡æœ€å¤§å¯ç¼–ç é•¿åº¦çš„åºåˆ—ï¼ˆç›®å‰ä¹Ÿæœ‰ç ”ç©¶è¡¨æ˜å¯ä»¥é€šè¿‡å…¶ä»–æ–¹å¼è®©å­¦ä¹ å¼ç¼–ç å…·å¤‡å¤–æ¨çš„èƒ½åŠ›ï¼‰ã€‚
- **äºŒè¿›åˆ¶ç¼–ç **ï¼šç”¨nä½äºŒè¿›åˆ¶çš„éšè—çŠ¶æ€å°±å¯ä»¥è¡¨ç¤º$2^n$ä¸ªè¯ï¼Œä½†ç»´åº¦ä¸Šä¸Embeddingä¸åŒ¹é…ä¼šæœ‰ç©ºå€¼ï¼Œä¸”ç¦»æ•£
- **ç”¨å‘¨æœŸå‡½æ•°å¦‚ä¸‰è§’å‡½æ•°sin**ï¼šå¦‚Sinusoidalæ­£å¼¦æ›²çº¿ç¼–ç ï¼ŒèŒƒå›´å›ºå®š[-1,1]æµ®ç‚¹æ•°ï¼Œè¿ç»­ï¼Œå¯å¤–æ¨ï¼Œä¸‰è§’å‡½æ•°å’Œå·®åŒ–ç§¯ã€ç§¯åŒ–å’Œå·®çš„ç‰¹æ€§å¯ä»¥ç¼–ç ç›¸å¯¹ä½ç½®ä¿¡æ¯ã€‚ä½†è¿™ç§è¡¨ç¤ºæ˜¯éçº¿æ€§çš„ï¼Œæœ‰å¾…æ”¹è¿›ã€‚å®é™…ä¸Šè¿™ä¸ªæ–¹æ³•å¹¶æ²¡æœ‰å¾ˆä¼˜è¶Šï¼Œåªæœ‰Transformer, Transformer XLåœ¨ä½¿ç”¨ã€‚

$$
sin(x+\Delta x)=\sin(x)\cos(\Delta x)+\cos(x)\sin(\Delta x)=[
\begin{matrix}
  \sin x & \cos x 
\end{matrix}]
[\begin{matrix}
  \cos \Delta x  \\ \sin \Delta x
\end{matrix}]
$$

## 1.2 ç›¸å¯¹ä½ç½®ç¼–ç 
å¯¹qkå‘é‡è¿›è¡Œä½ç½®ç¼–ç ï¼Œå¦‚ä¸‹ä¸¤ç§ç±»å‹ä¸ºä¸»ï¼š
- **ä¸‰è§’å‡½æ•°åæ ‡ç³»ï¼ˆä¸€å¯¹sinå’Œcosï¼‰**ï¼šå¦‚RoPEæ—‹è½¬ä½ç½®ç¼–ç ï¼Œå…·å¤‡ä»¥ä¸Šæ‰€æœ‰ä¼˜ç‚¹ï¼Œä¸»æµå¤§æ¨¡å‹ä½¿ç”¨ã€‚ä¸ºäº†è¡¨ç¤ºçº¿æ€§å˜æ¢ï¼Œå¯ä»¥å¼•å…¥**ä¸‰è§’å‡½æ•°åæ ‡ç³»**ï¼Œå°†è¾“å…¥çš„xç†è§£ä¸ºè§’åº¦$\theta$ã€‚é‚£ä¹ˆç›´è§’åæ ‡ç³»ä¸­çš„ä¸€ä¸ªä½ç½®å°±å¯ä»¥å†™ä½œä¸€å¯¹æ­£ä½™å¼¦ä¸‰è§’å‡½æ•°ã€‚
$[\begin{matrix}
  \sin x \\ \cos x 
\end{matrix}]$
æ­¤æ—¶å…¶ç»è¿‡ä¸€ä¸ªçº¿æ€§å˜æ¢å¯ä»¥è¡¨ç¤ºä»»æ„ç›¸å¯¹ä½ç½®çš„æ—‹è½¬ï¼ˆå³$x+\Delta x$è§†ä½œ$x$çš„æ—‹è½¬ï¼‰ï¼Œ$[
\begin{matrix}
  \sin x +\Delta x\\ \cos x+\Delta x 
\end{matrix}]$ã€‚

- **ç®€åŒ–ç¼–ç **ï¼šå¦‚ALibiï¼Œåœ¨Bloomã€Falconã€MPTã€‚ç›´æ¥å¯¹tokenä¹‹é—´çš„è·ç¦»æ–½åŠ æƒ©ç½šï¼ˆä¸‹ä¸‰è§’ï¼‰ï¼Œä¾‹å¦‚ä¸¤ä¸ªtokenç›¸éš”ä¸€ä½åˆ™åœ¨attn_weightsä¸Š-1ã€‚å¯¹multi-headæƒ…å†µåˆ™è®¾è®¡å¡åº¦ç³»æ•°å¦‚ä¸‹ï¼š
$$
m=
\begin{Bmatrix}
  2^\frac{-8}{i}, i\in [1,2,..., n]
\end{Bmatrix}
$$

# 2. RoPEä»£ç 
åŸå§‹è®ºæ–‡ä¸­çš„æ—‹è½¬æ€è·¯æ˜¯å¯¹åµŒå…¥ç»´åº¦çš„æ¯å¯¹ç›¸é‚»ç»´åº¦åˆ†é‡è¿›è¡Œæ—‹è½¬ï¼ˆ2iå’Œ2i+1ï¼Œå…¬å¼å¦‚ä¸‹ï¼š
$$
\begin{bmatrix} 
    e'_{2i}, e'_{2i+1}
\end{bmatrix} = 

\begin{bmatrix} 
    cos(\theta_i), -sin(\theta_i) \\
    sin(\theta_i), cos(\theta_i)
\end{bmatrix} Â·
\begin{bmatrix} 
    e_{2i} \\ e_{2i+1}
\end{bmatrix}

$$
ä¸Šå¼çš„æ ¸å¿ƒåœ¨äºå¯¹ç¼–ç åçš„å¶æ•°ä½ä½¿ç”¨-sinã€‚ä½†å®ç°å±‚é¢ç›®å‰ä¸»æµçš„æ—‹è½¬å¯¹è±¡æ˜¯å¯¹åµŒå…¥ç»´åº¦æ•´ä½“å¯¹åŠæ—‹è½¬ï¼ˆiå’Œd/2+iä¸€å¯¹ï¼‰ã€‚å‰è€…çš„å®ç°æœ‰GLMï¼Œåè€…æœ‰Llamaå’ŒMistralã€‚

## 2.1 ç›¸é‚»ç»´åº¦æˆå¯¹æ—‹è½¬
<p align="center">
  <img src="images/rotary_æ­£å¸¸.png" width=60%><br>
</p>

```python
import torch
import torch.nn as nn

class RotaryEmbedding( nn.Module ):
    """å³ç”¨å³ç®—"""
    def __init__(self, qk_dim, device = None, dtype = None ):
        """param qk_dim: Queryå’ŒKeyçš„ç»´åº¦"""
        super().__init__()
        self.dim = qk_dim
        inv_freq = 1.0 / (10000 ** (torch.arange(0, qk_dim, 2, device=device, dtype=dtype ) / qk_dim ))
        self.register_buffer( "inv_freq", inv_freq)

    def forward_impl( self, seq_len, n_elem, device, dtype, base = 10000 ):
        theta = 1.0 / (10000 ** (torch.arange(0, n_elem, 2, device=device, dtype=dtype ) / n_elem ))
        seq_idx = torch.arange( seq_len, dtype=torch.float, device=device )
        idx_theta = torch.outer( seq_idx, theta).float()
        # å°†cos, sinåŒ…è£…ä¸ºç›¸é‚»æˆå¯¹
        cache = torch.stack( [torch.cos(idx_theta), torch.sin(idx_theta)], dim=-1)
        return cache
    
    def forward( self, max_len ):
        return self.forward_impl(
            max_seq_len, self.dim, dtype=self.inv_freq.dtype, device=self.inv_freq.device
        )


def apply_rotary_pos_emb( X, rope_cache):
    """
    param X: [batch_size, num_heads, seq_len, qk_dim]
    param rope_cache: [seq_len, qk_dim//2, 2]ï¼Œcoså’Œsinçš„ç¼–ç 
    [X2i, X2i+1] * [[ cos2i, -sin2i ]
                    [ sin2i, cos2i ]] = [RoPE_X_2i, RoPE_X_2i+1]
    """
    batch_size, num_heads, seq_len, qk_dim = X.shape
    # å°†æœ€åä¸€ä¸ªç»´åº¦æ‹†åˆ†ä¸ºqk_dim // 2ä¸ªä¸¤ä¸¤ä¸€ç»„çš„ç›¸é‚»ç»´åº¦
    xshaped = X.reshape( batch_size, num_heads, seq_len, qk_dim // 2, 2)
    rope_cache = rope_cache.reshape( 1, 1, seq_len, qk_dim // 2, 2 )
    # æ—‹è½¬ä½ç½®ç¼–ç ï¼šX2i = cos2i*X2i - sin2i*X2i+1, X2i+1 = cos2i*X2i+1 + sin2i*X2i
    x_out2 = torch.stack(
        [
            xshaped[..., 0] * rope_cache[..., 0] - xshaped[..., 1] * rope_cache[..., 1],
            xshaped[..., 1] * rope_cache[..., 0] + xshaped[..., 0] * rope_cache[..., 1]
        ],
        dim = -1
    )
    # ä»ç¬¬ä¸‰ç»´åº¦å¼€å§‹ï¼Œå°†x_out2çš„æœ€åä¸€ä¸ªç»´åº¦åˆå¹¶åˆ°ç¬¬äºŒä¸ªç»´åº¦ï¼Œå³[batch_size, num_heads, seq_len, qk_dim]
    x_out2 = x_out2.flatten(3)
    return x_out2.reshape( batch_size, num_heads, seq_len, qk_dim)

```


## 2.2 ç»´åº¦æ•´ä½“å¯¹åŠæ—‹è½¬

<p align="center">
  <img src="images/rotary_hf.png" width=60%><br>
</p>


```python
import torch
import torch.nn as nn
class MistralRotaryEmbedding(nn.Module):
    """Mistralå’ŒLlamaç­‰æ¨¡å‹çš„æ—‹è½¬ä½ç½®ç¼–ç ï¼Œä¸é‡‡ç”¨è®ºæ–‡ç›¸é‚»ç»´åº¦æ—‹è½¬ï¼Œè€Œæ˜¯ç»´åº¦æ•´ä½“å¯¹åŠæ—‹è½¬"""
    def __init__(self, qk_dim, max_positions = 151643, base = 10000, device = None):
        super().__init__()
        self.dim = qk_dim
        self.max_postions = max_positions
        self.max_seq_len_cached = None
        self.base = base
        inv_freq = 1.0 / ( self.base ** (torch.arange( 0, self.dim, 2, dtype = torch.int64).float().to(device) / self.dim))
        # register_bufferä¸»è¦æ˜¯ä¸ºäº†ä¸éœ€è¦æ¢¯åº¦æ›´æ–°çš„å¼ é‡èƒ½è¢«éšç€torch.saveä¸€èµ·è¢«ä¿å­˜è®¾è®¡çš„ã€‚è€ƒè™‘åˆ°ä¸€å¼€å§‹persistent=Trueï¼ˆå³inv_freqä¼šè¢«ä¸€èµ·ä¿å­˜ï¼‰ï¼Œæˆ‘è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªé—ç•™çš„ä»£ç ä¹ æƒ¯ã€‚
        self.register_buffer( "inv_freq", inv_freq, persistent = False)

        self._set_cos_sin_cache( max_positions, self.inv_freq.device, self.inv_freq.dtype )
    
    def _set_cos_sin_cache( self, max_seq_len, device, dtype ):
        self.max_seq_len_cached = max_seq_len
        max_index = torch.arange( 0, self.max_seq_len_cached, device = device, dtype = torch.int64 )
        pos_freq = torch.outer( max_index, self.inv_freq )
        emb = torch.cat( pos_freq, pos_freq, dim = -1)
        self.register_buffer( "cos_cached", emb.cos().to(dtype), persistent=False )
        self.register_buffer( "sin_cached", emb.sin().to(dtype), persistent=False )

    def forward( self, X, seq_len = None):
        if seq_len == None:
            seq_len = 1
        elif seq_len > self.max_seq_len_cached:
            self._set_cos_sin_cache( seq_len, device=X.device, dtype=X.dtype )
        return (
            self.cos_cached[:seq_len].to(X.dtype),
            self.sin_cached[:seq_len].to(X.dtype)
        )

def rotate_half( x ):
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2: ]
    return torch.cat( (-x2, x1), dim = -1 )

def apply_rotary_pos_emb( Q, K, cos, sin, position_ids, unsqueeze_dim = 1):
    """
    Q/K.shape: [batch_size, num_heads, seq_len, qk_dim]
    cos/sin.shape: [seq_len, qk_dim]
    position_ids.shape: [batch_size, seq_len]
    """
    # å°†åŸå§‹cos/sinå˜å½¢ä¸ºç¬¦åˆè¾“å…¥çš„å½¢çŠ¶# [batch_size, 1, seq_len, qk_dim]
    cos = cos[position_ids].unsqueeze( unsqueeze_dim )
    sin = sin[position_ids].unsqueeze( unsqueeze_dim )
    # é€ä½ä¹˜ï¼Œç¬¬äºŒä¸ªç»´åº¦è‡ªåŠ¨å¹¿æ’­ã€‚æ­¤å¤„Rotaryå¹¶éåŸå§‹è®ºæ–‡ç›¸é‚»ç»´åº¦æ—‹è½¬ï¼Œè€Œæ˜¯æ•´ä¸ªå‘é‡å¯¹åŠæ—‹è½¬
    Q_emb = Q * cos + rotate_half(Q) * sin
    K_emb = K * cos + rotate_half(K) * sin
    return Q_emb, K_emb
```


