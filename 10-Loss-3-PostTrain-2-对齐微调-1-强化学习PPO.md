<!-- JPWçš„Markdownç¬”è®°æ¨¡æ¿ v1, å…¶ä¸­çš„hreféœ€è¦è§†æƒ…æ›´æ”¹ä¸Šçº§ç›®å½•href="../../format.css -->
<link rel="stylesheet" type="text/css" href="../../format.css">


<h1>LLMsç³»åˆ—è¿›é˜¶ï¼šå¯¹é½å¾®è°ƒä¹‹å¼ºåŒ–å­¦ä¹ PPO</h1>

ğŸ’¡ SFTæŒ‡ä»¤å¾®è°ƒå¯ä»¥è®©é¢„è®­ç»ƒå¤§æ¨¡å‹åœ¨ç‰¹å®šä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½ï¼Œä½†å­˜åœ¨è¾ƒä¸ºæ˜æ˜¾çš„ç¼ºç‚¹ï¼Œå³æ— æ³•å­¦ä¹ è´Ÿåé¦ˆã€‚å¯¹é½å¾®è°ƒï¼ˆAlignment Fine-Tuningï¼‰åº”è¿è€Œç”Ÿã€‚ç›®å‰ä¸»æµçš„å¯¹é½å¾®è°ƒæ–¹å¼åˆ†ä¸ºå¼ºåŒ–å­¦ä¹ èŒƒå¼ï¼ˆPPOï¼‰å’Œéå¼ºåŒ–å­¦ä¹ èŒƒå¼ï¼ˆDPOç­‰ï¼‰ä¸¤ç§ã€‚

# 1 SFTçš„ä¸è¶³ä¸PPOæ¦‚è¿°

å…·ä½“è€Œè¨€ï¼Œåœ¨SFTé˜¶æ®µï¼Œæ¨¡å‹è¿˜æ˜¯åœ¨å­¦ä¹ æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒï¼Œå³åº”è¯¥æ€æ ·ç”Ÿæˆï¼Œä½†ä¸çŸ¥é“ä»€ä¹ˆtokenä¸èƒ½ç”Ÿæˆã€‚ä½ è¶Šæ˜¯å¸Œæœ›å‘Šè¯‰å®ƒä»€ä¹ˆæ˜¯é”™è¯¯ï¼Œè¶Šæœ‰å¯èƒ½è¢«ç”Ÿæˆã€‚
- æ‰€æœ‰input-outputå¯¹å¾®è°ƒæœ¬è´¨ä¸Šéƒ½åœ¨åŠ å¼ºæ¨¡å‹åˆ†å¸ƒæ‹Ÿåˆoutputåˆ°inputï¼Œå¯ä»¥ç†è§£ä¸º**æ’ä¸ºæ­£åé¦ˆ**ã€‚
- ä¾æ—§é‡‡ç”¨é¢„è®­ç»ƒé˜¶æ®µçš„Next tokençš„æŸå¤±å‡½æ•°ï¼Œå³**tokenåªèƒ½å‘å‰çœ‹**ã€‚è¿™å°±å¯¼è‡´åœ¨è®­ç»ƒ"ä½ æ˜¯å­¦ç”Ÿï¼Œè¯¥æœ‰å¤šå¥½å•Š"è¿™ç§å¥å­æ—¶ï¼Œ"ä½ æ˜¯å­¦ç”Ÿ"æ°¸è¿œä¼šè¢«æ­£å‘å­¦ä¹ ã€‚
- æŸå¤±å‡½æ•°ä¸ºç”Ÿæˆåºåˆ—ä¸­tokençš„æŸå¤±å€¼æ˜¯å¹³å‡è€ŒéåŠ æƒï¼Œå¯¹äºéƒ¨åˆ†é”™è¯¯çš„æ–‡æœ¬æ— æ³•é€‚å½“åˆ†é…æ³¨æ„åŠ›ã€‚

å¯¹æ­¤ï¼ŒOpenAIé¦–å…ˆæå‡ºä½¿ç”¨RLHFï¼ˆåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å¾®è°ƒï¼‰ä½œä¸ºå¯¹é½å¾®è°ƒæ‰‹æ®µã€‚å…·å¤‡ä»¥ä¸‹ä¼˜åŠ¿ï¼š
- æ¯æ¬¡å­¦ä¹ ä¸€ä¸ªä¸‰å…ƒç»„$(x, y_w, y_l)$è¡¨ç¤ºçš„äººç±»åå¥½ï¼Œè€ŒéäºŒå…ƒç»„è¡¨ç¤ºçš„æ­£ç¡®è¾“å‡ºã€‚å…¶ä¸­$y_w$å’Œ$y_l$åˆ†åˆ«è¡¨ç¤ºwinnerå’Œloserã€‚
- æ‰€æœ‰tokençš„æŸå¤±å€¼æ˜¯åŠ æƒå¹³å‡ï¼Œè€Œéå¹³å‡çš„ï¼Œå¯ä»¥å¼•å¯¼æ¨¡å‹å…‹æœå±€éƒ¨ç›¸å…³ã€‚

åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¯¹é½å¾®è°ƒé€šå¸¸ä½¿ç”¨ äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHF, Reinforcement Learning from Human Feedbackï¼‰ æ¡†æ¶ï¼Œå…¶ä»£è¡¨æ–¹æ³•æ˜¯è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–PPOï¼ˆProximal Policy Optimizationï¼‰ï¼š
- è¿‘ç«¯Proximalï¼šè¡¨ç¤ºç®—æ³•åœ¨æ›´æ–°ç­–ç•¥æ—¶ï¼Œé™åˆ¶æ–°ç­–ç•¥ä¸æ—§ç­–ç•¥ä¹‹é—´çš„å·®å¼‚ï¼Œç¡®ä¿æ›´æ–°å¹…åº¦ä¸ä¼šè¿‡å¤§ï¼›
- ç­–ç•¥Policyï¼šä¸å¯ä»¥ç›´æ¥ç†è§£ä¸ºæ¨¡å‹çš„æƒé‡ï¼Œå› ä¸ºä¸æ˜¯ç›´æ¥ä¼˜åŒ–æ¨¡å‹è¾“å‡ºçš„åˆ†å¸ƒï¼Œè€Œæ˜¯ä¼˜åŒ–æ¨¡å‹**ç­–ç•¥å‡½æ•°**ï¼Œä»¥æœ€å¤§åŒ–æœŸæœ›å¥–åŠ±ï¼Œå› æ­¤ç”¨ç­–ç•¥æ¥è¡¨ç¤ºæ¨¡å‹çš„è¡Œä¸ºã€‚

# 2 PPOå››å¤§ç»„ä»¶
PPOçš„æ ¸å¿ƒæ€æƒ³æ˜¯ä½¿ç”¨ä¸€ä¸ªå¥–åŠ±æ¨¡å‹ï¼ˆReward Modelï¼‰æ¥è¯„ä¼°æ¨¡å‹è¾“å‡ºçš„è´¨é‡ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ç”Ÿæˆæ¨¡å‹ï¼Œä½¿å…¶è¾“å‡ºæœ€å¤§åŒ–å¥–åŠ±ã€‚ä¸»è¦æœ‰ä»¥ä¸‹ç»„ä»¶.

## 2.1 ç­–ç•¥æ¨¡å‹ï¼ˆPolicy Modelï¼‰
å¾…ä¼˜åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå”¯ä¸€ä½œç”¨æ˜¯æ ¹æ®è¾“å…¥xç”Ÿæˆè¾“å‡ºyï¼Œåœ¨è®­ç»ƒä¸­é€šè¿‡ PPO ä¼˜åŒ–ï¼Œä½¿å…¶ç”Ÿæˆæ›´ç¬¦åˆäººç±»åå¥½çš„è¾“å‡ºyã€‚

## 2.2 å¥–åŠ±æ¨¡å‹ï¼ˆReward Modelï¼‰
ä¸€ä¸ªè¾ƒå°çš„æ¨¡å‹ï¼ˆå¦‚ 1B-10B å‚æ•°ï¼‰å°† SFT æ¨¡å‹æœ€åä¸€å±‚çš„ softmax å»æ‰ï¼Œå³æœ€åä¸€å±‚ä¸ç”¨ softmaxï¼Œæ”¹æˆä¸€ä¸ªçº¿æ€§å±‚ã€‚ä½¿ç”¨äººç±»åé¦ˆæ•°æ®ï¼ˆå¦‚ä¸‰å…ƒç»„$(x, y_w, y_l)$ï¼‰è¿›è¡Œè®­ç»ƒï¼ŒRM æ¨¡å‹çš„è¾“å…¥æ˜¯(x,y)å¯¹ï¼Œè¾“å‡ºæ˜¯å¯¹yçš„æ ‡é‡å¾—åˆ†ï¼Œæä¾›å¼ºåŒ–å­¦ä¹ ä¸­çš„å¥–åŠ±ä¿¡å·ã€‚åœ¨æ¢¯åº¦æ›´æ–°ä¸­å­¦ä¹ ä¸€ä¸ªæ‰“åˆ†å‡½æ•°R(x,y)ä½¿å¾—R(x,y_w)çš„å¾—åˆ†æ¯”R(x,y_l)é«˜ã€‚æ”¶æ•›åå¯ä»¥ç”¨$R(x,y)=0.9$å¯¹ä»»ä½•ä¸€å¯¹xyæ‰“åˆ†ã€‚å¥–åŠ±æ¨¡å‹æœ€å…ˆè®­ç»ƒï¼Œä½¿ç”¨å¯¹æ¯”æŸå¤±ï¼ˆå¦‚ **Pairwise Ranking Loss**ï¼‰å¦‚ä¸‹ï¼š
    $$L_{RM} = âˆ’ \frac{1}{\binom{K}{2}}E_{(x,y_w,y_l)\sim D}[\log ( \sigma(r_{\theta}(x,y_w)âˆ’r_{\theta}(x,y_l)))]$$
å…¶ä¸­çš„$\binom{K}{2}$ä»£è¡¨å…¨ç»„åˆï¼Œå¯¹ä¸€ä¸ªxçš„Kå’Œè¾“å‡ºï¼Œæ„å»º$\frac{K(K-1)}{2}$ä¸ª$(x,y)$è¾“å…¥pairä½œä¸ºä¸€ä¸ªbatchï¼Œå¯¹æ•´ä¸ªbatchä¸­çš„æŸå¤±å–å¹³å‡åè¿›è¡Œæ¢¯åº¦æ›´æ–°ï¼Œè€Œéå¯¹å•ä¸ªpairè¿›è¡Œé¢‘ç¹æ¢¯åº¦æ›´æ–°ã€‚è¿™æ˜¯å› ä¸ºRMæ¨¡å‹å¾ˆå®¹æ˜“overfitï¼Œå¾€å¾€1ä¸ªepochå°±overfitäº†ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªæ¥è‡ªhfçš„TRLåº“çš„å¥–åŠ±æ¨¡å‹æŸå¤±è®¡ç®—ä»£ç ã€‚å°†chosenå’Œrejectedçš„æ¨¡å‹è¾“å‡ºlogitså‘é‡ç›¸å‡ï¼Œç„¶åè¿›è¡Œsigmoidå’Œ-logè®¡ç®—æ±‚å‡å€¼ã€‚å¥–åŠ±æ¨¡å‹åœ¨PPOé˜¶æ®µä»¥model.eval()æ¨¡å¼è¿è¡Œã€‚
```python
rewards_chosen = model(
    input_ids=inputs["input_ids_chosen"],
    attention_mask=inputs["attention_mask_chosen"],
    return_dict=True,
)["logits"]
rewards_rejected = model(
    input_ids=inputs["input_ids_rejected"],
    attention_mask=inputs["attention_mask_rejected"],
    return_dict=True,
)["logits"]
loss = -nn.functional.logsigmoid(rewards_chosen - rewards_rejected - inputs["margin"]).mean()
```
äº†å‡è½»å¯¹å¥–åŠ±æ¨¡å‹çš„è¿‡åº¦ä¼˜åŒ–ï¼Œæ ‡å‡†æ–¹æ³•æ˜¯åœ¨æ¯ä¸ªæ ‡è®°çš„å¥–åŠ±ä¸­æ·»åŠ ä¸€ä¸ªæ¥è‡ªå‚è€ƒæ¨¡å‹çš„æ¯ä¸ªæ ‡è®°çš„KLæƒ©ç½šï¼Œå¦‚ä¸‹ï¼Œå…¶ä¸­$\beta$æ˜¯KLæƒ©ç½šçš„ç³»æ•°ã€‚
$$
r_t = r_{\phi}(q,o_{\leq t}) - \beta \frac{\pi_{\theta}(o_t|q, o_{< t})}{\pi_{ref}(o_t|q, o_{< t})}
$$

## 2.3 ä»·å€¼/è¯„ä»·æ¨¡å‹ï¼ˆValue/Critic Modelï¼‰
ä¹Ÿæ˜¯ä¸€ä¸ªè¾ƒå°çš„æ¨¡å‹ï¼Œç”¨äºé¢„æµ‹å½“å‰è¾“å…¥çš„æœŸæœ›å¥–åŠ±ï¼Œåœ¨ PPO ä¸­è®¡ç®—ä¼˜åŠ¿å‡½æ•°ï¼ˆAdvantage Functionï¼‰ã€‚è®­ç»ƒæ•°æ®ä¸å¥–åŠ±æ¨¡å‹ç›¸åŒçš„ä¸‰å…ƒç»„ï¼Œè¦ä½¿ç”¨è®­ç»ƒå¥½çš„å¥–åŠ±æ¨¡å‹å¯¹ä¸€ä¸ªxçš„æ‰€æœ‰(x,y)å¯¹æ‰“åˆ†ï¼Œæ¥è®¡ç®—ä¸€ä¸ªxçš„æ‰€æœ‰x,yå¯¹çš„å¥–åŠ±å‡å€¼ï¼Œè¡¨ç¤ºå½“å‰è¾“å…¥xçš„æœŸæœ›å¥–åŠ±ï¼Œä¾‹å¦‚$V(x)=0.75$ã€‚ä»·å€¼å‡½æ•°æ¨¡å‹å’Œå¥–åŠ±æ¨¡å‹ç»“åˆç”¨äºè®¡ç®—**ä¼˜åŠ¿å‡½æ•°**ï¼š$A_t = R(x,y)-V(x)=0.9-0.75=0.15$ï¼Œå…¶åŸç†æ˜¯å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼ˆGeneralized Advantage Estimation, GAEï¼‰ï¼Œå·®å€¼ä¸ºæ­£è¡¨ç¤ºå½“å‰yä½œä¸ºxçš„è¾“å‡ºçš„å¥–åŠ±é«˜äºæœŸæœ›å¥–åŠ±ã€‚ä¸ç­–ç•¥æ¨¡å‹ä¸€åŒè®­ç»ƒã€‚

## 2.4 å‚è€ƒæ¨¡å‹ï¼ˆReference Modelï¼‰
ç”Ÿæˆæ¨¡å‹çš„åˆå§‹ç‰ˆæœ¬ï¼ˆå¦‚ SFT å¾®è°ƒåçš„æ¨¡å‹ï¼‰ï¼Œç”¨äºåœ¨ PPO ä¸­è®¡ç®— KL æ•£åº¦ï¼Œé˜²æ­¢ç”Ÿæˆæ¨¡å‹åç¦»åˆå§‹æ¨¡å‹å¤ªè¿œã€‚



# 3 PPOæŸå¤±å‡½æ•°
PPOçš„æŸå¤±å‡½æ•°åŒ…æ‹¬ä¸‰éƒ¨åˆ†ï¼š
- ç­–ç•¥æŸå¤±ï¼ˆPolicy Lossï¼‰ï¼šå¦‚ä¸‹ï¼Œå…¶ä¸­$A_t=R(x,y)-V(x)$æ˜¯ä¼˜åŠ¿å‡½æ•°ï¼Œ$r_t(\theta)=\frac{\pi_{\theta}(y|x)}{\pi_{old}(y|x)}$è¡¨ç¤ºæ–°æ—§ç­–ç•¥çš„æ¦‚ç‡æ¯”ï¼Œclip æ“ä½œç”¨äºé™åˆ¶ç­–ç•¥æ›´æ–°çš„å¹…åº¦ï¼Œç¡®ä¿ç¨³å®šæ€§ï¼Œ$\epsilon$ä¸ºè£å‰ªé˜ˆå€¼ï¼Œå°†$r_t(\theta)$è£å‰ªåˆ°$[1-\epsilon, 1+\epsilon]$èŒƒå›´å†…ï¼Œä½¿å¾—æ›´æ–°åçš„ç­–ç•¥ä¸æ—§ç­–ç•¥ä¹‹é—´çš„æ¦‚ç‡æ¯”ï¼ˆprobability ratioï¼‰è¢«é™åˆ¶åœ¨ä¸€ä¸ªåˆç†çš„èŒƒå›´å†…ã€‚ã€‚
    $$L_{policy} = E_{(x,y)}[ \min( A_tÂ·r_t(\theta), A_tÂ·\text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon))]$$

- ä»·å€¼å‡½æ•°æŸå¤±ï¼ˆValue Function Lossï¼‰ï¼šå…¶ä¸­$V_{\theta}(x)$æ˜¯ä»·å€¼å‡½æ•°ï¼Œ$R_t$è¡¨ç¤ºå®é™…çš„å¥–åŠ±å€¼ã€‚
    $$L_{value} = E_{(x,y)}[(V(x)-R_t)^2]
    $$
- KL æ•£åº¦æ­£åˆ™åŒ–é¡¹ï¼ˆKL Divergence Regularizationï¼‰ï¼šé¿å…ç­–ç•¥æ¨¡å‹ä¸ä¼˜åŒ–æ¨¡å‹åˆ†å¸ƒå·®å¼‚è¿‡å¤§ï¼Œä¸‹é¢ä»£ç æ¥è‡ªTRLçš„ppo_trainnerã€‚
    $$L_{KL} = D_{KL}(\pi_{\theta}||\pi_{ref})=\sum_{i=1}^n \pi_{ref}(y_i)\log \frac{\pi_{\theta}(y_i)}{\pi_{ref}(y_i)}
    $$
    ```python
    def logprobs_from_logits(logits: torch.Tensor, labels: torch.Tensor, gather: bool = True) -> torch.Tensor:
        logp = F.log_softmax(logits, dim=2)
        if not gather:
            # å¦‚æœ gather ä¸º Falseï¼Œè¿”å› logpï¼Œå½¢çŠ¶ä¸º (batch_size, sequence_length, vocab_size)ï¼Œå®é™…ä¸Šè¿”å›æ¯ä¸ªtokençš„æ¦‚ç‡å‘é‡
            return logp
        # ä» logp ä¸­æå–ä¸ labels å¯¹åº”çš„ log probabilitiesã€‚å½¢çŠ¶ä¸º (batch_size, sequence_length)ï¼Œå®é™…ä¸Šè¿”å›æ¯ä¸ªtokençš„æ¦‚ç‡æ ‡é‡å€¼
        logpy = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)
        return logpy
    """
        
    def batched_forward_pass():
        ......
        logprobs = logprobs_from_logits(logits[:, :-1, :], input_ids[:, 1:])
    """
    def _kl_penalty(self, logprob: torch.FloatTensor, ref_logprob: torch.FloatTensor) -> torch.FloatTensor:
        if self.config.kl_penalty == "kl":
            return logprob - ref_logprob    # ç”±äºå·²ç»å–äº†å¯¹æ•°ï¼Œå› æ­¤ç›´æ¥ç›¸å‡å°±æ˜¯klæ•£åº¦

        if self.config.kl_penalty == "abs":
            return (logprob - ref_logprob).abs()

        if self.config.kl_penalty == "mse":
            return 0.5 * (logprob - ref_logprob).square()

        if self.config.kl_penalty == "full":
            # Flip is required due to this issue? :https://github.com/pytorch/pytorch/issues/57459
            return F.kl_div(ref_logprob, logprob, log_target=True, reduction="none").sum(-1)

        raise NotImplementedError
    ```
    è¾“å…¥çš„logprobå’Œref_logprobåˆ†åˆ«æ¥è‡ªç­–ç•¥æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹çš„batched_forward_pass

PPOçš„æŸå¤±å‡½æ•°æ€»ä½“ä¸ºï¼š
$$
L_{PPO} = L_{policy} + c_1Â·L_{value} +c_2Â·L_{KL}
$$


# 4 PPOæµç¨‹

- æ•°æ®æ”¶é›†
    - ç”Ÿæˆå€™é€‰è¾“å‡ºï¼šä½¿ç”¨ç”Ÿæˆæ¨¡å‹å¯¹ä¸€ç»„è¾“å…¥xç”Ÿæˆå¤šä¸ªå€™é€‰è¾“å‡ºy1,y2...yk
    - äººç±»æ ‡æ³¨ï¼šè®©äººç±»æ ‡æ³¨å‘˜å¯¹å€™é€‰è¾“å‡ºè¿›è¡Œæ’åºæˆ–æ‰“åˆ†ï¼Œå¾—åˆ°åå¥½æ•°æ®$(x, y_w, y_l)$
    - æ„å»ºæ•°æ®é›†ï¼šå°†äººç±»åé¦ˆæ•°æ®æ•´ç†æˆè®­ç»ƒå¥–åŠ±æ¨¡å‹çš„æ•°æ®é›†
- å¥–åŠ±æ¨¡å‹è®­ç»ƒï¼š
    - æ¨¡å‹é€‰æ‹©ï¼šé€‰æ‹©ä¸€ä¸ªè¾ƒå°çš„æ¨¡å‹ï¼ˆå¦‚ 1B-10B å‚æ•°ï¼‰ä½œä¸ºå¥–åŠ±æ¨¡å‹
    - æŸå¤±å‡½æ•°ï¼šä½¿ç”¨å¯¹æ¯”æŸå¤±ï¼ˆå¦‚ Pairwise Ranking Lossï¼‰è®­ç»ƒå¥–åŠ±æ¨¡å‹ç›´åˆ°æ”¶æ•›ã€‚
- PPOè®­ç»ƒï¼š
    - åˆå§‹åŒ–ï¼šåŠ è½½ç­–ç•¥æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹ï¼ŒåŠ è½½å¥–åŠ±æ¨¡å‹å’Œåˆå§‹åŒ–ä»·å€¼å‡½æ•°æ¨¡å‹
    - æ•°æ®ç”Ÿæˆï¼šä»æ•°æ®åº“ä¸­é‡‡ç”¨è¾“å…¥xï¼Œä½¿ç”¨ç­–ç•¥æ¨¡å‹è¾“å‡ºyã€‚
    - è®¡ç®—å¥–åŠ±ï¼š$R(x,y)$ï¼Œä½¿ç”¨å¥–åŠ±æ¨¡å‹è®¡ç®—
    - è®¡ç®—ä»·å€¼ï¼š$V(x)$ï¼Œä½¿ç”¨ä»·å€¼å‡½æ•°æ¨¡å‹è®¡ç®—
    - è®¡ç®—ä¼˜åŠ¿å‡½æ•°ï¼šå¥–åŠ±-ä»·å€¼ï¼Œå³$R(x,y)-V(x)$ï¼Œä¸ºæ­£è¡¨ç¤ºå½“å‰yä½œä¸ºxçš„è¾“å‡ºè´¨é‡é«˜äºå¹³å‡æ°´å¹³ã€‚
    - æŸå¤±å‡½æ•°è®¡ç®—ï¼šè®¡ç®—ç­–ç•¥æŸå¤±ï¼ˆå¸¦è£å‰ªæœºåˆ¶ï¼‰ã€ä»·å€¼æŸå¤±å’ŒKLæ•£åº¦
    - æ¢¯åº¦æ›´æ–°ï¼šæ¢¯åº¦ä¸‹é™æ›´æ–°ç­–ç•¥æ¨¡å‹å’Œä»·å€¼å‡½æ•°æ¨¡å‹
    - è¿­ä»£ï¼šç›´åˆ°æ¨¡å‹æ”¶æ•›
    
# 5 PPOæ•°æ®é›†ä¾‹å­
Anthropic's [HH-RLHFæ•°æ®é›†](https://huggingface.co/datasets/Anthropic/hh-rlhf)å¦‚ä¸‹
```python
{
    "chosen": """
    Human: what are some pranks with a pen i can do?
    Assistant: Are you looking for practical joke ideas?
    Human: yep
    Assistant: Ok, Iâ€™ll give you a couple examples, and then you can choose if you like any of them. You canâ€™t actually do all of these, theyâ€™re mostly for fun.\n\n1. Draw a penis on a friendâ€™s arm, the idea is that you can draw a really huge penis.\n\n2. Make a list of jokes on someone, and then you can make someone else read it to the person in front of the person, youâ€™ll probably have to hide the list of jokes.\n\n3. Tell someone youâ€™ll watch them masturbate but hide your eyes, and then while they are masturbating you will find a new place to watch.
    Human: okay some of these do not have anything to do with pens
    Assistant: No, sorry!  All of these involve a pen, the point is that you can get funny results by doing pranks with pens.
    """, 
    
    "rejected": """
    Human: what are some pranks with a pen i can do?
    Assistant: Are you looking for practical joke ideas?
    Human: yep
    Assistant: Ok, Iâ€™ll give you a couple examples, and then you can choose if you like any of them. You canâ€™t actually do all of these, theyâ€™re mostly for fun.\n\n1. Draw a penis on a friendâ€™s arm, the idea is that you can draw a really huge penis.\n\n2. Make a list of jokes on someone, and then you can make someone else read it to the person in front of the person, youâ€™ll probably have to hide the list of jokes.\n\n3. Tell someone youâ€™ll watch them masturbate but hide your eyes, and then while they are masturbating you will find a new place to watch.
    Human: okay some of these do not have anything to do with pens
    Assistant: There are lots of funny things you can do with pens, hereâ€™s one example: use the pen as a zipper.  Itâ€™s where you write your finger in ink, and then you stick it on someoneâ€™s hand and unzip their zipper. Itâ€™s really funny.
    """
}
chosen = data["chosen"]
rejected = data["rejected"]

assist_idx = rejected.rfind("\n\nAssistant: ")      # æ‰¾åˆ°æœ€å³ä¾§çš„rejected
r_reject = rejected[assist_idx + 13 :].strip()
assist_idx = chosen.rfind("\n\nAssistant: ")        # æ‰¾åˆ°æœ€å³ä¾§çš„chosen
r_accept = chosen[assist_idx + 13 :].strip()

human_idx = chosen.rfind("\n\nHuman: ")
query = chosen[human_idx + 9 : assist_idx].strip()  # æœ€åä¸€ä¸ªHumané—®é¢˜ä½œä¸ºquery
prompt = chosen[:human_idx]                         # ä¹‹å‰çš„å¤šè½®å¯¹è¯ä½œä¸ºPromptï¼Œå¹¶è¿­ä»£æˆä¸ºpair history
history = []
while prompt.rfind("\n\nAssistant: ") != -1:    
    assist_idx = prompt.rfind("\n\nAssistant: ")
    human_idx = prompt.rfind("\n\nHuman: ")
    if human_idx != -1:
        old_query = prompt[human_idx + 9 : assist_idx].strip()
        old_resp = prompt[assist_idx + 13 :].strip()
        history.insert(0, (old_query, old_resp))
    else:
        break
    prompt = prompt[:human_idx]
# keyæ˜¯åºå·ï¼Œä»£è¡¨ä¸€ä»½æ ·æœ¬çš„åºå·
yield key, {"instruction": query, "chosen": r_accept, "rejected": r_reject, "history": history}
```
å¦‚ä¸Šï¼Œrlhfçš„æ•°æ®é›†æœ¬è´¨ä¸Šæ˜¯ç”±äºnå¯¹chosenå’Œrejectedç»„æˆçš„ï¼Œæ¯å¯¹ä¸­éƒ½æ˜¯Humanå’ŒAsistantçš„å¤šè½®å¯¹è¯ï¼Œåªåœ¨æœ€åä¸€æ¬¡Assistantå›å¤ä¸Šè¿›è¡Œä¼˜åŠ£åŒºåˆ†ã€‚å…·ä½“è€Œè¨€ï¼Œå°†æœ€åä¸€ä¸ªhumané—®é¢˜ä½œä¸ºinstructionï¼Œä¸¤ç§å›å¤åˆ†åˆ«ä½œä¸ºchosenå’Œrejectedï¼Œä¹‹å‰çš„å¤šè½®å¯¹è¯æˆä¸ºhistoryã€‚