{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "class MultiHeadAttention( nn.Module ):\n",
    "    \"\"\"多头注意力，qkv三者维度被均分为num_heads个部分，增强并行能力和表示能力，更鲁棒\"\"\"\n",
    "    def __init__( self, config: XXXConfig):\n",
    "        super().__init__()\n",
    "        # config参数内化：头数、维度等\n",
    "        self.num_heads = config.num_heads      # 头数，即qkv的维度被均分为多少部分\n",
    "        self.hidden_dim = config.hidden_dim    # 嵌入维度embedding_dim，即输入向量的最后一个维度\n",
    "        self.qk_dim = config.qk_dim            # query和key投影矩阵的维度，两者需要点积因此维度必须一致，可以任意，但通常简化为与hidden_dim一致。\n",
    "        self.v_dim = config.v_dim              # value投影矩阵的维度，可以与qk和hidden_dim不一致，但通常简化为与hidden_dim一致，如Baichuan2-7B就是三者都等于hidden_dim\n",
    "        self.head_dim = self.hidden_dim // self.num_heads       # 也有直接设置为config.kv_channels指定的，如chatglm3-6b\n",
    "        assert self.head_dim * self.num_heads == hidden_dim , \"Embedding size must be divisible by num_heads\"\n",
    "\n",
    "        # 投影矩阵组件：下面三个投影矩阵可以写为一个self.W_pack，要用时再拆分\n",
    "        self.query_linear = nn.Linear( self.hidden_dim, self.qk_dim )\n",
    "        self.key_linear = nn.Linear( self.hidden_dim, self.qk_dim )\n",
    "        self.value_linear = nn.Linear( self.hidden_dim, self.v_dim )\n",
    "        self.out_linear = nn.Linear( self.v_dim, self.hidden_dim)\n",
    "\n",
    "        # 旋转位置编码组件\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "        self.rotary_emb = RotaryEmbedding(self.head_dim, max_position_embeddings=self.max_position_embeddings)\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        hidden_states,                                          # 输入的embedding结果\n",
    "        attention_mask: Optional[torch.Tensor] = None,          # 掩码，用于训练和batch推理mask padding\n",
    "        position_ids: Optional[torch.LongTensor] = None,        # 位置id，用于Rotary旋转位置编码组件\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]]  = None,  # 是否有之前的Kv_cache，区分首次迭代和之后\n",
    "        use_cache: bool = False,                                # 是否启用kv_cache\n",
    "    ):\n",
    "        \"\"\"\n",
    "        inputs.shape = [batch_size, token_len, hidden_state]\n",
    "        \"\"\"\n",
    "        batch_size, q_len = hidden_states.shape[0], hidden_states.shape[1]\n",
    "\n",
    "        Q = self.query_linear( hidden_states )\n",
    "        K = self.key_linear( hidden_states )\n",
    "        V = self.value_linear( hidden_states )\n",
    "        # 先view重塑再transpose，可以使得张量在内存中数据的排列方式符合后续多头并行计算：\n",
    "        # view 操作要求张量在内存中是连续的（contiguous），view 不会改变张量在内存中的实际存储顺序，它只是重新解释张量的形状\n",
    "        # transpose 不会改变张量在内存中的实际存储顺序，但它会改变张量的步幅（stride），从而改变访问数据的方式。\n",
    "        # 先将 query 重塑为 (batch_size, seq_len, num_heads, head_dim)确保 seq_len 和 head_dim 在内存中是连续的。再将 num_heads 和 seq_len 的维度交换，改变了维度顺序，但保留了每个头的 seq_len 和 head_dim 的连续性。\n",
    "        # 如果直接使用 query.view(batch_size, num_heads, -1, head_dim)，虽然形状是对的，但数据在内存中的排列可能不符合多头注意力的计算需求，因为 seq_len 和 head_dim 可能不再是连续的。\n",
    "        Q = Q.view( batch_size, q_len, self.num_heads, self.head_dim ).transpose( 1, 2) \n",
    "        K = K.view( batch_size, q_len, self.num_heads, self.head_dim ).transpose( 1, 2)\n",
    "        V = V.view( batch_size, q_len, self.num_heads, self.head_dim ).transpose( 1, 2)\n",
    "\n",
    "        # 对QK进行位置编码：要求是获得当前长度\n",
    "        kv_seq_len = K.shape[-2]\n",
    "        if past_key_value != None:\n",
    "            kv_seq_len += past_key_value[0].shape[-2]\n",
    "        cos, sin = self.rotary_emb( value_states, seq_len = kv_seq_len)\n",
    "        Q, K = apply_rotary_pos_emb(Q, K, cos, sin, position_ids)\n",
    "\n",
    "        # 再拼接kv_cache中的K和V\n",
    "        if past_key_value != None:\n",
    "            K, V = torch.cat( [past_key_value[0], K], dim = 2 ), torch.cat( [past_key_value[1], V], dim = 2 )       # 在q_len维度进行拼接\n",
    "        # 更新kv_cache\n",
    "        if use_cache:\n",
    "            past_key_value = (K, V)\n",
    "\n",
    "        # 进行缩放点积SDPA\n",
    "        attn_output = F.scaled_dot_product_attention(Q, K, V, attn_mask = attention_mask)       # 或设置is_causal=True，也是默认单向注意力\n",
    "        attn_output = attn_output.transpose(1, 2)\n",
    "        attn_output = attn_output.reshape( batch_size, q_len, self.hidden_dim)\n",
    "\n",
    "        # 最后进行混淆\n",
    "        attn_output = self.out_linear( attn_output )\n",
    "        return attn_output, past_key_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "class MultiQueryAttention( nn.Mudule ):\n",
    "    \"\"\"\n",
    "    MQA和GQA，其中前者是后者一个特例，即group数量为1。\n",
    "    \"\"\"\n",
    "    def __init__( self, config: XXXConfig ):\n",
    "        # config参数内化\n",
    "        self.hidden_dim = config.hidden_dim         # embedding维度\n",
    "        self.qk_dim = config.qk_dim\n",
    "        self.v_dim = config.value_dim\n",
    "\n",
    "        self.num_heads = config.num_heads           # query组数\n",
    "        self.head_dim = self.qk_dim // self.num_heads\n",
    "\n",
    "        self.num_groups = config.num_groups         # kv组数，为1时是MQA，>1时为GQA\n",
    "        self.query_per_kv = self.num_heads // self.num_groups\n",
    "        assert self.query_per_kv * self.num_groups == self.num_heads, \"GQA组数必须可以整除Query头数\"\n",
    "\n",
    "        # 线性层实例化\n",
    "        self.query_linear = nn.Linear( self.hidden_dim, self.qk_dim * self.num_heads )\n",
    "        self.key_linear = nn.Linear( self.hidden_dim, self.qk_dim * self.num_groups )\n",
    "        self.value_linear = nn.Linear( self.hidden_dim, self.v_dim * self.num_groups )\n",
    "        self.out_linear = nn.Linear( self.v_dim * self.num_groups * , self.v_dim * self.hidden_dim )\n",
    "\n",
    "        # 位置编码层\n",
    "        self.rotary_emb = RotaryEmbedding( self.qk_dim, max_rotary_embeddings = self.max_rotary_embeddings)\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        hidden_states,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        use_cache: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        hidden_states.shape = [batch_size, q_lens, hidden_dim]\n",
    "        \"\"\"\n",
    "        batch_size, q_lens = hidden_states.shape[0], hidden_states.shape[1]\n",
    "        Q = self.query_linear( hidden_states ).view( batch_size, q_lens, self.num_heads, self.qk_dim )\n",
    "        K = self.key_linear( hidden_states ).view( batch_size, q_lens, self.num_groups, self.qk_dim )\n",
    "        V = self.value_linear( hidden_states ).view( batch_size, q_lens, self.num_groups, self.v_dim )\n",
    "        \n",
    "        # 位置编码\n",
    "        kv_seq_len = K.shape[1]\n",
    "        if past_key_value:\n",
    "            kv_seq_len += past_key_value[0].shape[1]\n",
    "        cos, sin = self.rotary_emb( V, kv_seq_len )\n",
    "        Q, K = apply_rotary_pos_emb(Q, K, cos, sin, position_ids)\n",
    "\n",
    "        # KV_cache：在seq维度扩展\n",
    "        if past_key_value:\n",
    "            K, V = torch.cat( [past_key_value[0], K], dim = 1), torch.cat( [past_key_value[1], V], dim = 1)\n",
    "        if use_cache:\n",
    "            past_key_value = (K. V)\n",
    "        \n",
    "        # 扩展以适应MQA和GQA点积\n",
    "        # 将KV的[batch_size, q_lens, num_groups, dim]四个维度的倒数第二个维度处插入一个维度，变成[batch_size, q_lens, 1, num_groups, dim]\n",
    "        # 并在该维度复制 num_heads // self.num_groups 份后与num_group维度合并，使得KV与Q在第三个维度的维数一致，便于计算\n",
    "        K = K.unsqueeze( -2 )\n",
    "        K = K.expand( -1, -1, -1, self.num_heads // self.num_groups, -1)\n",
    "        K = K.contiguous().view( k.shape[:2] + ( self.num_heads ,self.qk_dim) )\n",
    "        V = V.unsqueeze( -2 )\n",
    "        V = V.expand( -1, -1, -1, self.num_heads // self.num_groups, -1)\n",
    "        V = V.contiguous().view( k.shape[:2] + ( self.num_heads ,self.qk_dim) )\n",
    "\n",
    "        # 调整形状为 batch_size, num_heads, q_lens, dim进行并行计算SDPA\n",
    "        Q, K, V = [ states.transpose( 1,2 ) for states in [Q, K, V]]\n",
    "        attn_output = F.scaled_dot_product_attention( Q, K, V, is_causal = True )\n",
    "        attn_output.transpose( 1, 2)\n",
    "        return attn_output, past_key_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RotaryEmbedding( nn.Module ):\n",
    "    \"\"\"旋转位置编码，不是作用于embedding，而是Q和K\"\"\"\n",
    "    def __init__(self, qk_dim, max_positions = 151643, base = 10000 ):\n",
    "        \"\"\"\n",
    "        param qk_dim: Query和Key的维度\n",
    "        max_positions: 预存的最大token长度\n",
    "        base: 旋转角度的基数，即sin和cos的周期\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.qk_dim = qk_dim\n",
    "        self.base = base\n",
    "        self.max_positions = max_positions\n",
    "        # 获取表示维度的三角函数角频率向量列表，定长，即base^(2i/qk_dim)\n",
    "        self.inv_freq = 1.0 / self.base ** (torch.arrange(0, self.qk_dim, 2.0) / self.self.qk_dim)\n",
    "        # 预存最大编码长度范围内的编码值（sin和cos），但不是直接编码，需要转换。届时可以直接根据position_ids索引获取\n",
    "        self.rope_cache = self.get_cos_sin_cache( self.inv_freq, self.max_positions )\n",
    "\n",
    "    def get_cos_sin_cache( self, inv_freq, max_positions ):\n",
    "        \"预存最大长度的位置编码，shape = [max_positions, qk_dim // 2, 2]，前一半是cos，后一半是sin\"\n",
    "        dtype = inv_freq.dtype\n",
    "        token_pos = torch.arrange( 0, max_positions )\n",
    "        # 每个pos的theta*pos：将角频率向量与位置id向量外积，并double cat，获得shape = [max_positions, qk_dim]的张量\n",
    "        idx_theta = torch.outer( token_pos, inv_freq )\n",
    "        rope_cache = torch.stack( [ torch.cos(idx_theta), torch.sin(idx_theta)], dim = -1)\n",
    "        return rope_cache\n",
    "    \n",
    "    def forward( self, x, seq_len = None ):\n",
    "        \"根据实际序列长度和数据类型获取cos和sin\"\n",
    "        if seq_len == None:\n",
    "            seq_len = 1\n",
    "        if seq_len > self.max_positions:\n",
    "            self.rope_cache = self.get_cos_sin_cache( self.inv_freq, seq_len)\n",
    "        \n",
    "        return self.rope_cache[:seq_len].to( dtype = x.dtype )\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb( X, rope_cache):\n",
    "    \"\"\"\n",
    "    param X: [batch_size, num_heads, seq_len, qk_dim]\n",
    "    param rope_cache: [seq_len, qk_dim//2, 2]，cos和sin的编码\n",
    "    [X2i, X2i+1] * [[ cos2i, -sin2i ]\n",
    "                    [ sin2i+1, cos2i+1 ]] = [RoPE_X_2i, RoPE_X_2i+1]\n",
    "    \"\"\"\n",
    "    batch_size, num_heads, seq_len, qk_dim = X.shape\n",
    "    # 将最后一个维度拆分为qk_dim // 2个两两一组的相邻维度\n",
    "    xshaped = X.reshape( batch_size, num_heads, seq_len, qk_dim // 2, 2)\n",
    "    rope_cache = rope_cache.reshape( 1, 1, seq_len, qk_dim // 2, 2 )\n",
    "    # 旋转位置编码：X2i = cos*X2i + sin*X2i+1, X2i+1 = -sin*X2i + cos*X2i+1\n",
    "    x_out2 = torch.stack(\n",
    "        [\n",
    "            xshaped[..., 0] * rope_cache[..., 0] + xshaped[..., 1] * rope_cache[..., 1],\n",
    "            xshaped[..., 1] * rope_cache[..., 0] - xshaped[..., 0] * rope_cache[..., 1]\n",
    "        ],\n",
    "        dim = -1\n",
    "    )\n",
    "    # 从第三维度开始，将x_out2的最后一个维度合并到第二个维度，即[batch_size, num_heads, seq_len, qk_dim]\n",
    "    x_out2 = x_out2.flatten(3)\n",
    "    return x_out2.reshape( batch_size, num_heads, seq_len, qk_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class MistralRotaryEmbedding(nn.Module):\n",
    "    def __init__(self, qk_dim, max_positions = 151643, base = 10000):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-float('inf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
