Github主页: https://github.com/deepseek-ai
API文档: https://api-docs.deepseek.com/zh-cn/guides/json_mode
集成工具 (integration): https://github.com/deepseek-ai/awesome-deepseek-integration


开源五项基础训练工具
- FlashMLA：基于 Hopper GPU 的高效 MLA 解码内核
- DeepEP：专为 MoE 和 EP 设计的高效通信库
- DeepGEMM：高效矩阵乘法（GEMM）库
- DualPipe：双向流水线并行算法
	- EPLB：自动平衡 GPU 负载
	- profile-data：训练和推理框架的分析数据
- 3FS：高性能分布式文件系统

DeepSeek 系列模型的主要发展历程和关键技术创新：
1. DeepSeek LLM (V1基础版, 2024-01))：与Llama类似, 但优化如下
	- 多阶段学习率调度器而非Llama的余弦学习率调度器
	- 分组查询注意力机制（GQA）替代了传统的多头注意力机制
	- 采用更深结构而不是更宽的结构
2. DeepSeekMath: 与DeepSeek LLM相同的模型架构
3. DeepSeek-V2: 2024-05
	- Attention：**MLA**替代原来的GQA
	- FFN：采用了**DeepSeekMoE**体系结构，目的是为了实现最终的专家专业化
4. DeepSeek VL:  2024-05，多模态
5. DeepSeek-V3: 2024-12
	- 无辅助损失的负载均衡策略
	- 多词元预测训练目标：将预测范围扩展到每个位置的多个未来token
6. DeepSeek-R1(-Zero): 基于DeepSeek-V3-Base的纯文本模型，其中Zero用于生成训练用的数据
	- 推理模型：Reasoning Model，CoT提示链不够（不具备高度的通用性），需要训练内化CoT的能力
	- 核心逻辑：将推理过程文字化，固定后续推理结果不漂移
	- 数据合成：DeepSeek V3的另外一个微调版本生成训练数据（60万条长链CoT数据进行训练）
		- 1万条：使用三种方法收集到比较高质量的思维链数据（Few Shot Prompt/直接Prompt/利用DeepSeek-Zero)，并进行过滤（过滤不正确的样本拒绝采样）和人工标注
		- 60万条：用1万条数据微调DeepSeek V3，让微调模型能够生成大量高质量的CoT数据
	- 训练算法：GRPO强化学习算法，组近似策略优化
		- 先让模型具有输出思维链的能力: 
		- 再使用RLHF + rule_base_reward对模型进行强化学习的训练
7. DeepSeek-R1-Distill 

# R1论文阅读
title: DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning
date: 1/22 2025
core: 不使用SFT，直接通过强化学习激发LLMs的推理能力

## 1 Abstract
介绍首代推理模型DeepSeek-R1-Zero和DeepSeek-R1。
- DeepSeek-R1-Zero：第一步，直接通过大规模强化学习RL而不用SFT，展现remarkable推理能力。但存在一些问题：
	-  结果可读性readability较差
	-  语言mixing
- DeepSeek-R1：第二步，在RL之前使用multi-stage训练和cold-start数据，在推理任务上媲美OpenAI-o1-1217
	- cold-start：用于对Base模型做CoT的SFT，让模型把思考过程放在\<think>\</think>之间，他们把这个过程叫做Cold Start（冷启动），字节的ReFT文中把这个过程称过warm up
开源DeepSeek-R1-Zero和DeepSeek-R1，以及六个稠密的蒸馏模型（DeepSeek-R1为教师，Qwen和Llama为学生，1.5B, 7B, 8B, 14B, 32B, 70B）。综上，不用SFT直接用RL就能让模型拥有强大的推理能力，但RL之前SFT一下结果会更好（易读性、语言统一性）。


## 2 Intro

近年来LLMs的发展朝着AGI目标迈进（第一段）。开销较小的post-training重要性愈发体现，尤其是推理、价值观和偏好对齐方面（第二段）。OpenAI o1系列的推理模型依然最强，但其推理时扩展的性能和效率存在冲突：
- 创新：通过加长推理过程CoT的长度引入推理时间扩展（inference-time scaling），即不增加模型参数，仅让模型在推理阶段“思考更久”或“尝试不同的推理路径”来提高推理能力。
- 问题：Effective test-time scaling（有效的测试时扩展）问题。在测试时，即实际推理任务中，如何以最小的计算资源实现最佳的推理能力扩展，而不只是简单地增加计算预算。
### 2.1 主要贡献

针对现有研究状况，本文提出了一个新的训练框架：即通过仅使用RL就可以让模型获得强大推理能力，再增加一点冷启动微调数据可以表现得更好。总体贡献如下：
- Post-Training: 在基座模型上的大规模强化学习
	- DeepSeek-R1-Zero：**学术上**验证了仅RL的可行性。
		- 核心：直接使用名为GRPO的RL，不使用有监督SFT
		- 突破：以DeepSeek-V3-Base作为基座，进行thousands of RL steps (大概8000个steps)，推理性能强大。
		- 挑战：DeepSeek-R1-Zero存在可读性和语言混乱的问题
	- DeepSeek-R1：一个multi-stage训练pipeline，包括两个RL阶段和两个SFT阶段，**工业上**通过增加少量冷启动数据可以提供高可用LLMs
		- SFT-1: 使用数千收集到的数据微调DeepSeek-V3-Base作为冷启动cold-start，得到权重A。
		- RL-1: 在A上进行类似DeepSeek-R1-Zero的强化学习，但在快收敛convergence前停止，获得权重B。
		- SFT-2: 用B获取一批SFT数据（基于拒绝采样rejection sampling），并用DeepSeek-V3（非Base）获取一批多领域的SFT数据，一起微调B，得到C。
		- RL-2：在C上继续RL步骤，直到收敛。
- Distillation: 小模型蒸馏后也可以很强大
	- 展示了这种推理模式pattern可以被蒸馏（以Qwen2.5-32B为基础，使用DeepSeek-R1的结果直接对其RL）
	- 开源了一批基于Qwen和Llama的稠密蒸馏小模型。
### 2.2 评估结果概述

在推理相关的任务中，DeepSeek-R1在推理（AIME 2024）、数学（MATH-500）、代码（Codeforces以及工程相关任务）都好于OpenAI-o1-1217等模型。在知识方面，虽然略逊于OpenAI-o1-1217，但好于其他闭源模型和DeepSeek-V3。在其他任务上也表现出色。

## 3 Approach
之前得思路通过大量的监督数据来提高模型性能，而我们证明了：
- 不使用监督微调（SFT）作为冷启动，仅通过大规模强化学习（RL）也可以显著提高推理能力
- 通过包含少量冷启动数据，可以进一步提高性能。
在以下部分中，我们将介绍：
1. DeepSeek-R1-Zero：直接将RL应用于基础模型，而不需要任何SFT数据
2. DeepSeek-R1：它从一个检查点开始应用RL，该检查点经过数千个长思维链（CoT）示例的微调。
3. 蒸馏：将DeepSeek-R1的推理能力提取到小型密集模型中。
### 3.1 DeepSeek-R1-Zero：仅使用强化学习，理论探索

强化学习在推理任务中有效，但依赖于需要耗费大量时间收集的监督数据。为此，我们探索了通过纯强化学习过程的自我进化。
#### 3.1.1 强化学习算法：GRPO
Group Relative Policy Optimizatio，即组相关性策略优化，核心目标是节约传统RL中的开销。GRPO的核心在于
- 抛弃了评估模型critic model：用于值函数计算中的baseline，和policy model尺寸一样，RL过程中需要梯度更新。
- 用一组奖励模型的score的平均近似baseline：对每个输入的用户问题q，基于旧策略模型$\pi_{\theta_{old}}$生成一组G个输出$\{o_1, o_2,..., o_G\}$，取这G个输出的奖励平均值用于优化出新的策略模型$\pi_{\theta}$

GRPO的最大化优化目标如下：
$$
L_{GRPO}(\theta)= 
\text{E}[q\sim P(Q), {\{o_i\}}_{i=1}^G] \sim \pi_{\theta_{old}}(O|q)
$$
即，目标是最大化一个包含问题q（服从P(Q)分布）和回答o（服从基于旧策略模型和条件q的条件概率）的公式的均值，可以展开如下：
$$
\frac{1}{G}\sum_{i=1}^{G}
    (
        \min(
            \frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{old}}(o_i |q)} A_i, 
            
            \text{clip}(\frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{old}}(o_i |q)}, 1-\epsilon, 1+\epsilon)A_i
        )
        -
        \beta \text{D}_{KL}[\pi_{\theta}||\pi_{ref}]
    
    )
$$

其中$i$是输出o的序号，$A_{i}$是基于组内奖励均值的相对优势估计
$$
A_i = \frac{r_i-mean({r_1,r_2,...,r_G})}{std({r_1,r_2,...,r_G})}
$$

即对每个回答的奖励值$r_i$进行均值方差归一化，使得大于0表示回答$o_i$的指令优于平均水平。优化目标包含一个最大化策略和两个正则化策略，分别解释如下：
- 最大化：偏好让新策略相比旧策略有足够的改进，即通过连加最大化使得优质回答能尽可能提高新策略的自信，对于劣质回答能够减少新策略的自信，这样连加后的总比值就会尽可能大。
- 正则化：两部分组成，
	- 隐式正则：min函数+clip策略比值修剪结合，防止每轮更新策略时新旧策略比值过大
	- 显式正则：KL散度防止每轮更新时新策略和原始策略（参考模型）偏移过大。
##### (1) clip策略比值裁剪：隐式正则，限制对动作i过于自信或过度保守

是限制策略更新步幅的裁剪操作（来源于PPO），包含下面两组参数
- $\frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{old}}(o_i |q)}$: 策略比值，即新旧策略在该q条件下产生$o_i$动作的概率比值，如果大于1，说明新策略比旧策略更容易在特定条件下产生该动作。
- 裁剪范围$(1-\epsilon, 1+\epsilon)$: 约束策略比值的范围，即如果策略比值在范围内，则正常梯度下降；否则，将梯度固定为最大值或者最小值（旧策略梯度乘以区间两端之一）进行固定的梯度下降。
因此，最大化优化目标的前半部分（不考虑KL散度约束）的目标是，对所有回答$o_i$的平均策略比值做这样一个优化：
- 当$o_i$优于均值（$A_i>=0$）时：
	- 策略比值在裁剪范围内或者小于$1-\epsilon$：不管，连加对象是策略比值和优势函数的乘积$\frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{old}}(o_i |q)} A_i$。
	- **策略比值大于裁剪范围**：则连加对象是裁剪范围最大值$1+\epsilon$和优势函数的乘积$(1+\epsilon) A_i$
- 当$o_i$劣于均值（$A_i<0$）时：
	- 策略比值在裁剪范围内或大于$1+\epsilon$：不管，连加对象是策略比值和优势函数的乘积$\frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{old}}(o_i |q)} A_i$
	- **策略比值小于裁剪范围**：则连加对象是裁剪范围最小值$1-\epsilon$和优势函数的乘积$(1-\epsilon) A_i$。
即，对于优质回答（$A_i$>0）限大（不大于$1+\epsilon$）不限小，防止其导致的新策略在该动作上过于自信（策略比值过大）。对于劣质回答（$A_i<0$）限小（不小于$1-\epsilon$）不限大，防止新策略在该动作上过于保守（策略比值过小）。两者都是避免新策略相比旧策略在某个动作上偏移过大。

##### (2) KL散度惩罚: 显示正则，反向KL
与PPO的反向KL散度
$$
D_{KL}[\pi_{\theta}||\pi_{ref}] =\beta \log \frac{\pi_{\theta}(o_{i}|q)}{\pi_{ref}(o_{i}|q)}
$$
略有不同，GRPO使用下面的无偏估计来估计KL散度，下式是标准反向KL散度的泰勒展开，用于更快速地基于output样本计算散度。从标准反向KL散度（上式）推导出GRPO的KL散度如下，对$\log \frac{1}{x}$在x=1处展开得到：
$$
\log x \sim -(x-1)+\frac{(x-1)^2}{2}-O((x-1)^3)
$$
将$x = \frac{\pi_{ref}(o_{i}|q)}{\pi_{\theta}(o_{i}|q)}$代入，得到

$$
\log \frac{\pi_{ref}(o_{i}|q)}{\pi_{\theta}(o_{i}|q)} \sim -(\frac{\pi_{ref}(o_{i}|q)}{\pi_{\theta}(o_{i}|q)}-1)+\frac{(\frac{\pi_{ref}(o_{i}|q)}{\pi_{\theta}(o_{i}|q)}-1)^2}{2}-O((\frac{\pi_{ref}(o_{i}|q)}{\pi_{\theta}(o_{i}|q)}-1)^3)
$$
化简得到GRPO的KL散度近似

$$
D_{KL}[\pi_{\theta}||\pi_{ref}] =\beta (\frac{\pi_{ref}(o_{i}|q)}{\pi_{\theta}(o_{i}|q)} - \log \frac{\pi_{ref}(o_{i}|q)}{\pi_{\theta}(o_{i}|q)} - 1)
$$
所以虽然$\frac{\pi_{ref}(o_{i}|q)}{\pi_{\theta}(o_{i}|q)}$可能让人误会成正向KL，但总体上还是反向的KL散度，其核心原因就在于是用了$\log \frac{1}{x}$的形式（$x = \frac{\pi_{ref}(o_{i}|q)}{\pi_{\theta}(o_{i}|q)}$）。因此，可以总结GRPO的KL散度还是使用的反向KL散度（更保守的KL散度），只不过通过二阶泰勒展开的近似减少了计算量（PPO需要计算二阶导数，没有完全搞懂）。

#### 3.1.2 奖励建模：基于规则的奖励模型

基于规则的奖励系统，奖励有两种：
- 准确性Accuracy奖励: 评估回复是否正确。例如，在具有确定性结果的数学问题的情况下，模型需要以指定的格式（例如，在框内）提供最终答案，从而能够对正确性进行可靠的基于规则的验证。
- 格式Format奖励：强制将其思维过程置于\<think>和\</inthink>标签之间。
此外，没有使用结果或过程的神经奖励模型（neural reward model），因为这种模型在大规模强化学习过程中可能会遭受奖励黑客攻击，而重新训练奖励模型需要额外的训练资源，这使整个训练管道变得复杂。
#### 3.1.3 微调语料模板

![[grpo_template.png]]
如表1所示，该模板要求先生成推理过程（\<think>和\</think>标签之间），然后生成最终答案（\<answer>和\</answer>标签之间）。这种结构格式有助于避免任何**特定于内容的偏见**（例如基于答案进行反思性推理，推广特定的问题解决策略）以确保我们能够在强化学习过程中准确观察模型的自然进程。

#### 3.1.4 性能与结果

- 在AIME测试集上，6000-8000个RL steps后R1-Zero可以超越和逼近o1。
- 8000个RL steps中，平均回复长度增长（500-10000），表示仅用RL就能让模型自然学会**思考更久**来解决推理任务。随着时间增长，更复杂的推理行为如**反思**（模型修改和重新评估其先前步骤）和探索解决问题的**替代方法**等行为自发出现。
- 顿悟时刻aha moment：在推理过程中修正之前的步骤，重新评估并分配思考时间，强调了强化学习的力量和美丽
- 缺点：可读性差与语言混合，这是为什么开发R1的原因。

### 3.2 DeepSeek-R1：增加SFT，工业可用

DeepSeek-R1-Zero有希望的结果的启发，出现了两个自然的问题：
- 通过引入**少量高质量**数据作为冷启动（Cold Start），可以进一步提高推理性能或加速收敛吗？
- 我们如何训练一个用户友好的模型，不仅产生清晰连贯的思维链（CoT），而且表现出强大的通用能力？
为了解决这些问题，我们设计了一个训练DeepSeek-R1的管道。该管道由四个阶段组成，概述如下。

#### 3.2.1 SFT-1: Cold Start 侧重推理格式

基础模型（DeepSeek-V3-Base）直接进行RL训练的早期存在不稳定的冷启动阶段，因此先对其进行指令微调SFT。**数千条高质量长CoT数据**收集方式如下：
- few-shot：以一个长CoT为example，生成回答
- zero-shot：不同任何示例，直接提示模型生成详细、包含反思和验证的回答
- DeepSeek-R1-Zero的输出：转变为可读格式，并通过人类标注者进行后处理进行筛选。

冷启动数据的优势在于其可读性和迭代潜力：DeepSeek-R1-Zero的内容通常不适合阅读，例如混合多种语言，或者缺乏标记来为用户突出答案。相比之下，冷启动数据中设计了一个可读的模板，在每个response的末尾都包含summary，并过滤掉对读者不友好的response。在这里，将输出格式定义为|special_token|<reasoning_process>|special_token|\<summary>，其中reasoning_process是查询的CoT过程，summary用于总结推理结果。

#### 3.2.2 RL-1：侧重推理任务准确与语言一致性

- 训练目的：提升核心推理能力，尤其增强模型在数学、代码、科学等领域的复杂推理性能
- 输入模型：SFT-1的权重
- 奖励模型：由于这些问题具有明确解决答案，因此使用基于规则的奖励系统。强化学习奖励 = 语言一致性奖励(目标语言在输出中的比例) + 推理任务的准确性。语言一致性奖励会降低性能，但会提升可读性。注意，DeepSeek-R1-Zero的奖励系统（基于规则）也由任务准确性和格式准确性奖励组成。

#### 3.2.3 SFT-2: 多领域通用能力，80w条数据

- 训练目的：增强模型在非推理任务（如写作、事实问答）上的通用性
- 输入模型：RL-1将收敛的权重
- 数据来源：本轮主要采用下面两类数据进行微调
	- 推理能力：用下述方法生成60万条侧重于多领域能力的SFT数据，并过滤掉低质量部分（混合语言和过长的CoT链）。让RL-1后的模型生成用于SFT的推理数据，核心方法是**拒绝采样策略**，对一个输入生成多个输出，仅保留其中符合要求的数据。那么如何判断一个输出是否符合要求呢？主要有两种方法：
		- **基于规则的方法**：与前一个RL阶段中使用的基于规则的奖励系统的方法一致，对于有明确输出且可以通过规则判断是否正确的，则用规则筛选。
		- **基于神经网络的方法**：但许多输出是无法用规则判断的，因此使用传统的基于神经网络的奖励模型思路。具体而言，使用 DeepSeek-V3作为奖励模型，将每一组(TrueOutput-PredictOutput)输入模型判断是否一致。
	- 非推理能力：复用DeepSeek-V3的SFT数据集（写作、事实问答等，约20万条）
		- 对于某些非推理任务：仅基于prompt使用DeepSeek-V3，使其生成CoT过程。
		- 对更加简单的query：比如"hello"，则不使用CoT数据

使用上述80万条数据微调DeepSeek-V3-Base（是对第二阶段的模型进行继续微调，还是从一个全新的模型进行微调）。
#### 3.2.4 RL-2: 多信号多领域强化学习
两种奖励信号+多领域prompt分布：
- 兼顾推理任务和一般任务
	- **推理任务**：如代码、数学、逻辑推理等可以用规则判断结果是否准确的部分，使用DeepSeek-R1-Zero中的基于规则的奖励系统给出信号。
	- **一般任务**：如写作，事实问答等无法用规则系统解决的部分，使用奖励模型捕捉复杂场景中的人类偏好。奖励模型基于DeepSeek-V3构建，与偏好数据对和训练prompt有相似分布（相似分布指的是在上面微调过？）
- 兼顾模型的有益性和无害性
	- 有益性：只关注最终summary部分，确保这部分对用户的实用性和相关性，尽量减少推理过程的干扰。
	- 无害性：评估模型的整个response，包括推理过程和总结，以识别和减轻生成过程中可能出现的任何潜在风险、偏见或有害内容。

### 3.3 Distillation: 通过仅SFT蒸馏将推理能力教授给小模型

直接对Qwen和Llama等开源模型，使用使用DeepSeek-R1第三阶段产生的80万个有监督样本，进行SFT微调。对于蒸馏模型，只应用SFT，不包括RL阶段，尽管加入RL可以大大提高模型性能。我们的主要目标是证明蒸馏技术的有效性，将RL阶段的探索留给更广泛的研究界。

