<!-- JPW的Markdown笔记模板 v1, 其中的href需要视情更改上级目录href="../../format.css -->
<link rel="stylesheet" type="text/css" href="../../format.css">


<h1>LLMs组件系列进阶：分词法</h1>

💡 分词法的核心职能是基于训练获取某种/一组语言的最小意义单元（token）集合，即词表；基于这个词表，对应用场景见到的文本进行分词和编解码。因此一个分词器类可以由以下四种组件构成：
- trainer：训练器，对给定的语料集上进行训练，得到词表。
- vocabulary：词表（长度为n），每种语言任务都基于特定的词表，可以自己训练，也可使用预训练好的词表。
- encoder：编码器，应用过程中给定文本input，编码器按词表将input拆分为token并替换为其在词表中的索引id，此时每个token可以表示为一个长度为n的one-hot向量，用于后续embedding。
- decoder：解码器，给定id序列，转换为token序列并最终返回文本。


目前深度学习模型分词法主要采取子词级（subword-level）分词思路，依赖于构词法原则，可以权衡word-level的OOV、无词缀问题，以及character-level的粒度过细、词表冗余问题。

下面将对四种主流subword-level的分词法进行介绍：
- BPE字节对编码，2015
- ULM一元语言模型，2016
- WordPiece，2018
- SentencePiece，2018
- 
# 0 快速使用预训练分词器
所有主流的Transformer模型都自己训练了一个分词器，因此我们可以直接使用这些分词器完成快速任务。最便捷的方式之一是利用HF的transformers库中的AutoTokenizer类，指定模型名称后自动加载其分词器。因此选择不同的分词器变成了选择使用不同分词器的模型。

```python
from transformers import AutoTokenizer

# BPE分词器
# model_name = "gpt2" 

# WordPiece分词器
# model_name = "bert-base-chinese"

# ULM分词器
# model_name = "facebook/mbart-large-50"

# SentencePiece分词器
model_name = "xlm-roberta-base"

tokenizer = AutoTokenizer.from_pretrained(model_name)   # cache_dir指定从本地加载
# tokenizer.vocab_size    # 词表大小21128
# tokenizer.model_max_length # 预训练模型支持的最大文本长度
# tokenizer.model_input_names # 模型在进行前向传播时，需要传入的参数

input = [
    "空间和外观，外观大气是让我最看重的",
    "毕竟是微型小面的底子，铁板薄，防锈处理基本等于没有，夏天玻璃像放大镜一样会把太阳温度放大很多倍，必须贴好的太阳膜呦！空调没有玻璃和驾驶舱一起吹的档位"
]
# batch_inputs = tokenizer(input, padding=True, truncation=True)   # input_ids, token_type_ids, attention_mask，三个作为模型输入必需的参数
input_ids = tokenizer.encode(input[0])  # 如果传入的是列表，会先join成字符串
output = tokenizer.decode(input_ids)
```




# 1 BPE字节对编码：基于词频

基本信息
- 背景：最早1994年提出用于数据压缩，基本原理是哈夫曼树，2015被引入NLP领域。
- 作者/年份：Rico Sennrich 等，2015
- 论文：[Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909)
- 核心idea：数据驱动，词频统计，单词内/单句内最高频词元对merge为子词，迭代更新词表。
- 采用该分词法的模型：GPT-1/2、RoBERTa、BART 和 DeBERTa；GPT-3/4采用BBPE。

BPE (Byte-Pair Encoding)分词法的核心原理是**单词内高频词元对合并**与**基于合并规则的分词**，分别对应训练和应用阶段，目标是从单词内部寻找高频的字符或子词组合，而不是跨单词进行合并。英文等有天然分隔的，先根据空格分为单词，然后以单词为单位统计字符对（子词，不跨单词合并）；中文等无分隔，直接划分为句子或子句，以句子或子句为单位统计字对（词，不跨句合并）。训练和应用阶段原理如下。

## 1.1 训练步骤：高频词元对合并

BPE的trainer接收一个语料库，不断统计单词内词元对的频数进行合并，输出一个合并规则表merges（用于encode分词）和一个词表vocab（用于encode编码和decode解码）。
- **输入**：收集语料库，并确定退出条件，如最终词表大小、迭代次数，以及默认的最大频数为1时。
- **添加单词边界**：将语料中每一个单词在末尾添上特殊字符\</w>，该字符表示单词的结束，防止可以独立存在的单词被当作子词合并。例如，low能会被错误地认作子词与er合并为lower。但如果在最终词表中low 记作 low\</w>，lower 记作 low er\</w>，BPE 在合并时就会知道 low\</w> 是完整的一个词，而 low 只是 lower 的一部分。
- **统计初始化词表频数字典**：统计每个单词/子句在语料库中出现的次数，形成初始字典，例如low在训练语料中出现了5次，则记为'low\</w>': 5。
- **单词拆分为字符**：即'low\</w>': 5变成 l o w \</w>': 5
- **统计连续词元对出现频率**：假设我们有l o w \</w>': 5和'h o w \</w>': 3，则此时有('l', 'o'): 5, ('o', 'w'): 5+3 = 8, ('h', 'o'): 3。
- **合并最高频词元对并更新词表频数字典**：例如('o', 'w')出现频率最高，则此时变成'l ow \</w>': 5, 'h ow \</w>':3，此时的词表为'l', 'ow', 'h'。同时按序记录词元对用于构造合并规则表。
- **保存**：重复直到达到退出条件，或者所有连续字符对的最高出现频率为1。从顺序词元对列表解析合并规则列表（即常见的字符或子词合并方式，有优先顺序的pair元组列表），从词表频数字典中解析词表（单词到数字的字典，解码时临时构建反向映射），并保存两者到本地。

## 1.2 应用步骤：基于合并规则的分词

- 编码：基于合并规则
  - **初始化文本**：将每个句子拆分为单词字符串列表，添加单词末尾符号，并用空格分隔单词。
  - **逐个单词合并**：对字符串列表中每个单词进行遍历，收集存在于合并规则表中的词元对，并选择其中顺序最靠前（键值最小）的进行合并，替换列表中原单词直至没有词元对。
  - **编码**：遍历合并完的字符串列表，按照空格分隔并对每个词元查vocab表进行编码。
- 解码：构造vocab的反向映射字典
  - **解码**：将每个整数型id映射为字符串并合并所有字符串
  - **替换**：将\</w>的替换为空格代表单词间空格。

## 1.3 优缺点

**优点**：
- 计算高效，通过统计字符对的频率并逐步合并高频字符对，适合大规模文本
- 适应性强，较好地权衡了word-level和character-level的问题（前者容易OOV，后者被拆分得太碎），对于未见过的词汇（未登录词），可以通过子词组合进行处理。
- 可解释性强：分词过程基于频率统计，合并规则清晰，易于理解和调试，生成的子词单元通常具有一定的语义或形态学意义。
- 词表大小可控：通过控制合并次数，可以灵活调整词表大小，适应不同任务需求

**缺点**：
- 基于合并规则的分词倾向于更短的分词，例如对于由ABC子词组成的词，往往更容易被分词为AB和C。这因为合并规则的优先级是基于子词对频率的，ABC的频率一定小于AB。这导致对低频词处理不足，无法处理未见过的字符组合，会将其拆分得很碎。例如，低频词 "happiness" 可能被拆分为 "hap" + "piness"，丢失了完整语义。
- 无法跨单词合并词组：对于英语等语言无法跨单词合并为新token，对于短语、固定搭配、固有名词不友好。
- 跨语言合并困难：依赖预分词，通常需要以空格为分隔符对文本进行预分词，对于无空格语言（如中文、日文）需要额外处理。
- 完全基于频率无语义信息：BPE 完全基于字符对的频率，未考虑语义或上下文信息，能导致一些语义上不合理的子词组合，不适合强语义任务。
- 静态词表，词表生成不可逆，新数据集需要重新训练词表。
- 对未登录词的处理有限，虽然 BPE 可以通过子词组合处理未登录词，但对于非常罕见的字符或符号还是会出现OOV。

## 1.4 BBPE: Byte-level BPE

为了改进BPE，出现了基于utf-8的BBPE。BPE的最小单位是字符级，而BBPE的最小单位是字节级，依赖于utf-8编码（utf-8是基于Unicode字符集的变长字符编码方案，长度为1-4字节）。其初始词汇表是一个字节的256种（2的8次）表示（initial_vocab = [bytes([byte]) for byte in range(256)]），其余和BPE一样。其基于utf-8的优点如下：
- 基于不会出现OOV。
- 跨语言通用性。
- 不基于空格分词，可以跨单词学习到词组。

# 2 WordPiece分词法：基于统计语言模型

基本信息
- 背景：最早由Google提出，主要用于Google的神经机器翻译（NMT）系统。2018年BERT的论文中被正式采用。Google 从未开源 WordPiece 训练算法的实现，只有基于已发表文献的最佳猜测。它可能并非 100％ 准确的。
- 作者/年份：Google Research，2016
- 论文：[Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909)
- 核心idea：基于概率语言模型，在每一步贪心最大化当前步骤的数据集似然概率，通过每次选择点互信息值最大的pair合并高效近似。
- 采用该分词法的模型：Google系列模型如BERT、DistilBERT，MobileBERT，Funnel Transformers 和 MPNET

## 2.1 训练步骤：基于统计的最大似然MLE合并

与BPE总体逻辑相似，也是每次从词表中选出两个子词合并成新的子词。但合并策略不同，不同于BPE选择最高频的字符对合并，WordPiece选择点互信息PMI (Pointwise Mutual Information) 值最大的字节对合并。即针对每个pair = (A, B)，计算：score = freq_of_pair / (freq_of_A × freq_of_B)，或者：
$$
PMI = \frac{P(AB)}{P(A)P(B)}
$$

通过将两部分合在一起的频率除以其中各部分的频率的乘积（**频率的分母是语料库中所有单词数量，分子是出现A或B的单词数量**），该算法优先合并那些在词汇表中单独出现出现的对。例如：
- 即使 ("un", "##able") 这对在词汇表中出现的频率很高，它也不一定会被合并，因为 "un" 和 "##able" 这两对可能会在很多其他词中出现，频率很高。
- 相比之下，像 ("hu", "##gging") 这样的对可能会更快地被合并（假设单词“hugging”在词汇表中出现的频率很高），因为 "hu" 和 "##gging" 可能分别出现的频率较低。

其他步骤类似BPE：
- 初始化所有单词的词频字典，如{"hug": 10, "pug": 5}
- 遍历词频字典所有的键（分隔后的单词如"hug"——>"h##u##g"）,计算所有相邻子词pair的点互信息。
- 选择PMI值最大的pair进行合并，修改词频字典。
- 迭代至退出，仅保存一个词表文件用于MM匹配。

原则上，应该每次都要遍历所有子词pair合并后，整个数据集的似然概率（所有句子中所有子词的频率值/概率值相乘），然后选择似然概率最大的那个执行合并。但这样的计算量太大，因此选择直接计算所有子词对的点互信息并选择值最大的进行合并（合并PMI值高的子词对，通常会显著提升数据集的似然概率，在理想情况下等价于极大似然）。


## 2.2 应用步骤：基于最大正向匹配MM的贪心分词
基于词表文件进行最大正向匹配
- 最大：将词表文件根据词的长度降序排列，从左往右遍历时优先选择更长的子词（不同于基于合并规则的BPE更倾向于优先合并较短的子词）。
- 正向：对输入input字符串，从前往后正向匹配input的字串。例如对于"He love you"，先匹配He。


## 2.3 优缺点

优点：
- 更注重语义信息：PMI点互信息不仅考虑字节对频率，更注重整体的似然概率，更考虑语义信息。此外，保留更多的子词（如un able），从而对未登陆词处理更好
- 更倾向于优先切分出较长的子词：BPE基于合并规则分词，快速，但低频词可能被拆分过细（例如ABC组成的词，AB往往更有先被分词）。WordPiece基于最大正向匹配，优先切分为长词元。

缺点：
- 计算开销大：基于语言模型，除了计算频率，还涉及乘除法（PMI），因此需要更多计算时间
- 多语言和跨单词不理想：和BPE类似，依赖预分词。



# 3 ULM一元语言模型

基本信息
- 背景：传统方法如BPE和WordPiece依赖预分词，在处理未登录词和多语言场景时表现不佳。ULM名称来源于其基于一元语言模型（Unigram Language Model），表示假设子词概率独立。
- 作者/年份：Taku Kudo（Google），2018
- 论文：[Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates](https://arxiv.org/abs/1804.10959)
- 核心idea：与WordPiece类似，也基于概率的语言模型，但假设单词独立出现只计算独立概率而非联合概率（点互信息PMI）。此外，训练时不依赖预分词从小到大合并词表，而非从大到小剔除低概率组合；且分词时不采用贪心，而是通过概率模型直接评估每个子词的出现概率。
- 采用该分词法的模型：T5、mBART


## 3.1 训练步骤：

### 3.1.1 边际似然概率

ULM算法的核心思想是最大化边际似然(Marginal Likelihood)。边际似然表示在所有可能的分词方案下当前语料的概率之和。例如对于句子X="playing football is ...",其边际似然为：
$$
P_{句边际似然}(\text{playing football is ...}) = P_{方案序列似然}(A) + P_{方案序列似然}(B) + ...
$$

其中A, B, ...代表对句子X的不同分词方案，P(x)则表示该子词序列的似然，由于假设子词概率独立，因此子词序列的似然等于子词概率连乘，例如对于M个子词的序列方案A：
$$
A = x_1 + x_2 +... + x_M = pl+ay+ing + foot + ball + is + ... \\
P(A) = \prod_{i=1}^M x_{i}
$$

每个子词都有一个概率值，句子的概率是其所有可能子词分割方式中概率最大的一个。ULM通过EM算法迭代剔除部分小概率方案保留最大似然。

### 3.1.2 算法过程

1. 初始化词表：从训练数据中初始化一个较大的子词词表（例如，所有字符和常见子词），此时每个子词可以计算得到一个初始概率。
2. 训练模型：EM算法
    - E-step（期望步）：对于每个句子，计算所有可能的子词分割方案（ABC...）及其概率边际似然概率。
    - M-step（最大化步）：使用维特比算法找到边际似然概率最大的分割方案K。根据分割方案K，更新子词的概率分布（增加新组合子词的概率，移除低概率的子词，逐步剪枝词表）
3. 剪枝词表：重复 E-step 和 M-step，直到词表大小达到预定值。

### 3.1.3 示例
假设我们有以下训练语料
```python
corpus = ["hug", "hug"]

初始词表：["h", "u", "g", "hu", "ug", "hug"]

初始概率均等分配：p=1/6≈0.1667
```
#### 第一轮迭代：
##### E-Step（统计所有概率方案的边际似然概率）：

对单个句子，例如"hug"进行所有可能分词

|分词方案|	概率计算|	归一化概率|
| :----: | :----: | :----: |
|h u g|	0.1667^3=0.00463|	0.306|
|hu g|	(0.1667)^2=0.0278|	0.439|
|h ug|	(0.1667)^2=0.0278|	0.439|
|hug|	0.1667|	0.816|


##### M-Step（更新概率）：

对所有句子分词结果的子词概率进行加总更新（初始是平均概率），加总方式是，对每个句子的每个方案，只要有这个子词，就加总。例如下面的h子词，同时出现在两个句子hug中的"h u g"方案和"h ug"方案中，因此需要把四个概率加总。总体如下：

```python
h:   0.306*2 + 0.439*2 = 1.490
u:   0.306*2 = 0.612
g:   0.306*2 + 0.439*2 = 1.490
hu:  0.439*2 = 0.878
ug:  0.439*2 = 0.878
hug: 0.816*2 = 1.632
总计数：1.490+0.612+1.490+0.878+0.878+1.632 = 7.0
```
归一化并更新如下
```python
h: 1.490/7 ≈ 0.213
u: 0.612/7 ≈ 0.087
g: 1.490/7 ≈ 0.213 
hu: 0.878/7 ≈ 0.125
ug: 0.878/7 ≈ 0.125
hug: 1.632/7 ≈ 0.233
```

#### 第二轮迭代：
##### E-Step（统计所有概率方案的边际似然概率）：

对单个句子，例如"hug"进行所有可能分词

|分词方案|	概率计算|	归一化概率|
| :----: | :----: | :----: |
|h u g|	0.2130 × 0.087 × 0.213 ≈ 0.0039|	0.092|
|hu g|	0.125 × 0.213 ≈ 0.0266|	0.625|
|h ug|	0.213 × 0.125 ≈ 0.0266|	0.625|
|hug|	0.233|	0.817|


##### M-Step（更新概率）：

计数：

```python
hug: 0.817*2 = 1.634
hu: 0.625*2 = 1.250
ug: 0.625*2 = 1.250
h: 0.092*2 = 0.184
u: 0.092*2 = 0.184 
g: 0.092*2 = 0.184
总计数：1.634+1.250+1.250+0.184+0.184+0.184 = 4.686
```
归一化并更新如下
```python
hug: 1.634/4.686 ≈ 0.349
hu: 1.250/4.686 ≈ 0.267
ug: 1.250/4.686 ≈ 0.267
h: 0.184/4.686 ≈ 0.039
u: 0.184/4.686 ≈ 0.039
g: 0.184/4.686 ≈ 0.039
```
经过多次迭代后概率将收敛，最终"hug"的概率会变为最高。可以将其选择为最优的分词方案。

## 3.2 应用步骤：非贪心，维特比算法
使用维特比算法（Viterbi Algorithm，本质是动态规划算法找最短路径）找到概率最大的子词分割方式。具体可见[通俗讲解维特比算法](https://zhuanlan.zhihu.com/p/28274845)




## 3.3 优缺点

优点
- 概率驱动：基于概率模型选择子词，能够更好地捕捉语义信息，支持输出带概率的多种分词结果。
- 灵活性：支持动态调整词表大小，适合不同任务需求。
- 多分词可能性：能够生成多个可能的分割结果，适合需要多样性的任务。
- 对未登录词处理较好：通过子词分解（一开始是全组合），能够较好地处理未登录词。

缺点
- 实现复杂度较高：相比 BPE，实现和训练成本更高。
- 分词速度较慢：相比 BPE，分词速度稍慢，尤其是在使用维特比算法时。
- 依赖初始词表：初始词表（所有字符和一些常见子词）的质量会影响最终分词效果。

# 4 SenetencePiece

基本信息
- 背景：将文本直接视为字符序列，无需依赖预分词（如空格或语言特定的分词规则），并支持多种子词算法（如 BPE 和 ULM）
- 作者/年份：Taku Kudo（Google），2018
- 论文：[Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing](https://arxiv.org/abs/1808.06226)。
- 核心idea：不管语言类型、空格、特殊字符等预处理要求，直接视作一整串的字符序列。只是一种包装改进，底层还是用现有的子词合并方案如基于概率语言模型的ULM或基于词频的BPE。
- 使用该分词法的模型：最广泛、主流，大模型采用，如GLM等。

## 4.1 训练

- 文本编码：将输入文本转换为 Unicode 字符序列。
- 子词模型训练：
  - 使用 BPE 或 ULM 算法从字符序列中学习子词单元。
  - 如果是 ULM 算法，会通过概率模型优化子词词表。
- 生成词表：根据训练结果生成子词词表及其概率分布。

## 4.2 应用
基于底层具体的子词分隔工具如BPE或ULM。

## 优缺点
优点：
- 多语言支持：传统分词方法依赖于语言特定的规则（如中文需要分词，英文依赖空格），难以统一处理多语言任务。SentencePiece是字符级别处理，将文本视为字符序列，支持任意语言的字符集（如 Unicode）
- 未登录词问题：传统方法难以处理未登录词（OOV），而子词分割可以有效缓解这一问题。
- 简化预处理：SentencePiece 直接对原始文本进行处理，无需预分词或复杂的预处理。

缺点
- 计算开销：训练子词模型需要较大的计算资源。

- 长文本处理：对于非常长的文本，子词分割可能导致效率下降。

- 可解释性差：子词分割结果可能难以直观理解。