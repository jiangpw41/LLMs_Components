{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 PPO奖励模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "现有微调框架例如LLaMa-Factory有PPO实现的功能，主要依赖于trl这个库，TRL（Transformer Reinforcement Learning）是由 Hugging Face 提供的一个开源库，\n",
    "专门用于使用强化学习（Reinforcement Learning, RL）训练基于 Transformer 的语言模型。它是一个全面的工具集，支持多种强化学习方法，\n",
    "包括监督微调（Supervised Fine-Tuning, SFT）、奖励建模（Reward Modeling, RM）、近端策略优化（Proximal Policy Optimization, PPO）和直接偏好优化（Direct Preference Optimization, DPO）\n",
    "LLaMa-Factory train/ppo/trainer.py文件\n",
    "\"\"\"\n",
    "\n",
    "if is_peft_available():\n",
    "    from peft import PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "\n",
    "class RewardTrainer(Trainer):\n",
    "    r\"\"\"\n",
    "    建议使用“AutoModelForSequenceClassification”，即分类模型，作为奖励模型。\n",
    "    奖励模型应该在成对的examples数据集上进行训练，其中每个examples都是一个两个sequence组成的元组。\n",
    "    应该训练奖励模型来预测配对中的哪个例子与手头的任务更相关。\n",
    "\n",
    "    训练数据需要满足特定格式，至少包含以下四个字段：\n",
    "    - `input_ids_chosen`\n",
    "    - `attention_mask_chosen`\n",
    "    - `input_ids_rejected`\n",
    "    - `attention_mask_rejected`\n",
    "    此外，还可以提供一个可选的 margin 字段，用于调整损失函数\n",
    "    \"\"\"\n",
    "    \n",
    "    _tag_names = [\"trl\", \"reward-trainer\"]\n",
    "\n",
    "    # 检查配置和参数，并初始化类\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Optional[Union[PreTrainedModel, nn.Module]] = None,      # 要训练的模型，推荐使用 AutoModelForSequenceClassification\n",
    "        args: Optional[RewardConfig] = None,                            # 训练配置，推荐使用 RewardConfig\n",
    "        data_collator: Optional[DataCollator] = None,                   # 数据整理器，默认使用 RewardDataCollatorWithPadding\n",
    "        train_dataset: Optional[Dataset] = None,\n",
    "        eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] = None,\n",
    "        tokenizer: Optional[PreTrainedTokenizerBase] = None,            # 分词器，\n",
    "        model_init: Optional[Callable[[], PreTrainedModel]] = None,     \n",
    "        compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,                 # 评估指标，默认为 compute_accuracy\n",
    "        callbacks: Optional[List[TrainerCallback]] = None,              # 插件，在程序特定位置执行特定功能。回调函数是一种“你先把它交给别人，然后别人在需要的时候再调用它”的机制。\n",
    "        optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (\n",
    "            None,\n",
    "            None,\n",
    "        ),\n",
    "        preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,\n",
    "        max_length: Optional[int] = None,                               # 批中序列的最大长度。如果要使用默认数据整理器，则需要此参数。\n",
    "        peft_config: Optional[Dict] = None,                             # 用于模型PEFT微调。\n",
    "    ):\n",
    "        # （1）检查训练配置并警告\n",
    "        if type(args) == TrainingArguments:\n",
    "            warnings.warn(\n",
    "                \"Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.\",\n",
    "                FutureWarning,\n",
    "            )\n",
    "            if max_length is not None:\n",
    "                warnings.warn(\n",
    "                    \"The `max_length` argument is deprecated and will be removed in a future version. Please use the `RewardConfig` to set `max_length` instead.\",\n",
    "                    FutureWarning,\n",
    "                )\n",
    "        else:\n",
    "            if max_length is not None and args.max_length is not None:\n",
    "                raise ValueError(\n",
    "                    \"You cannot specify both `max_length` and `args.max_length`. Please use the `RewardConfig` to set `max_length` once.\"\n",
    "                )\n",
    "            if max_length is not None and args.max_length is None:\n",
    "                warnings.warn(\n",
    "                    \"The `max_length` argument is deprecated and will be removed in a future version. Please use the `RewardConfig` to set `max_length` instead.\",\n",
    "                    FutureWarning,\n",
    "                )\n",
    "        # （2）PEFT设置：如果提供了 peft_config，代码会检查是否安装了 PEFT 库，并将模型包装为 PEFT 模型。\n",
    "        if not is_peft_available() and peft_config is not None:\n",
    "            raise ValueError(\n",
    "                \"PEFT is not installed and you passed a `peft_config` in the trainer's kwargs, please install it to use the PEFT models\"\n",
    "            )\n",
    "        elif is_peft_available() and peft_config is not None:\n",
    "            if not isinstance(model, PeftModel):\n",
    "                if getattr(model, \"is_loaded_in_8bit\", False) or getattr(model, \"is_quantized\", False):\n",
    "                    _supports_gc_kwargs = \"gradient_checkpointing_kwargs\" in list(\n",
    "                        inspect.signature(prepare_model_for_kbit_training).parameters\n",
    "                    )\n",
    "\n",
    "                    prepare_model_kwargs = {\"use_gradient_checkpointing\": args.gradient_checkpointing}\n",
    "\n",
    "                    if not _supports_gc_kwargs and args.gradient_checkpointing_kwargs is not None:\n",
    "                        warnings.warn(\n",
    "                            \"You passed `gradient_checkpointing_kwargs` in the trainer's kwargs, but your peft version does not support it. \"\n",
    "                            \"please update to the latest version of peft to use `gradient_checkpointing_kwargs`.\"\n",
    "                        )\n",
    "                    elif _supports_gc_kwargs and args.gradient_checkpointing_kwargs is not None:\n",
    "                        prepare_model_kwargs[\"gradient_checkpointing_kwargs\"] = args.gradient_checkpointing_kwargs\n",
    "\n",
    "                    model = prepare_model_for_kbit_training(model, **prepare_model_kwargs)\n",
    "\n",
    "                model = get_peft_model(model, peft_config)\n",
    "\n",
    "        # （3）如果评估指标没指定，则使用默认的\n",
    "        if compute_metrics is None:\n",
    "            compute_metrics = compute_accuracy\n",
    "\n",
    "        # （4）如果未提供 data_collator，则会使用默认的 RewardDataCollatorWithPadding，并设置 max_length\n",
    "        if data_collator is None:\n",
    "            if tokenizer is None:\n",
    "                raise ValueError(\n",
    "                    \"max_length or a tokenizer must be specified when using the default RewardDataCollatorWithPadding\"\n",
    "                )\n",
    "            if type(args) == TrainingArguments:\n",
    "                if max_length is None:\n",
    "                    warnings.warn(\n",
    "                        \"When using RewardDataCollatorWithPadding, you should set `max_length` in RewardConfig.\"\n",
    "                        \" It will be set to `512` by default, but you should do it yourself in the future.\",\n",
    "                        UserWarning,\n",
    "                    )\n",
    "                    max_length = 512\n",
    "            else:\n",
    "                if max_length is None and args.max_length is None:\n",
    "                    warnings.warn(\n",
    "                        \"When using RewardDataCollatorWithPadding, you should set `max_length` in RewardConfig.\"\n",
    "                        \" It will be set to `512` by default, but you should do it yourself in the future.\",\n",
    "                        UserWarning,\n",
    "                    )\n",
    "                    max_length = 512\n",
    "                if max_length is None and args.max_length is not None:\n",
    "                    max_length = args.max_length\n",
    "\n",
    "            data_collator = RewardDataCollatorWithPadding(tokenizer, max_length=max_length)\n",
    "\n",
    "            if args.remove_unused_columns:\n",
    "                try:  # for bc before https://github.com/huggingface/transformers/pull/25435\n",
    "                    args.remove_unused_columns = False\n",
    "                except FrozenInstanceError:\n",
    "                    args = replace(args, remove_unused_columns=False)\n",
    "                # warn users\n",
    "                warnings.warn(\n",
    "                    \"When using RewardDataCollatorWithPadding, you should set `remove_unused_columns=False` in your RewardConfig\"\n",
    "                    \" we have set it for you, but you should do it yourself in the future.\",\n",
    "                    UserWarning,\n",
    "                )\n",
    "\n",
    "            self.use_reward_data_collator = True\n",
    "        else:\n",
    "            self.use_reward_data_collator = False\n",
    "        \n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            args=args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            model_init=model_init,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=callbacks,\n",
    "            optimizers=optimizers,\n",
    "            preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "        )\n",
    "\n",
    "        # Add tags for models that have been loaded with the correct transformers version\n",
    "        if hasattr(self.model, \"add_model_tags\"):\n",
    "            self.model.add_model_tags(self._tag_names)\n",
    "\n",
    "    # 损失函数计算\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        model: Union[PreTrainedModel, nn.Module],\n",
    "        inputs: Dict[str, Union[torch.Tensor, Any]],\n",
    "        return_outputs=False,\n",
    "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, Dict[str, torch.Tensor]]]:\n",
    "        # （1）使用模型分别计算 chosen 和 rejected 序列的奖励分数（logits）：一个n类别的得分向量\n",
    "        rewards_chosen = model(\n",
    "            input_ids=inputs[\"input_ids_chosen\"],\n",
    "            attention_mask=inputs[\"attention_mask_chosen\"],\n",
    "            return_dict=True,\n",
    "        )[\"logits\"]\n",
    "        rewards_rejected = model(\n",
    "            input_ids=inputs[\"input_ids_rejected\"],\n",
    "            attention_mask=inputs[\"attention_mask_rejected\"],\n",
    "            return_dict=True,\n",
    "        )[\"logits\"]\n",
    "\n",
    "        # （2）计算损失：winner - loser - margin后整个向量输入进行sigmoid和log计算。如果提供了 margin，则在损失计算中加入该值\n",
    "        if \"margin\" in inputs:\n",
    "            loss = -nn.functional.logsigmoid(rewards_chosen - rewards_rejected - inputs[\"margin\"]).mean()\n",
    "        else:\n",
    "            loss = -nn.functional.logsigmoid(rewards_chosen - rewards_rejected).mean()\n",
    "\n",
    "        # （3）返回损失值和两者的评分\n",
    "        if return_outputs:\n",
    "            return loss, {\n",
    "                \"rewards_chosen\": rewards_chosen,\n",
    "                \"rewards_rejected\": rewards_rejected,\n",
    "            }\n",
    "        return loss\n",
    "\n",
    "    # 预测：在评估阶段计算模型输出\n",
    "    def prediction_step(\n",
    "        self,\n",
    "        model: Union[PreTrainedModel, nn.Module],\n",
    "        inputs: Dict[str, Union[torch.Tensor, Any]],\n",
    "        prediction_loss_only: bool,\n",
    "        ignore_keys: Optional[List[str]] = None,\n",
    "    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        if ignore_keys is None:\n",
    "            if hasattr(self.model, \"config\"):\n",
    "                ignore_keys = getattr(self.model.config, \"keys_to_ignore_at_inference\", [])\n",
    "            else:\n",
    "                ignore_keys = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss, logits_dict = self.compute_loss(model, inputs, return_outputs=True)\n",
    "\n",
    "        if prediction_loss_only:\n",
    "            return (loss, None, None)\n",
    "\n",
    "        loss = loss.detach()\n",
    "        logits = tuple(v for k, v in logits_dict.items() if k not in ignore_keys)\n",
    "        logits = nested_detach(logits)\n",
    "        # Stack accepted against rejected, mean over logits\n",
    "        # and softmax to get preferences between accepted and rejected to sum to 1\n",
    "        logits = torch.stack(logits).mean(dim=2).softmax(dim=0).T\n",
    "\n",
    "        labels = torch.zeros(logits.shape[0])\n",
    "        labels = self._prepare_inputs(labels)\n",
    "\n",
    "        return loss, logits, labels\n",
    "\n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        num_print_samples = kwargs.pop(\"num_print_samples\", 4)\n",
    "        self.visualize_samples(num_print_samples)\n",
    "        return super().evaluate(*args, **kwargs)\n",
    "\n",
    "    def visualize_samples(self, num_print_samples: int):\n",
    "        \"\"\"\n",
    "        可视化奖励模型的预测结果\n",
    "\n",
    "        Args:\n",
    "            num_print_samples (`int`, defaults to `4`):\n",
    "                The number of samples to print. Set to `-1` to print all samples.\n",
    "        \"\"\"\n",
    "        eval_dataloader = self.get_eval_dataloader()\n",
    "        table = defaultdict(list)\n",
    "        # 遍历评估数据集，解码 chosen 和 rejected 序列的文本。\n",
    "        for _, inputs in enumerate(eval_dataloader):\n",
    "            _, logits, _ = self.prediction_step(self.model, inputs, prediction_loss_only=False)\n",
    "            chosen_text = self.tokenizer.batch_decode(inputs[\"input_ids_chosen\"], skip_special_tokens=True)\n",
    "            rejected_text = self.tokenizer.batch_decode(inputs[\"input_ids_rejected\"], skip_special_tokens=True)\n",
    "            table[\"chosen_text\"].extend(gather_object(chosen_text))\n",
    "            table[\"rejected_text\"].extend(gather_object(rejected_text))\n",
    "            table[\"logits\"].extend(\n",
    "                gather_object([[round(inner_item, 4) for inner_item in item] for item in logits.tolist()])\n",
    "            )\n",
    "            if num_print_samples >= 0 and len(table[\"chosen_text\"]) >= num_print_samples:\n",
    "                break\n",
    "        df = pd.DataFrame(table)\n",
    "        if self.accelerator.process_index == 0:\n",
    "            print_rich_table(df[:num_print_samples])\n",
    "            if \"wandb\" in self.args.report_to:\n",
    "                import wandb\n",
    "\n",
    "                if wandb.run is not None:\n",
    "                    wandb.log({\"completions\": wandb.Table(dataframe=df)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 PPO stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_train(self, resume_from_checkpoint: Optional[str] = None) -> None:\n",
    "    r\"\"\"实现PPO阶段的训练循环，如Huggingface训练器中的_inner_training_loop（）。\"\"\"\n",
    "    if resume_from_checkpoint is not None:\n",
    "        raise ValueError(\"`resume_from_checkpoint` will be supported in the future version.\")\n",
    "\n",
    "    # （1）在训练开始之前，代码首先计算了训练过程中的一些关键参数：总的训练批次大小，考虑了设备数量、梯度累积步数和PPO缓冲区大小。\n",
    "    total_train_batch_size = (\n",
    "        self.args.per_device_train_batch_size\n",
    "        * self.args.gradient_accumulation_steps\n",
    "        * self.finetuning_args.ppo_buffer_size\n",
    "        * self.args.world_size\n",
    "    )\n",
    "    if self.args.max_steps > 0:\n",
    "        num_examples = total_train_batch_size * self.args.max_steps     # 总的训练样本数量\n",
    "        num_train_epochs = sys.maxsize                                  # 训练的总轮数。\n",
    "        max_steps = self.args.max_steps                                 # 训练的最大步数，用于控制训练的总迭代次数。\n",
    "        steps_in_epoch = self.args.max_steps                            # 每轮训练的步数。\n",
    "    else:\n",
    "        len_dataloader = len(self.dataloader)\n",
    "        num_examples = len(self.dataset)\n",
    "        num_train_epochs = self.args.num_train_epochs\n",
    "        max_steps = math.ceil(num_train_epochs * len_dataloader)\n",
    "        steps_in_epoch = len_dataloader\n",
    "\n",
    "    self.state.max_steps = max_steps\n",
    "    self.state.num_train_epochs = num_train_epochs\n",
    "    self.state.is_local_process_zero = self.is_local_process_zero()\n",
    "    self.state.is_world_process_zero = self.is_world_process_zero()\n",
    "\n",
    "    # （2）日志和状态初始化：这些信息通过 logger.info_rank0 打印到日志中，方便调试和监控。\n",
    "    logger.info_rank0(\"***** Running training *****\")\n",
    "    logger.info_rank0(f\"  Num examples = {num_examples:,}\")\n",
    "    logger.info_rank0(f\"  Num Epochs = {num_train_epochs:,}\")\n",
    "    logger.info_rank0(f\"  Instantaneous batch size per device = {self.args.per_device_train_batch_size:,}\")\n",
    "    logger.info_rank0(\n",
    "        \"  Total train batch size (w. parallel, buffer, distributed & accumulation) = {:,}\".format(\n",
    "            total_train_batch_size\n",
    "        )\n",
    "    )\n",
    "    logger.info_rank0(f\"  Gradient Accumulation steps = {self.args.gradient_accumulation_steps:,}\")\n",
    "    logger.info_rank0(f\"  Num optimization epochs per batch = {self.finetuning_args.ppo_epochs:,}\")\n",
    "    logger.info_rank0(f\"  Total training steps = {max_steps:,}\")\n",
    "    logger.info_rank0(f\"  Number of trainable parameters = {count_parameters(self.model)[0]:,}\")\n",
    "\n",
    "    # （3）数据迭代器和状态初始化\n",
    "    dataiter = iter(self.dataloader)    # 创建一个数据迭代器 dataiter，用于在训练过程中逐批次获取数据。\n",
    "    loss_meter = AverageMeter()         # 用于跟踪训练过程中的损失和奖励。\n",
    "    reward_meter = AverageMeter()       \n",
    "    self.callback_handler.on_train_begin(self.args, self.state, self.control)       # 触发训练开始前的回调函数。\n",
    "    \n",
    "    # （4）训练循环的核心部分是一个 for 循环，迭代 max_steps 次\n",
    "    for step in tqdm(range(max_steps), disable=not self.is_local_process_zero()):\n",
    "        # 4.1 获取数据批次：从数据迭代器中获取一个批次的数据 batch。如果迭代器耗尽，则重新创建迭代器并继续获取数据。\n",
    "        try:\n",
    "            batch = next(dataiter)\n",
    "        except StopIteration:\n",
    "            dataiter = iter(self.dataloader)\n",
    "            batch = next(dataiter)\n",
    "\n",
    "        # 4.2 生成查询和响应：遍历batch中的mini batch\n",
    "        # Get inputs\n",
    "        self.model.eval()\n",
    "        self.tokenizer.padding_side = \"right\"  # change padding side\n",
    "        queries, responses, rewards = [], [], []\n",
    "        for idx in range(0, self.config.batch_size, self.config.mini_batch_size):\n",
    "            # 从当前批次中生成查询（queries）和响应（responses），并\n",
    "            mini_batch_queries, mini_batch_responses = self.get_inputs(\n",
    "                batch[idx : idx + self.config.mini_batch_size]\n",
    "            )\n",
    "            # 计算对应的奖励（rewards）。\n",
    "            mini_batch_rewards = self.get_rewards(mini_batch_queries, mini_batch_responses)\n",
    "            queries.extend(mini_batch_queries)\n",
    "            responses.extend(mini_batch_responses)\n",
    "            rewards.extend(mini_batch_rewards)\n",
    "\n",
    "        # Run PPO step\n",
    "        self.model.train()\n",
    "        stats = self.step(queries, responses, rewards)\n",
    "        self.tokenizer.padding_side = \"left\"  # restore padding side\n",
    "        loss_meter.update(float(stats[\"ppo/loss/total\"]), n=len(rewards))\n",
    "        reward_meter.update(torch.stack(rewards).mean().item(), n=len(rewards))\n",
    "\n",
    "        if self.config.log_with is not None:\n",
    "            try:\n",
    "                batch[\"query\"] = self.tokenizer.batch_decode(queries, skip_special_tokens=True)\n",
    "                batch[\"response\"] = self.tokenizer.batch_decode(responses, skip_special_tokens=True)\n",
    "                self.log_stats(stats, batch, rewards)\n",
    "            except Exception:\n",
    "                logger.warning_rank0(\"Failed to save stats due to unknown errors.\")\n",
    "\n",
    "        self.state.global_step += 1\n",
    "        self.callback_handler.on_step_end(self.args, self.state, self.control)\n",
    "\n",
    "        if self.is_local_process_zero() and (step + 1) % self.args.logging_steps == 0:\n",
    "            logs = dict(\n",
    "                loss=round(loss_meter.avg, 4),\n",
    "                reward=round(reward_meter.avg, 4),\n",
    "                learning_rate=stats[\"ppo/learning_rate\"],\n",
    "                epoch=round(step / steps_in_epoch, 2),\n",
    "            )\n",
    "            tqdm.write(str(logs))\n",
    "            logs[\"step\"] = step\n",
    "            self.state.log_history.append(logs)\n",
    "            self.callback_handler.on_log(self.args, self.state, self.control, logs)\n",
    "            loss_meter.reset()\n",
    "            reward_meter.reset()\n",
    "\n",
    "        if (step + 1) % self.args.save_steps == 0:  # save checkpoint\n",
    "            self.save_model(\n",
    "                os.path.join(self.args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}\")\n",
    "            )\n",
    "            self.callback_handler.on_save(self.args, self.state, self.control)\n",
    "\n",
    "        if self.control.should_epoch_stop or self.control.should_training_stop:\n",
    "            break\n",
    "\n",
    "    self.callback_handler.on_train_end(self.args, self.state, self.control)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
