<!-- JPWçš„Markdownç¬”è®°æ¨¡æ¿ v1, å…¶ä¸­çš„hreféœ€è¦è§†æƒ…æ›´æ”¹ä¸Šçº§ç›®å½•href="../../format.css -->
<link rel="stylesheet" type="text/css" href="../../format.css">


<h1>LLMsç³»åˆ—è¿›é˜¶ï¼šEmbedding</h1>

ğŸ’¡ åˆ†è¯åï¼Œè‡ªç„¶è¯­è¨€æ–‡æœ¬å°±å˜æˆäº†tokençš„æ•´æ•°åºåˆ—ï¼Œä½†è¿™ç§åºåˆ—ï¼ˆæˆ–one-hotï¼‰åªæ˜¯å”¯ä¸€æ€§ç¼–ç ï¼Œå¹¶ä¸è•´å«è¯­ä¹‰ä¿¡æ¯ã€‚å› æ­¤éœ€è¦å¯¹æ¯ä¸ªtokenè¿›è¡Œè¯­ä¹‰åµŒå…¥embeddingï¼Œæ ¹æ®åµŒå…¥å‘é‡çš„ç¨€ç–æ€§å¯ä»¥åˆ†ä¸ºDense Embeddingå’ŒSparse Embeddingã€‚

# 1 Embeddingåº”ç”¨å››å¤§åœºæ™¯
embedding æ¨¡å‹çš„æ ¸å¿ƒåŠŸèƒ½æ˜¯å°†æ–‡æœ¬ï¼ˆæˆ–å…¶ä»–æ¨¡æ€çš„æ•°æ®ï¼‰è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºï¼Œä»è€Œæ”¯æŒå„ç§ NLP ä»»åŠ¡ã€‚ä¸»è¦æ”¯æŒä»¥ä¸‹å››å¤§ç±»ä¸‹æ¸¸ä»»åŠ¡ï¼š
- **ä¿¡æ¯æ£€ç´¢ï¼ˆInformation Retrieval, IRï¼‰**ï¼šè®¡ç®— queryï¼ˆæŸ¥è¯¢ï¼‰ å’Œ documentï¼ˆæ–‡æ¡£ï¼‰ ä¹‹é—´çš„å‘é‡ç›¸ä¼¼åº¦ï¼ŒåŒ¹é…æœ€ç›¸å…³çš„æ–‡æ¡£
  - æœç´¢å¼•æ“ï¼ˆGoogle Searchã€ä¼ä¸šå†…éƒ¨æœç´¢ï¼‰
  - æ¨èç³»ç»Ÿï¼ˆä¸ªæ€§åŒ–å†…å®¹æ¨èï¼‰
  - é—®ç­”ç³»ç»Ÿï¼ˆQAï¼‰æ”¯æŒï¼ˆå¦‚ RAGï¼šæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰
  - ä»£ç æœç´¢ã€æ³•å¾‹æ–‡æ¡£æ£€ç´¢
- **æ— ç›‘ç£ï¼šæ–‡æœ¬èšç±»ï¼ˆText Clusteringï¼‰**ï¼šå°†è¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬ç»„ç»‡åœ¨ä¸€èµ·
  - ä¸»é¢˜æ£€æµ‹ï¼ˆæ–°é—»ã€ç¤¾äº¤åª’ä½“åˆ†æï¼‰
  - è‡ªåŠ¨æ ‡ç­¾ç”Ÿæˆï¼ˆæ–‡æ¡£åˆ†ç±»ã€è®ºæ–‡åˆ†ç±»ï¼‰
  - å¼‚å¸¸æ£€æµ‹ï¼ˆåƒåœ¾é‚®ä»¶æ£€æµ‹ã€èˆ†æƒ…ç›‘æ§ï¼‰
- **æœ‰ç›‘ç£ï¼šæ–‡æœ¬åˆ†ç±»ï¼ˆText Classificationï¼‰**ï¼Œç”¨ embedding ä½œä¸ºç‰¹å¾ï¼Œå¯¹æ–‡æœ¬è¿›è¡Œåˆ†ç±»å­¦ä¹ 
  - æƒ…æ„Ÿåˆ†æï¼ˆSentiment Analysisï¼‰ï¼ˆå¥½è¯„/å·®è¯„åˆ†ç±»ï¼‰
  - åƒåœ¾é‚®ä»¶æ£€æµ‹ï¼ˆSpam Detectionï¼‰
  - æ³•å¾‹/åŒ»å­¦æ–‡æ¡£åˆ†ç±»
  - ç¤¾äº¤åª’ä½“ç›‘æ§ï¼ˆå¦‚å‡æ–°é—»æ£€æµ‹ï¼‰
- **è‡ªç›‘ç£Seq2Seqç”Ÿæˆä»»åŠ¡**ï¼šåºåˆ—åˆ°åºåˆ—ä»»åŠ¡ï¼Œè¾“å…¥ä¸€ä¸ªæ–‡æœ¬åºåˆ—ï¼ŒåŸºäº embedding è¿›è¡Œåºåˆ—ç”Ÿæˆ
  - æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰ï¼ˆå¦‚ Google Translateï¼‰
  - æ–‡æœ¬æ‘˜è¦ï¼ˆSummarizationï¼‰
  - ä»£ç ç”Ÿæˆï¼ˆCode Generationï¼‰
  - å¯¹è¯ç³»ç»Ÿï¼ˆChatbotsï¼‰

# 2 å¸¸ç”¨embeddingåº“

ä¸»æµçš„embeddingåº“æœ‰å¦‚ä¸‹å‡ ç§
- **Transformers**ï¼šHugging Faceï¼Œå¹¿æ³›æ”¯æŒ BERT, RoBERTa, GPT, T5, BART ç­‰æ¨¡å‹
- **OpenAI Embeddings**ï¼štext-embedding-ada-002ï¼ŒåŸºäº OpenAI GPT è®­ç»ƒï¼Œ1536 ç»´åµŒå…¥ï¼Œè¯­ä¹‰èƒ½åŠ›å¼ºï¼Œä½†æ”¶è´¹éœ€è°ƒç”¨ OpenAI API è·å–
- **SentenceTransformers**ï¼šä¸»æµï¼Œä¾èµ–äºHugging Face Transformersï¼Œå¯åŠ è½½ BERTã€RoBERTaã€MiniLMã€DistilBERT ç­‰æ¨¡å‹ã€‚å¯ä»¥ç”¨äºå¥å­ã€æ–‡æœ¬å’Œå›¾åƒåµŒå…¥çš„Pythonåº“ã€‚è¯¥æ¡†æ¶åŸºäº PyTorch å’Œ Transformersã€‚ç”± UKPLabï¼ˆå¾·å›½è¾¾å§†æ–½å¡”ç‰¹å·¥ä¸šå¤§å­¦ï¼ŒTechnical University of Darmstadtï¼‰ å¼€å‘å’Œç»´æŠ¤çš„ã€‚
- **Cohere Embeddings**ï¼šcohere.aiï¼Œæ”¯æŒ 1024 ç»´çš„é«˜ç²¾åº¦ embeddingï¼Œç±»ä¼¼ OpenAI embeddingï¼Œæä¾› API è®¿é—®

# 3 å¸¸ç”¨embeddingæ¨¡å‹
ä¸»æµçš„Embeddingæ¨¡å‹ä¸»è¦åˆ†ä¸ºä¼ ç»Ÿçš„é™æ€è¯å‘é‡æ¨¡å‹ã€ä¼ ç»ŸåŸºäºLSTMçš„åŠ¨æ€è¯å‘é‡æ¨¡å‹å’Œç›®å‰ä¸»æµçš„åŸºäºTransformersçš„åŠ¨æ€è¯å‘é‡æ¨¡å‹ä¸‰ç§ç±»å‹ã€‚ä¼ ç»Ÿçš„Word2Vecç”±äºå…¶å†…å­˜å ç”¨ä½åœ¨ç‰¹å®šçš„å®æ—¶æœç´¢åœºæ™¯è¿˜åœ¨ä½¿ç”¨ã€‚ä¸»æµçš„æ¨¡å‹åˆ™åŒ…æ‹¬

- Google
  - BERTåŠå…¶è¡ç”Ÿå¦‚RoBERTaï¼šEncoder-only
  - T5ç³»åˆ—ï¼šEncoder-Decoderï¼Œå°†å„ç§ä¸‹æ¸¸NLPä»»åŠ¡éƒ½ç»Ÿä¸€è½¬åŒ–æˆText-to-Textæ ¼å¼
- Meta(FaceBook)
  - BARTï¼šEncoder-Decoderï¼Œæ–‡æœ¬ç†è§£ä¸RoBERTaæŒå¹³ï¼Œå…¼é¡¾ç”Ÿæˆå¼ä»»åŠ¡ï¼Œæ¨¡å‹ä½“ç§¯ä»…æ¯”BERTå¤§10%ï¼Œæ€§ä»·æ¯”æé«˜ã€‚
- OpenAI
  - text-embedding-ada-002: Encoder-onlyåŸºäºGPT-3çš„Adaæ¶æ„,
- å¾®è½¯
  - MiniLM: å¯¹BERTæ¨¡å‹è¿›è¡Œè’¸é¦ï¼Œä½“é‡é€šå¸¸åªæœ‰å‡ åMBã€‚

## 3.1 ä¼ ç»ŸéTransformerè¯å‘é‡
- åŸºäºè¯è¢‹æ¨¡å‹çš„é™æ€è¯å‘é‡
  - Word2Vecï¼ˆGoogleï¼Œ2012ï¼‰: è¯å‘é‡ç»´åº¦50-300, æ»‘åŠ¨çª—å£ï¼ˆé€šå¸¸5-10è¯ï¼‰
  - GloVeï¼ˆStanfordï¼Œ2014ï¼‰: 
  - FastTextï¼ˆMetaï¼Œ2016ï¼‰
- åŸºäºLSTMçš„åŠ¨æ€è¯å‘é‡
  -  ELMoï¼ˆAllen AI2ï¼Œ2018ï¼‰
  -  CoVeï¼ˆMcCannï¼Œ2017ï¼‰
  -  ULMFitï¼ˆFast.ai+DeepMindï¼Œ2018ï¼‰

## 3.2 Transformerè¯å‘é‡

### 3.2.1 Encoder-onlyæ¨¡å‹
ä»£è¡¨æ˜¯BERTç³»åˆ—åŠå…¶å„ç§æ”¹è¿›å¦‚RoBERTa
- Google BERTç³»åˆ—
  - BERT-baseï¼š110M, 512 tokensï¼Œä¾‹å¦‚bert-base-chinese
  - BERT-large: 340M, 512 tokens
  - ALBERT: 12M (base) / 18M (large), 512 tokens
  - ELECTRA: 110M (base) / 335M (large), 512 tokens
 
- Meta(Facebook)
  - **RoBERTa**ï¼š125M (base) / 355M (large), 512 tokens, æ›´é²æ£’çš„è¯­ä¹‰è¡¨ç¤ºï¼Œé€‚ç”¨äºé—®ç­”å’Œæ–‡æœ¬æ¨ç†ã€‚ä¾‹å¦‚ä½¿ç”¨roberta-largeè¿›è¡Œç›¸ä¼¼åº¦è¯„ä¼°ã€‚
  - XLM-RoBERTa (XLM-R): 270M (base) / 550M (large), 512 tokens, 100+è¯­è¨€

### 3.2.2 ç”Ÿæˆå™¨æ”¹è¿›çš„Embedding
ä¸»è¦åŒ…å«ä½¿ç”¨Encoder-Decoderæ¶æ„çš„æ¨¡å‹
  - OpenAI text-embedding-ada-002: é«˜è´¨é‡è¯­ä¹‰åµŒå…¥ï¼Œ1536 ç»´ï¼Œ8192 tokens, é€‚ç”¨äºå„ç§ä»»åŠ¡
  - Google T5 (Text-To-Text Transfer Transformer): 220M (small) è‡³ 11B, 512 tokens
  - Meta (Facebook) BART: 140M (base) / 400M (large), 1024 tokens

### 3.2.3 è’¸é¦è½»é‡åŒ–æ¨¡å‹
ä¸»è¦æ˜¯å¯¹ä¸»æµå¤§å‹è¯å‘é‡æ¨¡å‹çš„è’¸é¦
- å¾®è½¯MiniLMç³»åˆ—ï¼šå¯¹BERTè¿›è¡Œè’¸é¦
  - MiniLM-L12-H384
  - MiniLM-L6-H384
  - all-MiniLM-L6-v2ï¼šæƒé‡çº¦90MBï¼Œè½»é‡é«˜æ•ˆï¼Œé€‚ç”¨äºå¤§è§„æ¨¡æ•°æ®
  - ms-marco-MiniLM-L-6-v2: æƒé‡çº¦90MBï¼Œè¯­ä¹‰æ£€ç´¢ï¼ˆç‰¹åˆ«æ˜¯ MS MARCO æ•°æ®é›†ä¼˜åŒ–çš„
- HuggingFace DistillBERTï¼šå¯¹BERTè¿›è¡Œè’¸é¦
  - distilbert-base-uncased: é€‚ç”¨äºå°å‹åˆ†ç±»ä»»åŠ¡

### 3.2.4 å…¶ä»–ï¼šå¤šè¯­è¨€ä¸ä¸“ç”¨æ¨¡å‹
- LaBSE (Language-agnostic BERT Sentence Embedding): 109ç§è¯­è¨€çš„è·¨è¯­è¨€è¯­ä¹‰åŒ¹é…ã€‚
- Universal Sentence Encoder (USE)
- LASER (Language-Agnostic SEntence Representations): 93ç§è¯­è¨€çš„é›¶æ ·æœ¬è¿ç§»å­¦ä¹ 

# 4 ä»£ç è°ƒç”¨
ä¸»è¦ä½¿ç”¨HFçš„transformersåº“æˆ–è€…sentence_transformersåº“æŒ‡å®šæ¨¡å‹ç±»å‹è¿›è¡Œè°ƒç”¨ã€‚

```python
from sentence_transformers import SentenceTransformer
sentences = ["This is an example sentence", "Each sentence is converted"]

model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
embeddings = model.encode(sentences)
print(embeddings)

# æˆ–è€…ä½¿ç”¨transformers
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')
```