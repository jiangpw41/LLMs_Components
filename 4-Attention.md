<!-- JPWçš„Markdownç¬”è®°æ¨¡æ¿ v1, å…¶ä¸­çš„hreféœ€è¦è§†æƒ…æ›´æ”¹ä¸Šçº§ç›®å½•href="../../format.css -->
<link rel="stylesheet" type="text/css" href="../../format.css">


<h1>LLMsç³»åˆ—è¿›é˜¶ï¼šAttentionæœºåˆ¶</h1>
ğŸ’¡ Attentionæœºåˆ¶æ¨¡æ‹Ÿç”Ÿç‰©æ³¨æ„åŠ›ï¼Œé€šè¿‡æŸ¥è¯¢é”®å€¼å¯¹ï¼ˆQKVï¼‰è¿›è¡Œè®¡ç®—ã€‚

# 1. åŸå§‹Attentionæ¶æ„ï¼šEncoder-Decoder

## 1.1 æ¶æ„æµç¨‹
åŸå§‹AttentionæŒ‡çš„æ˜¯2017å¹´Googleè®ºæ–‡Attention is all you needä¸­æå‡ºçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…¶é‡‡ç”¨äº†ç¿»è¯‘è¿™ä¸€Seq2Seqå¸¸ç”¨çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼ˆEncoder-Decoderï¼‰ã€‚ç›®å‰ä¸»æµå¤§æ¨¡å‹å‡æ²¿ç”¨è¿™ä¸€è®¾å®šï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šä¿®æ”¹ã€‚
<p align="center">
  <img src="images/æ³¨æ„åŠ›æœºåˆ¶å±‚çº§.PNG" width=100%><br>
</p>
å¦‚ä¸Šå›¾ï¼ŒåŸå§‹çš„Attentionæ¶æ„åŒ…å«Encoderå’ŒDecoderä¸¤ä¸ªç»„ä»¶ï¼Œæ¯ä¸ªç»„ä»¶ç”±Attentionä¸»æ¨¡å—ã€MLPä¸»æ¨¡å—å’Œæ®‹å·®å½’ä¸€åŒ–è¾…åŠ©æ¨¡å—ç»„æˆã€‚

Encoder-Decoderæ¶æ„ä¸»è¦ç”¨äºæœºå™¨ç¿»è¯‘ç­‰Seq2Seqä»»åŠ¡ï¼ˆè¾“å…¥è¾“å‡ºä¸ºåºåˆ—å¯¹ï¼‰ï¼Œå·¥ä½œæµç¨‹å¦‚ä¸‹ï¼š
- è®­ç»ƒé˜¶æ®µï¼šEncoderä¸€æ¬¡æ€§æ¥æ”¶æ‰€æœ‰è¾“å…¥åºåˆ—è½¬æ¢ä¸ºcontext vectorï¼ˆä¸Šä¸‹æ–‡å‘é‡ï¼‰ï¼Œå¹¶å°†è¿™ä¸ªå‘é‡ä½œä¸ºä¸Šä¸‹æ–‡æä¾›ç»™decoderçš„äº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼›decoderä»å¼€å§‹æ ‡è¯†ç¬¦\<start>ä½œä¸ºè¾“å…¥ï¼Œç»“åˆencoderçš„è¾“å…¥ï¼Œé¢„æµ‹è¾“å‡ºåºåˆ—çš„ä¸‹ä¸€ä¸ªtokenï¼Œå¹¶å’ŒçœŸå®çš„tokenæ¯”è¾ƒè®¡ç®—lossè¿›è¡Œåå‘ä¼ æ’­ï¼ˆæ›´æ–°encoderå’Œdecoderï¼‰ã€‚è¿™é‡Œé‡‡ç”¨teacher forcingæŠ€å·§ï¼Œå³æ¯ä¸ªtokené¢„æµ‹å®Œæ¯•åéƒ½ä½¿ç”¨çœŸå®çš„tokenä½œä¸ºåŸºç¡€é¢„æµ‹ä¸‹ä¸€ä¸ªã€‚

- æ¨ç†é˜¶æ®µï¼šencoderä½œç”¨ä¸€è‡´ï¼Œå¯¹è¾“å…¥åºåˆ—è¡¨ç¤ºä¸ºä¸Šä¸‹æ–‡å‘é‡ï¼Œæä¾›ç»™decoderä½¿ç”¨ã€‚decoderåˆ™æ²¡æœ‰è¾“å‡ºåºåˆ—ä½œä¸ºlabeläº†ï¼Œåªèƒ½ä»\<start>ä½œä¸ºåˆå§‹è¾“å…¥ï¼Œç»“åˆencoderè¾“å‡ºï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªtokenã€‚å¹¶è¿­ä»£ç›´è‡³ç”Ÿæˆç»“æŸæ ‡è¯†ç¬¦ã€‚

## 1.2 æ³¨æ„åŠ›æœºåˆ¶çš„æ–¹å‘

æ ¹æ®æ³¨æ„åŠ›æ–¹å‘çš„ä¸åŒï¼Œå¤§æ¨¡å‹ç»„ä»¶å¯ä»¥åˆ†ä¸ºEncoderï¼ˆåŒå‘Attentionï¼‰å’ŒDecoderï¼ˆå•å‘Attentionï¼‰ã€‚å‰è€…æ ¹æ®å‰åæ–‡å¡«ç©ºï¼Œé€‚ç”¨äºè¯­ä¹‰ç†è§£ã€å»ºæ¨¡ç±»ä»»åŠ¡ï¼›åè€…æ ¹æ®å‰æ–‡tokené¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼Œé€‚ç”¨äºç”Ÿæˆç±»ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œdecoder-onlyæ¶æ„ç”±åŒºåˆ†ä¸ºä¼ ç»Ÿçš„Causal-decoderï¼ˆæ³¨æ„åŠ›è®¡ç®—ä¸Šå½“å‰tokenå•å‘å…³æ³¨ä¹‹å‰æ‰€æœ‰tokenï¼‰å’ŒPrefix-decoderï¼ˆprefixéƒ¨åˆ†åŒå‘å…³æ³¨ï¼ŒååŠéƒ¨åˆ†å•å‘ï¼‰ã€‚

åŸå§‹çš„Attentionä¸»æ¨¡å—ä¸»è¦æœ‰ä¸‰ç§ç±»å‹ã€‚
- Multi-Head Self-Attentionï¼šï¼ˆæ— æ©ç ï¼‰å¤šå¤´è‡ªæ³¨æ„åŠ›
    - è‡ªæ³¨æ„åŠ›ï¼šæŒ‡QKVä¸‰çŠ¶æ€å‘é‡æ˜¯ç”±åŒä¸€ä¸ªtoken embeddingå‘é‡åœ¨ä¸‰ä¸ªä¸åŒçš„çŸ©é˜µï¼ˆQKVï¼‰ä¸ŠæŠ•å½±å¾—åˆ°ã€‚è¿™ä¸»ç”¨äºå’Œdecoderç»„ä»¶ä¸­çš„äº¤å‰æ³¨æ„åŠ›cross-attentionè¿›è¡ŒåŒºåˆ«ã€‚
    - å¤šå¤´ï¼šMHï¼Œå°†ä¸€ç»„QKVåˆ†è§£ä¸ºå¤šç»„ï¼Œæ¯ç»„éƒ½æœ‰è‡ªå·±çš„QçŸ©é˜µã€KçŸ©é˜µã€VçŸ©é˜µã€‚å¢å¼ºå¹¶è¡Œèƒ½åŠ›ã€ä¸°å¯Œæ¨¡å‹è¡¨è¾¾ã€æ›´åŠ é²æ£’ã€‚
    - åŒå‘æ³¨æ„åŠ›ï¼šæ²¡æœ‰attention maskï¼Œå¯¹è¾“å…¥tokenåºåˆ—ä¸­è¿›è¡Œä¸¤ä¸¤è®¡ç®—å…³æ³¨åº¦ã€‚
    - åº”ç”¨ï¼šEncoder-onlyæ¶æ„ä¸­çš„æ³¨æ„åŠ›ï¼Œä»¥åŠEncoder-Decoderæ¶æ„ä¸­çš„Encoderç»„ä»¶ï¼Œè´Ÿè´£è¡¨ç¤ºå­¦ä¹ ã€‚
- Masked Multi-Head Self-Attentionï¼šæ©ç å¤šå¤´è‡ªæ³¨æ„åŠ›
    - attention maskæ©ç ï¼šåœ¨è®­ç»ƒé˜¶æ®µå¯¹æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œæ–¹å‘æ§åˆ¶ï¼Œæœ€ä¸»æµçš„æ˜¯Causal-Decoderæ¶æ„ä¸­é‡‡ç”¨çš„å•å‘æ³¨æ„åŠ›ï¼Œå³å½“å‰tokenå¯¹è‡ªèº«åŠä¹‹å‰æ‰€æœ‰tokenè¿›è¡Œå…³æ³¨ï¼Œä½†ä¹‹å‰tokenä¸ä¼šå¯¹å½“å‰tokenè¿›è¡Œå…³æ³¨ã€‚æ­¤å¤–è¿˜æœ‰Prefix-Decoderï¼Œtokenåºåˆ—å‰åŠéƒ¨åˆ†é‡‡ç”¨åŒå‘çš„æ³¨æ„åŠ›ï¼ŒååŠéƒ¨åˆ†é‡‡ç”¨å•å‘çš„æ³¨æ„åŠ›ã€‚
    - åº”ç”¨ï¼šå› æœæ¨¡å‹ï¼Œå³Decoder-onlyæ¶æ„ï¼Œå½“å‰ä¸»æµçš„LLMsã€‚è¿™ç§next tokençš„é¢„æµ‹æ¨¡å¼é€‚åˆç”Ÿæˆä»»åŠ¡ã€‚
- Multi-Head Cross-Attentionï¼šå¤šå¤´äº¤å‰æ³¨æ„åŠ›
    - äº¤å‰æ³¨æ„åŠ›ï¼šå³Qæ¥è‡ªdecoderå†…éƒ¨å‰ä¸€ä¸ªAttentionæ¨¡å—çš„è¾“å‡ºï¼ŒKVæ¥è‡ªEncoderæ¨¡å—çš„è¾“å‡ºï¼‰
    - åº”ç”¨ï¼šä»…ç”¨äºEncoder-decoderæ¶æ„çš„decoderç»„ä»¶ä¸­ã€‚

æ ¸å¿ƒæ˜¯é€šè¿‡Attention Maskä½¿å¾—tokenäº¤äº’åªæ„ŸçŸ¥å‰å‘åºåˆ—ï¼Œå³é€šè¿‡ä¸€ä¸ªä¸‹ä¸‰è§’æœ‰æ•ˆçš„MaskçŸ©é˜µæ¥è¿›è¡ŒCausalæ©ç ã€‚

## 1.3 ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›SDPA
<p align="center">
  <img src="images/Attention.png" width=60%><br>
</p>



åœ¨AttentionåŸå§‹è®ºæ–‡ä¸­ï¼Œä½œè€…æ¯”è¾ƒäº†ä¸¤ç§æ³¨æ„åŠ›æ–¹æ³•ï¼Œåˆ†åˆ«ä¸ºç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›SDPAï¼ˆScaled Dot-Product Attentionï¼‰å’ŒåŠ æ€§æ³¨æ„åŠ›AAï¼ˆAdditive Attentionï¼‰ã€‚
- ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›SDPAï¼šå¦‚ä¸Šå¼ï¼ŒQKç‚¹ç§¯è·å¾—tokené—´çš„è¯„åˆ†çŸ©é˜µåï¼Œé™¤ä»¥æ ¹å·ä¸‹keyçš„ç»´åº¦è¿›è¡Œç¼©æ”¾ï¼ˆé˜²æ­¢äº†ç‚¹ç§¯å€¼è¿‡å¤§ï¼Œå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±æˆ–ä¸ç¨³å®šçš„æƒ…å†µï¼‰ï¼Œå¹¶ç”¨softmaxè¿›è¡Œ0-1å½’ä¸€åŒ–ï¼ˆè½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œæ‰€æœ‰çš„æ³¨æ„åŠ›æƒé‡ä¹‹å’Œä¸º 1ï¼Œè¿™æ ·å¯ä»¥æœ‰æ•ˆåœ°åˆ†é…æ³¨æ„åŠ›ï¼‰ï¼Œç„¶åå†å°†ç¼©æ”¾åçš„è¯„åˆ†çŸ©é˜µä¸Vè¿›è¡Œç‚¹ç§¯ï¼ˆåŠ æƒæ±‚å’Œï¼‰ã€‚ç›¸è¾ƒäºåŠ æ€§æ³¨æ„åŠ›ï¼Œè™½ç„¶éƒ½å¹³æ–¹å¤æ‚åº¦ï¼Œä½†æ­é…ç¡¬ä»¶å’Œæ•°å­¦åº“ç‚¹ç§¯è®¡ç®—æ•ˆç‡æ¯”çº¿æ€§æŠ•å½±å†ç›¸åŠ æ•ˆç‡æ›´é«˜ï¼Œä¸”ç¼©æ”¾åæ¢¯åº¦æ•°å€¼æ›´ç¨³å®šã€‚
- åŠ æ€§æ³¨æ„åŠ›ï¼šå¦‚ä¸‹ï¼ŒQKå‘é‡å…ˆé€šè¿‡çº¿æ€§å˜æ¢Wè¿›è¡Œæ˜ å°„ï¼Œå†åšç›¸åŠ ï¼Œç„¶åä¼ å…¥æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ ReLUï¼‰ã€‚è¿™ç§æ–¹å¼è®¡ç®—ç›¸å¯¹è¾ƒæ…¢ï¼Œå› ä¸ºå®ƒæ¶‰åŠæ›´å¤šçš„çŸ©é˜µè¿ç®—ï¼ˆå°¤å…¶æ˜¯åŠ æ³•å’Œæ¿€æ´»ï¼‰ï¼Œå¹¶ä¸”æ²¡æœ‰ç‚¹ç§¯é‚£ä¹ˆå®¹æ˜“å¹¶è¡ŒåŒ–ã€‚
$$e_{ij}=v^Ttanh(W_qQ_i+W_kK_j)$$

# 2. MHAä»£ç 
å¤šç»„QKVï¼Œå¯ä»¥å¹¶è¡Œè®¡ç®—ï¼Œæ•è·ä¸åŒçš„ç‰¹å¾ã€‚
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Optional, Tuple

class MultiHeadAttention( nn.Module ):
    """å¤šå¤´æ³¨æ„åŠ›ï¼Œqkvä¸‰è€…ç»´åº¦è¢«å‡åˆ†ä¸ºnum_headsä¸ªéƒ¨åˆ†ï¼Œå¢å¼ºå¹¶è¡Œèƒ½åŠ›å’Œè¡¨ç¤ºèƒ½åŠ›ï¼Œæ›´é²æ£’"""
    def __init__( self, config: XXXConfig):
        super().__init__()
        # configå‚æ•°å†…åŒ–ï¼šå¤´æ•°ã€ç»´åº¦ç­‰
        self.num_heads = config.num_heads      # å¤´æ•°ï¼Œå³qkvçš„ç»´åº¦è¢«å‡åˆ†ä¸ºå¤šå°‘éƒ¨åˆ†
        self.hidden_dim = config.hidden_dim    # åµŒå…¥ç»´åº¦embedding_dimï¼Œå³è¾“å…¥å‘é‡çš„æœ€åä¸€ä¸ªç»´åº¦
        self.qk_dim = config.qk_dim            # queryå’ŒkeyæŠ•å½±çŸ©é˜µçš„ç»´åº¦ï¼Œä¸¤è€…éœ€è¦ç‚¹ç§¯å› æ­¤ç»´åº¦å¿…é¡»ä¸€è‡´ï¼Œå¯ä»¥ä»»æ„ï¼Œä½†é€šå¸¸ç®€åŒ–ä¸ºä¸hidden_dimä¸€è‡´ã€‚
        self.v_dim = config.v_dim              # valueæŠ•å½±çŸ©é˜µçš„ç»´åº¦ï¼Œå¯ä»¥ä¸qkå’Œhidden_dimä¸ä¸€è‡´ï¼Œä½†é€šå¸¸ç®€åŒ–ä¸ºä¸hidden_dimä¸€è‡´ï¼Œå¦‚Baichuan2-7Bå°±æ˜¯ä¸‰è€…éƒ½ç­‰äºhidden_dim
        self.head_dim = self.hidden_dim // self.num_heads       # ä¹Ÿæœ‰ç›´æ¥è®¾ç½®ä¸ºconfig.kv_channelsæŒ‡å®šçš„ï¼Œå¦‚chatglm3-6b
        assert self.head_dim * self.num_heads == hidden_dim , "Embedding size must be divisible by num_heads"

        # æŠ•å½±çŸ©é˜µç»„ä»¶ï¼šä¸‹é¢ä¸‰ä¸ªæŠ•å½±çŸ©é˜µå¯ä»¥å†™ä¸ºä¸€ä¸ªself.W_packï¼Œè¦ç”¨æ—¶å†æ‹†åˆ†
        self.query_linear = nn.Linear( self.hidden_dim, self.qk_dim )
        self.key_linear = nn.Linear( self.hidden_dim, self.qk_dim )
        self.value_linear = nn.Linear( self.hidden_dim, self.v_dim )
        self.out_linear = nn.Linear( self.v_dim, self.hidden_dim)

        # æ—‹è½¬ä½ç½®ç¼–ç ç»„ä»¶
        self.max_position_embeddings = config.max_position_embeddings
        self.rotary_emb = RotaryEmbedding(self.head_dim, max_position_embeddings=self.max_position_embeddings)
    
    def forward(
        self, 
        hidden_states,                                          # è¾“å…¥çš„embeddingç»“æœ
        attention_mask: Optional[torch.Tensor] = None,          # æ©ç ï¼Œç”¨äºè®­ç»ƒå’Œbatchæ¨ç†mask padding
        position_ids: Optional[torch.LongTensor] = None,        # ä½ç½®idï¼Œç”¨äºRotaryæ—‹è½¬ä½ç½®ç¼–ç ç»„ä»¶
        past_key_value: Optional[Tuple[torch.Tensor]]  = None,  # æ˜¯å¦æœ‰ä¹‹å‰çš„Kv_cacheï¼ŒåŒºåˆ†é¦–æ¬¡è¿­ä»£å’Œä¹‹å
        use_cache: bool = False,                                # æ˜¯å¦å¯ç”¨kv_cache
    ):
        """
        inputs.shape = [batch_size, token_len, hidden_state]
        """
        batch_size, q_len = hidden_states.shape[0], hidden_states.shape[1]

        Q = self.query_linear( hidden_states )
        K = self.key_linear( hidden_states )
        V = self.value_linear( hidden_states )
        # å…ˆviewé‡å¡‘å†transposeï¼Œå¯ä»¥ä½¿å¾—å¼ é‡åœ¨å†…å­˜ä¸­æ•°æ®çš„æ’åˆ—æ–¹å¼ç¬¦åˆåç»­å¤šå¤´å¹¶è¡Œè®¡ç®—ï¼š
        # view æ“ä½œè¦æ±‚å¼ é‡åœ¨å†…å­˜ä¸­æ˜¯è¿ç»­çš„ï¼ˆcontiguousï¼‰ï¼Œview ä¸ä¼šæ”¹å˜å¼ é‡åœ¨å†…å­˜ä¸­çš„å®é™…å­˜å‚¨é¡ºåºï¼Œå®ƒåªæ˜¯é‡æ–°è§£é‡Šå¼ é‡çš„å½¢çŠ¶
        # transpose ä¸ä¼šæ”¹å˜å¼ é‡åœ¨å†…å­˜ä¸­çš„å®é™…å­˜å‚¨é¡ºåºï¼Œä½†å®ƒä¼šæ”¹å˜å¼ é‡çš„æ­¥å¹…ï¼ˆstrideï¼‰ï¼Œä»è€Œæ”¹å˜è®¿é—®æ•°æ®çš„æ–¹å¼ã€‚
        # å…ˆå°† query é‡å¡‘ä¸º (batch_size, seq_len, num_heads, head_dim)ç¡®ä¿ seq_len å’Œ head_dim åœ¨å†…å­˜ä¸­æ˜¯è¿ç»­çš„ã€‚å†å°† num_heads å’Œ seq_len çš„ç»´åº¦äº¤æ¢ï¼Œæ”¹å˜äº†ç»´åº¦é¡ºåºï¼Œä½†ä¿ç•™äº†æ¯ä¸ªå¤´çš„ seq_len å’Œ head_dim çš„è¿ç»­æ€§ã€‚
        # å¦‚æœç›´æ¥ä½¿ç”¨ query.view(batch_size, num_heads, -1, head_dim)ï¼Œè™½ç„¶å½¢çŠ¶æ˜¯å¯¹çš„ï¼Œä½†æ•°æ®åœ¨å†…å­˜ä¸­çš„æ’åˆ—å¯èƒ½ä¸ç¬¦åˆå¤šå¤´æ³¨æ„åŠ›çš„è®¡ç®—éœ€æ±‚ï¼Œå› ä¸º seq_len å’Œ head_dim å¯èƒ½ä¸å†æ˜¯è¿ç»­çš„ã€‚
        Q = Q.view( batch_size, q_len, self.num_heads, self.head_dim ).transpose( 1, 2) 
        K = K.view( batch_size, q_len, self.num_heads, self.head_dim ).transpose( 1, 2)
        V = V.view( batch_size, q_len, self.num_heads, self.head_dim ).transpose( 1, 2)

        # å¯¹QKè¿›è¡Œä½ç½®ç¼–ç ï¼šè¦æ±‚æ˜¯è·å¾—å½“å‰é•¿åº¦
        kv_seq_len = K.shape[-2]
        if past_key_value != None:
            kv_seq_len += past_key_value[0].shape[-2]
        cos, sin = self.rotary_emb( value_states, seq_len = kv_seq_len)
        Q, K = apply_rotary_pos_emb(Q, K, cos, sin, position_ids)

        # å†æ‹¼æ¥kv_cacheä¸­çš„Kå’ŒV
        if past_key_value != None:
            K, V = torch.cat( [past_key_value[0], K], dim = 2 ), torch.cat( [past_key_value[1], V], dim = 2 )       # åœ¨q_lenç»´åº¦è¿›è¡Œæ‹¼æ¥
        # æ›´æ–°kv_cache
        if use_cache:
            past_key_value = (K, V)

        # è¿›è¡Œç¼©æ”¾ç‚¹ç§¯SDPA
        attn_output = F.scaled_dot_product_attention(Q, K, V, attn_mask = attention_mask)       # æˆ–è®¾ç½®is_causal=Trueï¼Œä¹Ÿæ˜¯é»˜è®¤å•å‘æ³¨æ„åŠ›
        attn_output = attn_output.transpose(1, 2)
        attn_output = attn_output.reshape( batch_size, q_len, self.hidden_dim)

        # æœ€åè¿›è¡Œæ··æ·†
        attn_output = self.out_linear( attn_output )
        return attn_output, past_key_value
```

# 3. GHAçš„ç»“æ„å˜ä½“ï¼šMQAã€GQA

<p align="center">
  <img src="images/mha.png" width=100%><br>
</p>

æœ€åŸå§‹çš„Attentionæœºåˆ¶é‡‡å–äº†MHAå¤šå¤´æ³¨æ„åŠ›çš„ç»“æ„ï¼Œå³QKVæœ‰å¤šç»„ï¼Œå¹³åˆ†hidden_statesç»´åº¦ã€‚è¿™ä¸€ç»“æ„ä½¿å¾—ä¸åŒçš„headä¹‹é—´å¯ä»¥å¹¶è¡Œè®¡ç®—ï¼Œä¸”ç›¸äº’ç‹¬ç«‹ï¼Œ
æ¨¡å‹æœ‰æ›´å¥½çš„è¡¨è¾¾èƒ½åŠ›ï¼Œä½†è®¡ç®—å¼€é”€è¾ƒå¤§ã€‚ä¸ºäº†é™ä½è®¡ç®—å¼€é”€å’ŒKv_cacheæ˜¾å­˜å ç”¨ï¼Œå¢åŠ ä¸Šä¸‹æ–‡é•¿åº¦ï¼ŒMHAå‡ºç°äº†å˜ä½“å¤šqueryæ³¨æ„åŠ›MQAå’Œç»„queryæ³¨æ„åŠ›GQAï¼š
- Multi-Query Attention (MQA)ï¼šMHAçš„æç®€ç‰ˆæœ¬ï¼Œåªæœ‰Queryä¿ç•™å¤šç»„ï¼ŒKVå¯¹åˆ™åªæœ‰ä¸€å¯¹ï¼Œå¤šç»„Queryå…±äº«ä¸€å¯¹KVï¼Œå¤§å¤§æå‡è®¡ç®—æ•ˆç‡ï¼Œä½†è¡¨è¾¾èƒ½åŠ›å—é™ï¼Œæ€§èƒ½å—æŸã€‚
- Grouped Query Attention (GQA)ï¼šMHAå’ŒMQAçš„æŠ˜ä¸­ï¼Œå³Queryåˆ†ç»„å…±äº«å¤šç»„KVå¯¹ï¼Œé€‚åˆéœ€è¦åœ¨è®¡ç®—æ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ä¹‹é—´æƒè¡¡çš„ä»»åŠ¡ã€‚éœ€è¦æ‰‹åŠ¨è°ƒæ•´åˆ†ç»„æ•°é‡ï¼Œå¢åŠ äº†è°ƒå‚çš„å¤æ‚æ€§ã€‚
æ€»ä½“è€Œè¨€ï¼ŒMQAæ˜¯GQAåœ¨ç»„æ•°ä¸º1æ—¶çš„ç‰¹ä¾‹ã€‚LLaMaæ¶æ„é‡‡ç”¨çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚

# 4. GQAå’ŒMQAä»£ç 
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, List, Tuple

class MultiQueryAttention( nn.Mudule ):
    """
    MQAå’ŒGQAï¼Œå…¶ä¸­å‰è€…æ˜¯åè€…ä¸€ä¸ªç‰¹ä¾‹ï¼Œå³groupæ•°é‡ä¸º1ã€‚
    """
    def __init__( self, config: XXXConfig ):
        # configå‚æ•°å†…åŒ–
        self.hidden_dim = config.hidden_dim         # embeddingç»´åº¦
        self.qk_dim = config.qk_dim
        self.v_dim = config.value_dim

        self.num_heads = config.num_heads           # queryç»„æ•°
        self.head_dim = self.qk_dim // self.num_heads

        self.num_groups = config.num_groups         # kvç»„æ•°ï¼Œä¸º1æ—¶æ˜¯MQAï¼Œ>1æ—¶ä¸ºGQA
        self.query_per_kv = self.num_heads // self.num_groups
        assert self.query_per_kv * self.num_groups == self.num_heads, "GQAç»„æ•°å¿…é¡»å¯ä»¥æ•´é™¤Queryå¤´æ•°"

        # çº¿æ€§å±‚å®ä¾‹åŒ–
        self.query_linear = nn.Linear( self.hidden_dim, self.qk_dim * self.num_heads )
        self.key_linear = nn.Linear( self.hidden_dim, self.qk_dim * self.num_groups )
        self.value_linear = nn.Linear( self.hidden_dim, self.v_dim * self.num_groups )
        self.out_linear = nn.Linear( self.v_dim * self.num_groups * , self.hidden_dim )

        # ä½ç½®ç¼–ç å±‚
        self.rotary_emb = RotaryEmbedding( self.qk_dim, max_rotary_embeddings = self.max_rotary_embeddings)
    
    def forward(
        self, 
        hidden_states,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor]] = None,
        use_cache: bool = False
    ):
        """
        hidden_states.shape = [batch_size, q_lens, hidden_dim]
        """
        batch_size, q_lens = hidden_states.shape[0], hidden_states.shape[1]
        Q = self.query_linear( hidden_states ).view( batch_size, q_lens, self.num_heads, self.qk_dim )
        K = self.key_linear( hidden_states ).view( batch_size, q_lens, self.num_groups, self.qk_dim )
        V = self.value_linear( hidden_states ).view( batch_size, q_lens, self.num_groups, self.v_dim )
        
        # ä½ç½®ç¼–ç 
        kv_seq_len = K.shape[1]
        if past_key_value:
            kv_seq_len += past_key_value[0].shape[1]
        cos, sin = self.rotary_emb( V, kv_seq_len )
        Q, K = apply_rotary_pos_emb(Q, K, cos, sin, position_ids)

        # KV_cacheï¼šåœ¨seqç»´åº¦æ‰©å±•
        if past_key_value:
            K, V = torch.cat( [past_key_value[0], K], dim = 1), torch.cat( [past_key_value[1], V], dim = 1)
        if use_cache:
            past_key_value = (K. V)
        
        # æ‰©å±•ä»¥é€‚åº”MQAå’ŒGQAç‚¹ç§¯
        # å°†KVçš„[batch_size, q_lens, num_groups, dim]å››ä¸ªç»´åº¦çš„å€’æ•°ç¬¬äºŒä¸ªç»´åº¦å¤„æ’å…¥ä¸€ä¸ªç»´åº¦ï¼Œå˜æˆ[batch_size, q_lens, 1, num_groups, dim]
        # å¹¶åœ¨è¯¥ç¬¬å››ä¸ªç»´åº¦å¤åˆ¶ num_heads // num_groups ä»½åä¸ç¬¬ä¸‰ä¸ªçš„num_groupç»´åº¦åˆå¹¶ï¼Œä½¿å¾—KVä¸Qåœ¨ç¬¬ä¸‰ä¸ªç»´åº¦çš„ç»´æ•°ä¸€è‡´éƒ½ç­‰äº (num_heads // num_groups)*num_groups = num_headsï¼Œä¾¿äºè®¡ç®—
        K = K.unsqueeze( -2 )
        K = K.expand( -1, -1, -1, self.num_heads // self.num_groups, -1)
        K = K.contiguous().view( k.shape[:2] + ( self.num_heads ,self.qk_dim) )
        V = V.unsqueeze( -2 )
        V = V.expand( -1, -1, -1, self.num_heads // self.num_groups, -1)
        V = V.contiguous().view( k.shape[:2] + ( self.num_heads ,self.v_dim) )

        # è°ƒæ•´å½¢çŠ¶ä¸º batch_size, num_heads, q_lens, dimè¿›è¡Œå¹¶è¡Œè®¡ç®—SDPA
        Q, K, V = [ states.transpose( 1,2 ) for states in [Q, K, V]]
        attn_output = F.scaled_dot_product_attention( Q, K, V, is_causal = True )
        attn_output = attn_output.transpose(1, 2)
        attn_output = attn_output.reshape( batch_size, q_len, self.num_groups * self.v_dim)

        # æœ€åè¿›è¡Œæ··æ·†
        attn_output = self.out_linear( attn_output )
        return attn_output, past_key_value
```

# 5. é«˜æ•ˆAttentionï¼šFlash/Page/MLA
ç”±äºAttentionæœºåˆ¶æ˜¯LLMsè®¡ç®—çš„æ ¸å¿ƒï¼Œè€Œå…¶è®¡ç®—å¤æ‚åº¦æ˜¯O(n^2)ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨æ ‡å‡†çš„è‡ªæ³¨æ„åŠ›è®¡ç®—ä¸­ï¼Œé€šå¸¸éœ€è¦å­˜å‚¨ä»¥ä¸‹å‡ é¡¹å†…å®¹ï¼š
- QK^Tï¼ˆæ³¨æ„åŠ›æƒé‡çŸ©é˜µï¼‰
- Softmax ç»“æœ
- åŠ æƒåçš„ V å€¼
ä¸ºäº†æé«˜è®¡ç®—æ•ˆç‡ï¼Œå‡ºç°äº†ä¸€äº›ä¼˜åŒ–åŠ é€Ÿæ–¹æ¡ˆã€‚å…¶ä¸­æœ€è‘—åçš„æ˜¯Flash Attentionå’ŒPage Attentionã€‚

## 5.1 Flash Attention
é€šè¿‡åˆ†å—è®¡ç®—QKç‚¹ç§¯åŠå…¶Softmaxæµæ°´çº¿åŒ–ï¼Œå¤§å¤§å‡å°‘å†…å­˜æ¶ˆè€—ã€‚
### 5.1.1 tiling(å¹³é“º): åˆ†å—è®¡ç®—
QKç‚¹ç§¯å¯ä»¥åˆ†å—è®¡ç®—ï¼Œä½†å› ä¸ºAttentionè®¡ç®—ä¸­æ¶‰åŠSoftmaxï¼Œæ‰€ä»¥éœ€è¦æ ¹æ®softmaxæ“ä½œçš„row-wiseç‰¹æ€§ï¼ˆå³æ¯è¡Œéƒ½ç®—ä¸€æ¬¡softmaxï¼‰è¿›è¡Œå¹³é“ºåˆ†å—è®¡ç®—ã€‚

### 5.1.2 recomputationï¼ˆé‡æ–°è®¡ç®—ï¼‰
å¼•å…¥äº† Softmax è®¡ç®—çš„ç»Ÿè®¡é‡ï¼Œå®ƒä¸å†å°†æ•´ä¸ªæ³¨æ„åŠ›çŸ©é˜µ QK^T å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼Œè€Œæ˜¯åŠ¨æ€è®¡ç®— Softmax ç»“æœï¼Œå¹¶åœ¨æ¯ä¸ªæ­¥éª¤ä¸­é‡Šæ”¾æ— ç”¨çš„çŸ©é˜µï¼Œè¿›è€Œå‡å°‘æ˜¾å­˜å ç”¨ã€‚

## 5.2 Page Attention
vLLMçš„æ ¸å¿ƒæŠ€æœ¯ï¼Œå€Ÿé‰´æ“ä½œç³»ç»Ÿå†…å­˜Pageå‘½ä¸­å’Œæ¸…é€€é€»è¾‘ï¼Œå°†æ³¨æ„åŠ›çŸ©é˜µçš„è®¡ç®—åˆ†å¸ƒåˆ°å¤šä¸ªå†…å­˜é¡µä¸­æ¥ä¼˜åŒ–è®¡ç®—ã€‚
å…·ä½“æ¥è¯´ï¼Œæ³¨æ„åŠ›çŸ©é˜µé€šå¸¸ä¼šå ç”¨å¤§é‡å†…å­˜ï¼Œè€Œ Page Attention å°†çŸ©é˜µåˆ’åˆ†ä¸ºå¤šä¸ªè¾ƒå°çš„ "é¡µ"ï¼Œæ¯ä¸€é¡µå¯ä»¥å¹¶è¡Œå¤„ç†ã€‚è¿™ç§æ–¹å¼å‡å°‘äº†å†…å­˜çš„å ç”¨å¹¶æé«˜äº†è®¡ç®—çš„å¹¶è¡Œæ€§ã€‚

## 5.3 Multi-head Latent Attention (MLA)
å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›ï¼ŒDeepSeekä¸­çš„é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡ä½ç§©è”åˆå‹ç¼©æŠ€æœ¯ï¼Œå°†kv_cacheä¸­åŸæœ¬è¦ä¿å­˜çš„kvå¯¹ç”¨å‹ç¼©åçš„ä½ç§©è”åˆå‘é‡$c_t^{KV}$ä»£æ›¿ï¼Œå‡å°‘æ¨ç†æ—¶KV cacheç¼“å­˜ï¼Œä»è€Œåœ¨ä¿æŒæ€§èƒ½æ—¶æ˜¾è‘—é™ä½å†…å­˜å ç”¨ã€‚

åŒæ—¶ï¼ŒMLAéœ€è¦ä¿®æ”¹RoPEçš„ä½¿ç”¨ï¼Œå› ä¸ºé€šè¿‡ä½ç§©å‹ç¼©åçš„$k_t = RW^{UK}c_j^{KV}$ï¼Œæ­¤æ—¶ä¸$q_t^T=q_t^CR_t^T$åšçŸ©é˜µä¹˜æ³•æ—¶$q_t^CR_t^T RW^{UK}c_j^{KV}$ï¼Œä½ç½®ä¿¡æ¯ä¸ä½œç”¨åˆ°$W^{UK}$çŸ©é˜µä¸Šè€Œé$c_j^{KV}$ä¸Šï¼ˆçŸ©é˜µä¹˜æ³•ä¸æ»¡è¶³äº¤æ¢å¾‹ï¼‰ï¼Œå› æ­¤éœ€è¦å°†kçš„è®¡ç®—è§£è€¦ï¼Œå°†kåˆ†ä¸ºå¸¦RoPEçš„éƒ¨åˆ†å’Œä¸å¸¦çš„éƒ¨åˆ†ï¼Œåˆ†åˆ«è¿›è¡Œè®¡ç®—ã€‚https://zhuanlan.zhihu.com/p/15153745590

ä¸»è¦æ€è·¯ï¼š

<p align="center">
  <img src="images/MLA.jpg" width=80%><br>
</p>


### 5.3.1 å¯¹KVè¿›è¡Œä½ç§©è”åˆå‹ç¼©
ç”¨ä¸¤ä¸ªçŸ©é˜µWè¿›è¡Œçº¿æ€§å˜æ¢ï¼Œ$W^{KR}$å¯¹$h_t$è¿›è¡Œéå‹ç¼©çš„å˜æ¢ç”¨äºRoPEç”Ÿæˆ$k_t^R$ï¼›$W^{DKV}$å¯¹$h_t$è¿›è¡Œå‹ç¼©çš„å˜æ¢ç”Ÿæˆéšå‘é‡$c_t^{KV}$ï¼Œå†è§£å‹ç¼©å¾—åˆ°$k_t^C$å’Œ$v_t^C$ã€‚
- å‹ç¼©é”®å€¼ï¼šå…ˆç”¨ä¸€ä¸ªä¸‹æŠ•å½±çŸ©é˜µ$W^{DKV}$å°†$d$ç»´çš„è¾“å…¥$h_t$ï¼ˆè¡¨ç¤ºç¬¬tä¸ªtokençš„embeddingï¼‰æŠ•å½±åˆ°$d_c$ç»´æˆä¸ºlatentæ½œåœ¨KVå‘é‡$c_t^{KV}$ï¼Œå…¶ä¸­$d_c$è¿œè¿œå°äº$d_hn_h$ï¼ˆdim_head*num_headï¼‰ï¼Œå¾€å¾€ä¸º1/4æˆ–1/8ã€‚
$$c_t^{KV} = W^{DKV}h_t$$

- é‡å»ºé”®å€¼ï¼ˆå®é™…ä¸Šè®¡ç®—æ—¶ä¸ä¼šè§£å‹ç¼©ï¼Œè€Œæ˜¯çŸ©é˜µå¸æ”¶ï¼‰ï¼šç„¶åå†é€šè¿‡ä¸¤ä¸ªè¾“å…¥ç»´åº¦å‡ä¸º$d_c$ã€è¾“å‡ºç»´åº¦å‡ä¸º$d_hn_h$çš„ä¸ŠæŠ•å½±çŸ©é˜µ$W^{UK}$å’Œ$W^{UV}$ï¼Œå°†å‹ç¼©åçš„æ½œåœ¨KVå‘é‡$c_t^{KV}$é‡å»ºä¸º$d_hn_h$ç»´çš„Kå’ŒVçŸ©é˜µã€‚
$$k_t^C=W^{UK}c_t^{KV}\\v_t^C=W^{UV}c_t^{KV}$$

- è§£è€¦é”®å¹¶åº”ç”¨RoPEï¼šç›´æ¥å¯¹è¾“å…¥hidden_statesåº”ç”¨ä¸€ä¸ªå˜æ¢$W^{KR}$ï¼Œç”Ÿæˆè§£è€¦é”®çŸ©é˜µã€‚çŸ©é˜µçš„è¾“å…¥ç»´åº¦ä¸ºåŸå§‹çš„$d$ï¼Œè¾“å‡ºç»´åº¦ä¸º$d_h^R$ï¼Œè¡¨ç¤ºRoPEç¼–ç åçš„headç»´åº¦ã€‚
$$k_t^R=\text{RoPE}(W^{KR}h_t)$$

- æœ€ç»ˆKVï¼šé”®ç”±é‡å»ºé”®å’Œè§£è€¦é”®ä¸¤éƒ¨åˆ†ç»„åˆè€Œæˆï¼Œå€¼Våˆ™æ˜¯é‡å»ºå€¼ã€‚
$$k_t = [k_t^C;k_t^R]\\
v_t=v_t^C$$

### 5.3.2 å¯¹Qè¿›è¡Œä½ç§©å‹ç¼©
ä¸Kä¸€æ ·ï¼Œå…ˆä¸‹æŠ•å½±å‹ç¼©åˆ°$d_c'$ç»´åº¦å¾—åˆ°å‹ç¼©å‘é‡$c_t^Q$ï¼Œå¯¹å‹ç¼©å‘é‡è¿›è¡Œè§£è€¦å’ŒRoPEç¼–ç ï¼ˆä¹Ÿæ˜¯ç”¨ä¸¤ä¸ªæƒé‡Wåˆ†åˆ«å¯¹å‹ç¼©åçš„è¿›è¡Œè½¬æ¢ï¼‰ï¼Œå…¶ä¸­$W^{UQ}$ç”¨äºéRoPEå¾—åˆ°$q_t^C$ï¼Œ$W^{QR}$ç”¨äºRoPEå¾—åˆ°$q_t^R$ã€‚

$$c_t^Q = W^{DQ}h_t \\ 
q_t^C=W^{UQ}c_t^Q \\
q_t^R = \text{RoPE}(W^{QR}c_t^Q)\\
q_t = [q_t^C;q_t^R]
$$
ä¸Šå¼ä¸­Cè¡¨ç¤ºcompresså‹ç¼©ï¼ŒRè¡¨ç¤ºRoPEç¼–ç ï¼Œä¹Ÿå³QKéƒ½æ˜¯å‹ç¼©å’Œç¼–ç åçš„ç»„åˆï¼ŒVä»…æ˜¯å‹ç¼©åçš„ã€‚

### 5.3.3 QKè®¡ç®—çŸ©é˜µå¸æ”¶
åˆ†åˆ«å¯¹QKVè¿›è¡Œä½ç§©å‹ç¼©åï¼Œæˆ‘ä»¬å¾—åˆ°è§£è€¦çš„qkå’Œvå¦‚ä¸‹
$$
q_t = [q_t^C;q_t^R] \\

k_t = [k_t^C;k_t^R] \\

v_t=v_t^C
$$

æ­¤æ—¶qkè®¡ç®—åˆ†ä¸ºRoPEéƒ¨åˆ†å’ŒéRoPEéƒ¨åˆ†ï¼ŒéRoPEéƒ¨åˆ†å¦‚ä¸‹ï¼š
$$
q_t^{CT}k_t^C = (W^{UQ}c_t^Q )^T W^{UK}c_t^{KV} = c_t^QW^{UQ}W^{UK}c_t^{KV} = \\
(c_t^{QT}W^{UQT}W^{UK})c_t^{KV}
$$
é€šè¿‡ä½¿ç”¨çŸ©é˜µä¹˜æ³•çš„ç»“åˆç‡ï¼Œå…ˆè¡Œè®¡ç®—å‰ä¸€éƒ¨åˆ†ï¼Œé¿å…äº†å„è‡ªè®¡ç®—æœ€ç»ˆQKçŸ©é˜µè¿‡ç¨‹ä¸­$W^{UK}c_t^{KV}$å°†å‹ç¼©çš„å‘é‡è§£å‹ç¼©ä¸ºå®Œæ•´çš„KçŸ©é˜µï¼Œä¼˜åŒ–äº†æ˜¾å­˜å’Œè®¡ç®—é‡ï¼›åŒæ—¶ï¼Œ$W^{UQT}W^{UK}$éƒ¨åˆ†æ˜¯æ¨¡å‹æƒé‡ï¼Œå¯ä»¥åœ¨ä¸€å¼€å§‹å°±ç¦»çº¿è®¡ç®—ï¼ŒåŠ é€Ÿæ¨ç†è¿‡ç¨‹ï¼Œä¸¤ä¸ªWçŸ©é˜µå…ˆè®¡ç®—çš„è¿‡ç¨‹å«**çŸ©é˜µå¸æ”¶**ã€‚åŒç†ï¼Œæ³¨æ„åŠ›çŸ©é˜µMå’ŒVç›¸ä¹˜è¿‡ç¨‹ä¸­ä¹Ÿå¯ä»¥è¿›è¡ŒçŸ©é˜µå¸æ”¶$output = MV = MW^{UK}c_t^{KV}$ï¼Œä»è€Œé¿å…å°†Vä¹Ÿè§£å‹ç¼©ã€‚

åŒç†ï¼ŒRoPEéƒ¨åˆ†å¦‚ä¸‹ï¼š
$$
q_t^{RT}k_t^R = (RW^{UQ}c_t^Q )^T RW^{UK}c_t^{KV} = c_t^QW^{UQ} R^TRW^{UK}c_t^{KV} = \\
(c_t^{QT}W^{UQT}R^TRW^{UK})c_t^{KV}
$$

### 5.3.4 å…¶ä»–ç»†èŠ‚
- SDPAï¼šåˆ†å­ä¸€æ ·ï¼Œç¼©æ”¾å› å­ä»d_kå˜ä¸ºæ³¨æ„åŠ›å¤´ç»´åº¦åŠ è§£è€¦QKçš„ç»´åº¦$d_h+d_h^R$ï¼Œç¬¬iä¸ªheadçš„æ³¨æ„åŠ›å¦‚ä¸‹ï¼Œjè¡¨ç¤ºè¡Œï¼ˆå› ä¸ºæ³¨æ„åŠ›è®¡ç®—æ˜¯å•ä¸ªtokenå¯¹ä¹‹å‰æ‰€æœ‰tokenï¼‰ï¼š
    $$\text{Score}_{t,j,i} = \frac{q_{t,i}^Tk_{j,i}}{\sqrt{d_h+d_h^R}}$$
- Softmax:
    $$\alpha_{t,j,i} = \text{Softmax}_j(\text{Score}_{t,j,i})$$
- è§£è€¦çš„RoPEï¼šåŸæœ¬QKéƒ½éœ€è¦RoPEç¼–ç ï¼Œä½†MLAä¸­çš„QKéƒ½æ˜¯ç”±æ½œåœ¨ç©ºé—´å‹ç¼©é‡å»ºç»“æœä»¥åŠåŸå§‹å¤šå¤´æ³¨æ„åŠ›RoPEçš„ç»“æœæ‹¼æ¥è€Œæˆï¼Œæ½œåœ¨ç©ºé—´ä¸­çš„è¡¨ç¤ºå¹¶ä¸å®Œå…¨ä¾èµ–äºè¾“å…¥çš„åŸå§‹ç©ºé—´ï¼Œè€Œæ˜¯é€šè¿‡ç¼–ç å™¨å’Œè§£ç å™¨ä¸­é—´çš„æŠ½è±¡å±‚ç”Ÿæˆçš„ã€‚å› æ­¤ï¼Œè§£è€¦çš„RoPEèƒ½å¤Ÿæ›´å¥½åœ°é€‚é…è¿™ç§æ½œåœ¨ç©ºé—´çš„è¡¨ç¤ºï¼Œé¿å…ä½ç½®ç¼–ç å’Œæ½œåœ¨è¡¨ç¤ºä¹‹é—´çš„è¿‡åº¦è€¦åˆï¼Œå…è®¸å®ƒä»¬ç‹¬ç«‹å¤„ç†ä»ä¸åŒä¿¡æ¯æºå¾—åˆ°çš„ç‰¹å¾ã€‚